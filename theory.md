# Содержание:
<a id="sections"></a>

* [Python](#Python)
   * [Разница 2 и 3 версии python](#Difference-2-and-3-python-version)  
   * [ООП](#oop)  
      - [SOLID](#SOLID)  
      - [Что такое классы?](#classes)  
      - [Что такое магические методы?](#magic-methods)  
      - [Что такое миксин?](#mixin) 
   * [Типы и структуры данных в python](#Types-and-data-structures-in-python)  
      - [Какие типы и структуры данных бывают в python?](#Types-and-data-structures)  
      - [Что такое мутабельные и иммутабельные типы данных?](#mutable-and-immutable-data-types)  
      - [Что может быть в качестве ключа словаря?](#dictionary-key)  
      - [Что такое хеш-функция?](#python-hash)  
      - [В чём особенность словаря в python?](#dictionary-in-python)  
      - [Списки, кортежи и множества в чём их отличие?](#lists-tuples-and-sets) 
   * [GIL](#GIL) 
      - [Что такое GIL?](#what-is-GIL)  
      - [Многопоточные и многопроцессорные программы в python](#multithreaded-and-multiprocessing-programs-in-python)  
      - [Разница между потоками и процессами](#difference-between-threads-and-processes)  
      - [Что такое условия гонки и потокобезопасность?](#what-are-race-conditions-and-thread-safety)  
      - [Алгоритм планирования доступа потоков к общим данным](#algorithm-for-scheduling-access-of-streams-to-shared-data)  
   * [GC](#GC)  
      - [Как в питоне обстоят дела с памятью (управлением памятью)](#memory-management-in-python)  
      - [Сколько стоит проверка элемента в нотации?](#how-much-does-it-cost-to-validate-an-element-in-notation)  
      - [Если есть два объекта и они указывают друг на друга](#two-objects) 
   * [Итераторы, декораторы и генераторы](#Iterators-decorators-and-generators)  
      - [Что такое итератор?](#what-is-iterator)  
      - [Что такое генератор?](#what-is-generator)
      - [Что такое декоратор?](#what-is-decorator)  
      - [Какие декораторы стандартной библиотеки вы знаете?](#standard-library-decorators) 
      - [Что такое list comprehension, какой синтаксис создания генераторов?](list-comprehension)
      - [Является ли range итератором?](is-range-an-iterator)
   * [Алгоритмы](#Algorithms)  

* [SQL](#Database)
   * [Что такое СУБД?](#What-is-a-DBMS)
   * [Какие типы СУБД в соответствии с моделями данных существуют?](#types-of-DBMS)
   * [Что такое первичный ключ?](#primary-key)
   * [Что такое внешний ключ?](#external-key)
   * [Ограничения в SQL](#limitations-in-SQL)
   * [Что такое Self JOIN?](#self-join)
   * [Подстановочные знаки](#wildcards)
   * [Что делают псевдонимы Aliases?](#aliases)
   * [Разница между командами DELETE и TRUNCATE](#difference-between-DELETE-and-TRUNCATE)
   * [Разница между WHERE и HAVING](#difference-between-WHERE-and-HAVING)
   * [Основные команды SQL](#basic-sql-commands)
   * [Математические функции в SQL](#math-functions)
   * [Оконные функции в SQL](#window-functions)

* [Big Data](#Big-Data)
   * [Что такое DWH](#dwh)  
   * [Data Lake](#data-lake)
   * [Витрины данных](#data-marts)
   * [ETL и ETL-запросы](#ETL)
   * [Что такое Hadoop?](#hadoop)
   * [Data Vault](#data-vault)
   * [Распределенная файловая система HDFS](#distributed-file-system-HDFS)
     * [Архитектура HDFS](#HDFS-architecture)
     * [Shell-команды](#Shell-commands)
     * [Java API](#Java_API1)
   * [MapReduce](#MapReduce)
     * [Парадигма MapReduce](#MapReduce-paradigm)
     * [Фреймворк MapReduce](#MapReduce-framework)
     * [Java API](#Java_API2)
     * [Hadoop Streaming](#Hadoop-Streaming)
   * [Решение задач с помощью MapReduce](#Solving-problems-with-MapReduce)
     * [Алгоритмы на MapReduce](#Algorithms-on-MapReduce)
     * [Реляционные функции](#Relational-functions)
     * [Расчет TF-IDF](#TF-IDF-calculation)
   * [Алгоритмы на графах в MapReduce](#Graph-Algorithms-in-MapReduce)
     * [Графы в MapReduce](#Graphs-in-MapReduce)
     * [Поиск кратчайшего пути в графе](#Finding-the-shortest-path-in-a-graph)
     * [PageRank](#PageRank)
     * [Проблемы MR-алгоритмов на графах](#Problems-of-MR-algorithms-on-graphs)
   * [Pig и Hive](#Pig-and-Hive)
     * [Pig](#Pig)
     * [Основные операторы PigLatin](#Basic-operators-of-PigLatin)
     * [Hive](#Hive)
     * [Pig vs Hive](#Pig-vs-Hive)
   * [NoSQL базы данных: HBase и Cassandra](#HBase-and-Cassandra)
     * [Способы хранения данных](#Data-storage-methods)
     * [NoSQL](#NoSQL)
     * [Введение в HBase](#Introduction-to-HBase)
     * [Архитектура HBase](#HBase-architecture)
     * [Cassandra](#Cassandra)
   * [Spark](#Spark)
     * [Основные понятия Spark](#Basic-concepts-of-Spark)
     * [Операторы Spark](#Spark-Operators)
     * [Фреймворк Spark](#Spark-framework)
   * [YARN. MapReduce 2.0](#YARN)
     * [Что такое YARN?](#What-is-YARN)
     * [Компоненты YARN](#YARN-components)
     * [MapReduce 2.0](#MapReduce-2.0)

* [МатСтат и ТерВер](#statistics-and-probability-theory)
   * [Меры центральной тенденции](#сentral-trend-measures)
   * [Процентили и квартили](#percentile-and-quartile)
   * [Математическое ожидание](#expected-value)
   * [Меры изменчивости](#Measures-of-variability)

* [Задачи и алгоритмы машинного обучения](#Machine-learning-tasks-and-algorithms)
   * [Задачи машинного обучения](#machine-learning-tasks)  
     * [Что такое обучение с учителем?](#supervised-learning)  
     * [Что такое обучение без учителя?](#unsupervised-learning)  
     * [Что такое классификация?](#classification)  
     * [Зачем использовать отбор (или селекцию) признаков (Feature selection)?](#Feature-selection)  
     * [Основные метрики задач классификации](#classification-metrics)
     * [Что такое кластеризация?](#clustering)  
     * [Что такое понижение размерности?](#dimension-reduction)  
     * [Что такое смещение и дисперсия, и каковы их отношения в моделировании данных?](#bias-and-variance)  
  * [Алгоритмы машинного обучения](#machine-learning-algorithms)  
     * [Линейная регрессия](#linear-regression)  
     * [Логистическая регрессия](#logistic-regression)  
     * [Линейный дискриминантный анализ (LDA)](#LDA)
     * [Случайный лес (Random Forest)](#Random-Forest)
     * [Метод опорных векторов (SVM)](#SVM)
     * [K-ближайших соседей (KNN)](#KNN)
     * [Наивный Байесовский классификатор](#Bayes)
   * [Бустинг](#boosting)  
   * [Метрики регрессии](#Regression-metrics)
   * [Обучение с помощью градиентного спуска](#gradient-descent-training)
   * [Алгоритмы кластеризации такие как k-means и c-means, dbscan](#k-means-c-means-dbscan)
   * [Иерархическая кластеризация](#hierarchical-clustering)
   * [Алгоритмы понижения размерности такие как PCA, t-SNE](#lPCA-t-SNE)
   * [Что такое регуляризация и чем она полезна?](#regularization)
   * [Алгоритмы для нейронных сетей](#algorithms-for-neural-networksn)
   * [Полносвязная нейронная сеть](#fully-connected-neural-network)
   * [Свёрточная нейронная сеть](#convolutional-neural-networkn)
   * [Рекуррентная нейронная сеть](#recurrent-neural-network)
   * [Для каких задач подходит тот или иной алгоритм?](#algorithm-suitable)
   * [Что такое градиент и для чего он нужен?](#gradient)
* [Что такое функция активации?](#activation-function)

* [Сеть](#network)
   * [Что такое веб-сервисы?](#web-services)
   * [REST](#rest)  
   * [http](#http)
   * [JSON](#json)   
   * [XML](#xml)
   * [Очереди сообщений](#message-queues)
   * [Что происходит в тот момент, когда вы вводите в адресной строке браузера URL сайта и нажимаете ввод?](#www)

* [Бизнес](#business)

* [Источники](#sources)
<br/>



# Python
<a id="Python"></a>
  * [Разница 2 и 3 версии python](#Difference-2-and-3-python-version)  
  * [ООП](#oop)  
  * [Типы и структуры данных в python](#Types-and-data-structures-in-python)  
  * [GIL](#GIL)  
  * [GC](#GC)  
  * [Итераторы, декораторы и генераторы](#Iterators-decorators-and-generators)
  * [Алгоритмы](#Algorithms)  

([наверх](#sections))

## Разница 2 и 3 версии python
<a id="Difference-2-and-3-python-version"></a>

В Python 2 print был оператором: ```print "Hello, world"```  
В Python 3 print - функция: ```print ("Hello, world")```

В Python 2 были две функции: range - возвращает список; xrange - возвращает итератор  
В Python 3 есть только функция range, и она возвращает итератор

В Python 2 при делении целых чисел возвращает целоче число  
В Python 3 при делении целых чисел возвращает вещественное число

**Магические методы**

* Так как в Питоне 3 различий между строкой и юникодом больше нет, ``` __unicode__```  исчез, а появился ``` __bytes__```  (который ведёт себя так же как ``` __str__```  и ``` __unicode__```  в 2.7) для новых встроенных функций построения байтовых массивов.  
* Так как деление в Питоне 3 теперь по-умолчанию «правильное деление», ``` __div__```  больше нет.  
* ``` __coerce__```  больше нет, из-за избыточности и странного поведения.  
* ``` __cmp__```  больше нет, из-за избыточности.  
* ``` __nonzero__```  было переименовано в ``` __bool__``` .  
* ``` next```  у итераторов был переименован в ``` __next__``` .

## ООП
<a id="oop"></a>
* [SOLID](#SOLID)  
* [Что такое классы?](#classes)  
* [Магические методы](#magic-methods)  
* [Что такое миксин?](#mixin)  

([наверх](#sections))

### SOLID
<a id="SOLID"></a>

S - Принцип единственной ответственности (single responsibility principle)
Для каждого класса должно быть определено единственное назначение. Не должно возникать God object, который занимается всем в программе. 

O - Принцип открытости/закрытости (open–closed principle)
«программные сущности … должны быть открыты для расширения, но закрыты для модификации». Мы должны иметь возможность добавлять функциональность. 

L - Принцип подстановки Барбары Лисков (Liskov substitution principle)
«объекты в программе должны быть заменяемыми на экземпляры их подтипов без изменения 
правильности выполнения программы». У объекта есть тип, но сами классы выстраиваются в иерархию классов. Подтип типа всю старую функциональность должен выполнять. 

I - Принцип разделения интерфейса (interface segregation principle)
«много интерфейсов, специально предназначенных для клиентов, лучше, чем один интерфейс 
общего назначения». Интерфейс - способ провзаимодействовать с какой-то программной действующей функцией. 

D - Принцип инверсии зависимостей (dependency inversion principle) «Зависимость на Абстракциях. Нет зависимости на что-то конкретное». 

### Что такое классы?
<a id="classes"></a>
([наверх](#sections))

Класс — тип, описывающий устройство объектов. Объект — это экземпляр класса.
```
class C: 
  pass
имя_объекта = имя_класса()
```
У класса может не быть тела. 

Простейший пример класса:
```
class Rectangle:
  default_color = "green" # статический атрибут
  def __init__(self, width, height): # конструктор класса
    self.width = width # динамический атрибут
    self.height = height # динамический атрибут
```
В python нет возможности сделать несколько конструкторов. 

### Магические методы
<a id="magic-methods"></a>
([наверх](#sections))

Если какой-то идентификатор начинается с двух подчёркиваний, дальше пишется что-либо, потом снова два подчёркивания, то это спец метод. 

**Какие магические методы и для чего используются?**
```
class FileObject:
  def __init__(self, filepath='~', filename='sample.txt'): # Обёртка для файлового объекта, чтобы быть уверенным в том, что файл будет закрыт при удалении.
    self.file = open(join(filepath, filename), 'r+') # Открыть файл filename в filepath в режиме чтения и записи
  def __del__(self):
    self.file.close()
    del self.file
``` 
Метод, который будет вызван при инициализации объекта.  
Это первый метод, который будет вызван при инициализации объекта. Он принимает в качестве параметров класс и потом любые другие аргументы, которые будут переданы в ``` __init__``` . ``` __new__```  используется весьма редко, но иногда бывает полезен, в частности, когда класс наследуется от неизменяемого (immutable) типа, такого как кортеж (tuple) или строка.
``` 
__new__(cls, [...])
``` 
Инициализатор класса. Самый базовый магический метод, ``` __init__``` . С его помощью мы можем инициализировать объект.
``` 
__init__(self, [...]) 
``` 
Деструктор объекта
``` 
__del__
``` 

Самый базовый из методов сравнения ``` __cmp__(self, other)``` . Он, в действительности, определяет поведение для всех операторов сравнения (>, ==, !=, итд.), но не всегда так, как вам это нужно (например, если эквивалентность двух экземпляров определяется по одному критерию, а то что один больше другого по какому-нибудь другому). ``` __cmp__```  должен вернуть отрицательное число, если ``` self < other``` , ноль, если ``` self == other``` , и положительное число в случае ``` self > other``` . Но, обычно, лучше определить каждое сравнение, которое вам нужно, чем определять их всех в ``` __cmp__``` . Но ``` __cmp__```  может быть хорошим способом избежать повторений и увеличить ясность, когда все необходимые сравнения оперируют одним критерием.

* ``` __eq__(self, other)``` 
Определяет поведение оператора равенства, ==.

* ``` __ne__(self, other)``` 
Определяет поведение оператора неравенства, !=.

* ``` __lt__(self, other)``` 
Определяет поведение оператора меньше, <.

* ``` __gt__(self, other)``` 
Определяет поведение оператора больше, >.

* ``` __le__(self, other)``` 
Определяет поведение оператора меньше или равно, <=.

* ``` __ge__(self, other)``` 
Определяет поведение оператора больше или равно, >=.

**Унарные операторы и функции**

Унарные операторы и функции имеют только один операнд — отрицание, абсолютное значение, и так далее.

* ``` __pos__(self)``` 
Определяет поведение для унарного плюса ```(+some_object)```

* ``` __neg__(self)``` 
Определяет поведение для отрицания```(-some_object)```

* ``` __abs__(self)``` 
Определяет поведение для встроенной функции ```abs()```.

* ``` __invert__(self)``` 
Определяет поведение для инвертирования оператором ~. Для объяснения что он делает смотри статью в Википедии о бинарных операторах.

* ``` __round__(self, n)``` 
Определяет поведение для встроенной функции ```round()```. n это число знаков после запятой, до которого округлить.

* ``` __floor__(self)``` 
Определяет поведение для ```math.floor()```, то есть, округления до ближайшего меньшего целого.

* ``` __ceil__(self)``` 
Определяет поведение для ```math.ceil()```, то есть, округления до ближайшего большего целого.

* ``` __trunc__(self)``` 
Определяет поведение для ```math.trunc()```, то есть, обрезания до целого.

**Обычные арифметические операторы**

* ```__add__(self, other)```
Сложение.

* ```__sub__(self, other)```
Вычитание.

* ```__mul__(self, other)```
Умножение.

* ```__floordiv__(self, other)```
Целочисленное деление, оператор //.

* ```__div__(self, other)```
Деление, оператор /.

* ```__truediv__(self, other)```
Правильное деление. Заметьте, что это работает только когда используется ```from __future__ import division```.

* ```__mod__(self, other)```
Остаток от деления, оператор %.

* ```__divmod__(self, other)```
Определяет поведение для встроенной функции divmod().

* ```__pow__```
Возведение в степень, оператор **.

* ```__lshift__(self, other)```
Двоичный сдвиг влево, оператор <<.

* ```__rshift__(self, other)```
Двоичный сдвиг вправо, оператор >>.

* ```__and__(self, other)```
Двоичное И, оператор &.

* ```__or__(self, other)```
Двоичное ИЛИ, оператор |.

* ```__xor__(self, other)```
Двоичный xor, оператор ^.

**Магические методы преобразования типов**

В Питоне множество магических методов, предназначенных для определения поведения для встроенных функций преобразования типов.

* ```__int__(self)```
Преобразование типа в int.

* ```__long__(self)```
Преобразование типа в long.

* ```__float__(self)```
Преобразование типа в float.

* ```__complex__(self)```
Преобразование типа в комплексное число.

* ```__oct__(self)```
Преобразование типа в восьмеричное число.

* ```__hex__(self)```
Преобразование типа в шестнадцатеричное число.

* ```__index__(self)```
Преобразование типа к int, когда объект используется в срезах (выражения вида ```[start:stop:step]```). Если вы определяете свой числовой тип, который может использоваться как индекс списка, вы должны определить ```__index__```.

* ```__trunc__(self)```
Вызывается при ```math.trunc(self)```. Должен вернуть своё значение, обрезанное до целочисленного типа (обычно long).

* ```__coerce__(self, other)```
Метод для реализации арифметики с операндами разных типов. ```__coerce__``` должен вернуть None если преобразование типов невозможно. Если преобразование возможно, он должен вернуть пару (кортеж из 2-х элементов) из self и other, преобразованные к одному типу.

**Представление своих классов**

Часто бывает полезно представление класса в виде строки. В Питоне существует несколько методов, которые вы можете определить для настройки поведения встроенных функций при представлении вашего класса.

* ```__str__(self)```
Определяет поведение функции ```str()```, вызванной для экземпляра вашего класса.

* ```__repr__(self)```
Определяет поведение функции ```repr()```, вызванной для экземпляра вашего класса. Главное отличие от ```str()``` в целевой аудитории. ```repr()``` больше предназначен для машинно-ориентированного вывода (более того, это часто должен быть валидный код на Питоне), а ```str()``` предназначен для чтения людьми.

* ```__unicode__(self)```
Определяет поведение функции ```unicode()```, вызванной для экземпляра вашего класса. ```unicode()``` похож на ```str()```, но возвращает строку в юникоде. Если клиент вызывает ```str()``` на экземпляре вашего класса, а вы определили только ```__unicode__()```, то это не будет работать. Постарайтесь всегда определять ```__str__()``` для случая, когда кто-то не имеет такой роскоши как юникод.

* ```__format__(self, formatstr)```
Определяет поведение, когда экземпляр вашего класса используется в форматировании строк нового стиля. Например, ```"Hello, {0:abc}!".format(a)``` приведёт к вызову ``` a.__format__("abc")```. Это может быть полезно для определения ваших собственных числовых или строковых типов, которым вы можете захотеть предоставить какие-нибудь специальные опции форматирования.

* ```__hash__(self)```
Определяет поведение функции ```hash()```, вызванной для экземпляра вашего класса. Метод должен возвращать целочисленное значение, которое будет использоваться для быстрого сравнения ключей в словарях. Заметьте, что в таком случае обычно нужно определять и ```__eq__``` тоже. Руководствуйтесь следующим правилом: ```a == b``` подразумевает ```hash(a) == hash(b)```.

* ```__nonzero__(self)```
Определяет поведение функции ```bool()```, вызванной для экземпляра вашего класса. Должна вернуть ```True``` или ```False```, в зависимости от того, когда вы считаете экземпляр соответствующим ```True``` или ```False```.

* ```__dir__(self)```
Определяет поведение функции ```dir()```, вызванной на экземпляре вашего класса. Этот метод должен возвращать пользователю список атрибутов. Обычно, определение ```__dir__``` не требуется, но может быть жизненно важно для интерактивного использования вашего класса, если вы переопределили ```__getattr__``` или ```__getattribute__```.

* ```__sizeof__(self)```
Определяет поведение функции``` sys.getsizeof()```, вызванной на экземпляре вашего класса. Метод должен вернуть размер вашего объекта в байтах.

**Магия контейнеров**

Магические методы, используемые контейнерами.

* ```__len__(self)```
Возвращает количество элементов в контейнере. Часть протоколов для изменяемого и неизменяемого контейнеров.

* ```__getitem__(self, key)```
Определяет поведение при доступе к элементу, используя синтаксис ```self[key]```. Тоже относится и к протоколу изменяемых и к протоколу неизменяемых контейнеров. Должен выбрасывать соответствующие исключения: TypeError если неправильный тип ключа и KeyError если ключу не соответствует никакого значения.

* ```__setitem__(self, key, value)```
Определяет поведение при присваивании значения элементу, используя синтаксис ```self[nkey] = value```. Часть протокола изменяемого контейнера. Опять же, вы должны выбрасывать KeyError и TypeError в соответствующих случаях.

* ```__delitem__(self, key)```
Определяет поведение при удалении элемента (то есть ```del self[key]```). Это часть только протокола для изменяемого контейнера. Вы должны выбрасывать соответствующее исключение, если ключ некорректен.

* ```__iter__(self)```
Должен вернуть итератор для контейнера. Итераторы возвращаются в множестве ситуаций, главным образом для встроенной функции ```iter()``` и в случае перебора элементов контейнера выражением ```for x in container:```. Итераторы сами по себе объекты и они тоже должны определять метод ```__iter__```, который возвращает ```self```.

* ```__reversed__(self)```
Вызывается чтобы определить поведения для встроенной функции ```reversed()```. Должен вернуть обратную версию последовательности. Реализуйте метод только если класс упорядоченный, как список или кортеж.

* ```__contains__(self, item)```
```__contains__``` предназначен для проверки принадлежности элемента с помощью in и not in. Вы спросите, почему же это не часть протокола последовательности? Потому что когда ```__contains__``` не определён, Питон просто перебирает всю последовательность элемент за элементом и возвращает True если находит нужный.

* ```__missing__(self, key)```
```__missing__``` используется при наследовании от ```dict```. Определяет поведение для для каждого случая, когда пытаются получить элемент по несуществующему ключу (так, например, если у меня есть словарь ```d``` и я пишу ```d["george"]``` когда "george" не является ключом в словаре, вызывается ```d.__missing__("george"))```.

**Построение дескрипторов**

Дескрипторы — это такие классы, с помощью которых можно добавить свою логику к событиям доступа (получение, изменение, удаление) к атрибутам других объектов. Дескрипторы не подразумевается использовать сами по себе; скорее, предполагается, что ими будут владеть какие-нибудь связанные с ними классы. Дескрипторы могут быть полезны для построения объектно-ориентированных баз данных или классов, чьи атрибуты зависят друг от друга. В частности, дескрипторы полезны при представлении атрибутов в нескольких системах исчисления или каких-либо вычисляемых атрибутов (как расстояние от начальной точки до представленной атрибутом точки на сетке).

Чтобы класс стал дескриптором, он должен реализовать по крайней мере один метод из ```__get__```, ```__set__``` или ```__delete__```. 

* ```__get__(self, instance, instance_class)```
Определяет поведение при возвращении значения из дескриптора. ```instance``` это объект, для чьего атрибута-дескриптора вызывается метод. owner это тип (класс) объекта.

* ```__set__(self, instance, value)```
Определяет поведение при изменении значения из дескриптора. ```instance``` это объект, для чьего атрибута-дескриптора вызывается метод. value это значение для установки в дескриптор.

* ```__delete__(self, instance)```
Определяет поведение для удаления значения из дескриптора. ```instance``` это объект, владеющий дескриптором.

**Как вызывать магические методы** 

| **Магический метод**                  |	**Когда он вызывается (пример)**        |	**Объяснение**                                       |
|:--------------------------------------|:----------------------------------------|:-----------------------------------------------------| 
| ```__new__(cls [,...])```             |	```instance = MyClass(arg1, arg2)```    |	```__new__``` вызывается при создании экземпляра     |
| ```__init__(self [,...])```           |	```instance = MyClass(arg1, arg2)```    |	```__init__``` вызывается при создании экземпляра    |
| ```__cmp__(self, other)```            |	```self == other, self > other, etc.```	| Вызывается для любого сравнения                      |
| ```__pos__(self)```                   |	```+self```                            	| Унарный знак плюса                                   |
| ```__neg__(self)```                   |	```-self```	                            | Унарный знак минуса                                  |
| ```__invert__(self)```                |	```~self```	                            | Побитовая инверсия                                   |
| ```__index__(self)```                 |	```x[self]```	                          | Преобразование, когда объект используется как индекс |
| ```__nonzero__(self)```               |	```bool(self), if self:```             	| Булевое значение объекта                             |
| ```__getattr__(self, name)```         |	```self.name # name не определено```	  | Пытаются получить несуществующий атрибут             |
| ```__setattr__(self, name, val)```    |	```self.name = val```	                  | Присвоение любому атрибуту                           |
| ```__delattr__(self, name)```         |	```del self.name```	                    | Удаление атрибута                                    |
| ```__getattribute__(self, name)```    |	```self.name```	                        | Получить любой атрибут                               |
| ```__getitem__(self, key)```          |	```self[key]```	                        | Получение элемента через индекс                      |
| ```__setitem__(self, key, val)```     |	```self[key] = val```                  	| Присвоение элементу через индекс                     |
| ```__delitem__(self, key)```          |	```del self[key]```	                    | Удаление элемента через индекс                       |
| ```__iter__(self)```                  |	```for x in self```	                    | Итерация                                             |
| ```__contains__(self, value)```       |	```value in self, value not in self```	| Проверка принадлежности с помощью in                 |
| ```__call__(self [,...])```           |	```self(args)```	                      | «Вызов» экземпляра                                   |
| ```__enter__(self)```                 |	```with self as x:```	                  | ```with``` оператор менеджеров контекста             |
| ```__exit__(self, exc, val, trace)``` |	```with self as x:```	                  | ```with``` оператор менеджеров контекста             |
| ```__getstate__(self)```              |	```pickle.dump(pkl_file, self)```	      | Сериализация                                         |
| ```__setstate__(self)```              |	```data = pickle.load(pkl_file)```	    | Сериализация                                         |

### Что такое миксин?
<a id="mixin"></a>
([наверх](#sections))

Это класс, который реализует несколько методов, которые ты добавляешь к разным классам для того, чтобы они унаследовали и тоже получили какие-то методы. 

## Типы и структуры данных в python
<a id="Types-and-data-structures-in-python"></a>

* [Какие типы и структуры данных бывают в python?](#Types-and-data-structures)  
* [Что такое мутабельные и иммутабельные типы данных?](#mutable-and-immutable-data-types)  
* [Что может быть в качестве ключа словаря?](#dictionary-key)  
* [Что такое хеш-функция?](#python-hash)  
* [В чём особенность словаря в python?](#dictionary-in-python)  
* [Списки, кортежи и множества в чём их отличие?](#lists-tuples-and-sets)  

([наверх](#sections))

### Какие типы и структуры данных бывают в python?
<a id="Types-and-data-structures"></a>

| *Объект*                          | *Тип*                      | 
|:---------------------------------:|:--------------------------:| 
| Строка                            | str                        |
| Целое число                       | int                        |
| Число с плавающей точкой          | float                      |
| Список                            | list                       |
| Кортеж                            | tuple                      |
| Словарь                           | dict                       |
| Множество                         | set                        |
| Логический                        | bool                       |
| Функция                           | function                   |
| Класс, определяемый пользователем | type                       |
| Экземпляр класса                  | class                      |
| Встроенная функция                | builtin_function_or_method |
| type                              | type                       |

### Что такое мутабельные и иммутабельные типы данных?
<a id="mutable-and-immutable-data-types"></a>
([наверх](#sections))

Объекты в питоне бывают двух значительно отличающихся сортов: изменяемые (mutable) и неизменяемые (immutable). Неизменяемыми являются целые и действительные числа (int, float), строки (str), последовательности байтов (бинарные данные, bytes), а также кортежи, все элементы которых неизменяемы (tuple). Напротив, списки (list), словари (dict) и множества (set) являются изменяемыми.

### Что может быть в качестве ключа словаря?
<a id="dictionary-key"></a>
([наверх](#sections))

Только неизменяемые типы данных. Ключами словаря могут являться только объекты, поддерживающие хеширование. Таким образом, использовать в качестве ключей списки, словари и другие изменяемые типы не получится. Если в словарь будут добавлены несколько значений с одним и тем же ключом, словарь сохранит последнее.

Не рекомендуется использоваться в качестве ключей числа с плавающей запятой, так как они хранятся в памяти в виде приближений.

### Что такое хеш-функция?
<a id="python-hash"></a>
([наверх](#sections))

Хэш-функция - это функция, которая принимает на вход какие-либо данные (например, строки) и возвращает число по некоторому заданному алгоритму.  
Назначением хэш-функций является возможность помещения некоторого элемента (например, строки) в хэш-таблицу, на основе которых реализованы, например, словари и множества в Python. 

Одинаковые данные будут иметь одинаковое хеш-значение.  
* Даже небольшое изменение исходных данных может привести к совершенно иному хеш-значению.  
* Хеш получается из хеш-функции, в обязанности которой входит преобразование данной информации в закодированный хеш.   
* Очевидно, что количество объектов может быть намного больше, чем количество хеш-значений, и поэтому два объекта могут хешировать одно и то же. Это называется конфликтом хэша. Это означает, что если два объекта имеют одинаковый хэш-код, они не обязательно имеют одно и то же значение.  

Cрок жизни хэша зависит только от области действия программы, и он может измениться, как только программа завершится.

### В чём особенность словаря в python?
<a id="dictionary-in-python"></a>
([наверх](#sections))

Словари в Python - неупорядоченные коллекции произвольных объектов с доступом по ключу. Их иногда ещё называют ассоциативными массивами или хеш-таблицами.  

Чтобы работать со словарём, его нужно создать. Сделать это можно несколькими способами:  

* Во-первых, с помощью литерала:
  ```
  d = {}
  print(d)
  {}
  d = {'dict': 1, 'dictionary': 2}
  prtin(d)
  {'dict': 1, 'dictionary': 2}
  ```
  
* Во-вторых, с помощью функции dict:
```
  d = dict(short='dict', long='dictionary')
  prtin(d)
  {'short': 'dict', 'long': 'dictionary'}
  d = dict([(1, 1), (2, 4)])
  prtin(d)
  {1: 1, 2: 4}
```

* В-третьих, с помощью метода fromkeys:
```
  d = dict.fromkeys(['a', 'b'])
  prtin(d)
  {'a': None, 'b': None}
  d = dict.fromkeys(['a', 'b'], 100)
  prtin(d)
  {'a': 100, 'b': 100}
```

* В-четвертых, с помощью генераторов словарей, которые очень похожи на генераторы списков.
```
  d = {a: a ** 2 for a in range(7)}
  prtin(d)
  {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36}
```

**Методы словарей**  

* ```dict.clear()``` - очищает словарь.
* ```dict.copy()``` - возвращает копию словаря.
* ```classmethod dict.fromkeys(seq[, value])``` - создает словарь с ключами из ```seq``` и значением ```value``` (по умолчанию ```None```).
* ```dict.get(key[, default])``` - возвращает значение ключа, но если его нет, не бросает исключение, а возвращает ```default``` (по умолчанию ```None```).
* ```dict.items()``` - возвращает пары (ключ, значение).
* ```dict.keys()``` - возвращает ключи в словаре.
* ```dict.pop(key[, default])``` - удаляет ключ и возвращает значение. Если ключа нет, возвращает ```default``` (по умолчанию бросает исключение).
* ```dict.popitem()``` - удаляет и возвращает пару (ключ, значение). Если словарь пуст, бросает исключение ```KeyError```. Важно помнить, что словари неупорядочены.
* ```dict.setdefault(key[, default])``` - возвращает значение ключа, но если его нет, не бросает исключение, а создает ключ со значением ```default``` (по умолчанию ```None```).
* ```dict.update([other])``` - обновляет словарь, добавляя пары (ключ, значение) из ```other```. Существующие ключи перезаписываются. Возвращает ```None``` (не новый словарь!).
* ```dict.values()``` - возвращает значения в словаре.

### Списки, кортежи и множества в чём их отличие?
<a id="lists-tuples-and-sets"></a>
([наверх](#sections))

_List (список)_
Базовая структура данных в python. Элементы в списке хранятся последовательно, каждому из них присвоены индексы, начиная с нуля. В отличие от массива, список может хранить объекты любого типа.

Создание списка
```
my_list = [] # Создание пустого списка с помощью литерала списка
my_list = list() # Создание пустого списка с помощью встроенной функции

my_list = [1,2,['a','b'],4,5] # Инициализация списка

my_list = list('hello world') # Создание списка из итерируемого объекта
print(my_list)
['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']

my_list = [x for x in range(10)] # Генератор списков в действии
print(my_list)
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
```
Доступные методы
* ```my_list.append(x)``` - добавляет x в конец списка
* ```my_list.clear()``` - очищает 
* ```my_list.copy()``` - возвращает копию списка ```my_list```
* ```my_list.count(x)``` - возвращает кол-во элементов со значением x
* ```my_list.extend(x)``` - добавляет элементы списка x к концу списка ```my_list```
* ```my_list.index(x,start,end)``` - возвращает индекс первого найденного x, можно задать промежуток для поиска (опционально)
* ```my_list.insert(index, x)``` - вставляет x на заданную позицию
* ```my_list.pop(index)``` - возвращает элемент с указанным индексом и удаляет его, если индекс не указан - возвращается и удаляется последний элемент
* ```my_list.remove(x)``` - удаляет первый элемент со значением x
* ```my_list.reverse()``` - инвертирует порядок элементов в списке
* ```my_list.sort(key=x)``` сортирует список на основе функции x  

В каких случаях использовать?
Для хранения элементов, объединенных каким либо признаком. В случае, если изменение элементов и/или расширение списка не предполагается, следует использовать неизменяемый аналог - кортеж.

_Tuple (кортёж)_
Кортеж - это неизменяемый и более быстрый аналог списка. Он защищает хранимые данные от непреднамеренных изменений и может использоваться в качестве ключа в словарях (словарь - ассоциативный массив в python).

Создание кортежа.
```
my_tuple = () # Создание кортежа с помощью литерала
my_tuple = tuple() # Создание кортежа с помощью встроенной функции

my_tuple = (1,2,['a','b'],4,5) # Инициализация кортежа

my_tuple = tuple('hello world') # Создание кортежа из итерируемого объекта
print(my_tuple)
('h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd')

my_tuple = tuple(2**x for x in [0, 1, 2, 3]) # Генератор кортежей
print(my_tuple)
(1, 2, 4, 8)
```
Доступные методы
* ```my_tuple.count(x)``` - возвращает кол-во элементов со значением x
* ```my_tuple.index(x,start,end)``` - возвращает индекс первого найденного x, можно задать промежуток для поиска (опционально)  

В каких случаях использовать?
Для хранения данных вместо списка (если они не предполагают изменений).

_Set (множество)_
Множество - это набор уникальных элементов в случайном порядке (неупорядоченный список). Множества примечательны тем, что операция проверки "принадлежит ли объект множеству" происходит значительно быстрее аналогичных операций в других структурах данных.

Создание множества
```
my_something = {} # !!! Попытка создать множество при помощи литерала даст нам словарь
type(my_something)
<class 'dict'> 

my_set = set() # Создание при помощи встроенной функции

my_set = {1,2,3,4,5} # Инициализация множества

my_set = set('hello world') # Создания множества из итерируемого объекта
print(my_set)
{'r', 'o', 'e', 'h', 'd', 'w', 'l', ' '}

my_set = {x for x in range(10)} # Генератор множеств
print(my_set)
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
```
Доступные методы
* ```my_set.add(x)``` - добавляет x во множество
* ```my_set.difference(x)``` - возвращает множество элементов ```my_set```, которые не входят во множество ```x```
* ```my_set.difference_update(x)``` - удаляет из множества my_set все элементы, которые входят во множество ```x```
* ```my_set.discard(x)``` - удаляет элемент ```x``` из ```my_set```
* ```my_set.intersection(x)``` - возвращает элементы общие для множеств ```my_set``` и ```x```
* ```my_set.intersection_update(x)``` - удаляет из множества ```my_set``` элементы, которых нет во множестве ```x```
* ```my_set.isdisjoint(x)``` - возвращает ```true``` если ```my_set``` и ```x``` не содержат одинаковых значений
* ```my_set.issubset(x)``` - возвращает true если все элементы ```my_set``` входят во множество ```x```
* ```my_set.issuperset(x)``` - возвращает ```true``` если все элементы x входят во множество ```my_set```
* ```my_set.pop()``` - возвращает и удаляет первый (на данный момент) элемент множества
* ```my_set.remove(x)``` - удаляет ```x``` из множества
* ```my_set.symmetric_difference(x)``` - возвращает все элементы из ```x``` и ```my_set```, которые встречаются только в одном из множеств
* ```my_set.symmetric_difference_update(x)``` - обновляет исходное множество таким образом, что оно будет состоять из всех элементов ```x``` и ```my_set```, которые встречаются только в одном из множеств
* ```my_set.union(x)``` - возвращает новое множество, состоящее из всех элементов ```x``` и ```my_set```
* ```my_set.update(x)``` - добавляет в ```my_set``` все элементы ```x``` 

В каких случаях использовать?
Когда необходимо проверять принадлежит ли значение набору уникальных элементов и отсутствует необходимость поддерживать порядок в данном наборе.

## GIL
<a id="GIL"></a>

* [Что такое GIL?](#what-is-GIL)  
* [Многопоточные и многопроцессорные программы в python](#multithreaded-and-multiprocessing-programs-in-python)  
* [Разница между потоками и процессами](#difference-between-threads-and-processes)  
* [Что такое условия гонки и потокобезопасность?](#what-are-race-conditions-and-thread-safety)  
* [Алгоритм планирования доступа потоков к общим данным](#algorithm-for-scheduling-access-of-streams-to-shared-data)  

([наверх](#sections))

### Что такое GIL?
<a id="what-is-GIL"></a>

В Python используется глобальная блокировка интерпретатора (Global Interpreter Lock — GIL), накладывающая некоторые ограничения на потоки. А именно, нельзя использовать несколько процессоров одновременно. Блокировка, позволяет только одному потоку управлять интерпретатором Python. Это означает, что в любой момент времени будет выполняться только один конкретный поток. Из этого следует, что с потоками невозможно использовать несколько ядер процессора.

GIL был введен в Python потому, что управление памятью CPython не является потокобезопасным. Имея такую блокировку Python может быть уверен, что никогда не будет условий гонки. (об условиях гонки в следующем разделе)

Во многопоточных программах отсутствие GIL может негативно сказываться на производительности процессоро-зависымых программ.

![image](https://user-images.githubusercontent.com/54559853/127723378-17c5daa9-46c0-4a24-b493-de66914bd3b1.png)  

Python подсчитывает количество ссылок для корректного управления памятью. Это означает, что созданные в Python объекты имеют переменную подсчёта ссылок, в которой хранится количество всех ссылок на этот объект. Как только эта переменная становится равной нулю, память, выделенная под этот объект, освобождается.

### Многопоточные и многопроцессорные программы в python
<a id="multithreaded-and-multiprocessing-programs-in-python"></a>
([наверх](#sections))

Параллелизм дает возможность работать над несколькими вычислениями одновременно в одной программе. Такого поведения в Python можно добиться несколькими способами:  
* Используя многопоточность ```threading```, позволяя нескольким потокам работать по очереди.  
* Используя несколько ядер процессора ```multiprocessing```. Делать сразу несколько вычислений, используя несколько ядер процессора. Это и называется параллелизмом.  
* Используя асинхронный ввод-вывод с модулем ```asyncio```. Запуская какую то задачу, продолжать делать другие вычисления, вместо ожидания ответа от сетевого подключения или от операций чтения/записи.  

### Разница между потоками и процессами
<a id="difference-between-threads-and-processes"></a>
([наверх](#sections))

Поток ```threading``` - это независимая последовательность выполнения каких то вычислений. Поток ```thread``` делит выделенную память ядру процессора, а так же его процессорное время со всеми другими потоками, которые создаются программой в рамках одного ядра процессора. Программы на языке Python имеют, по умолчанию, один основной поток. Можно создать их больше и позволить Python переключаться между ними. Это переключение происходит очень быстро и кажется, что они работают параллельно.

Понятие процесс в ```multiprocessing``` - представляет собой так же независимую последовательность выполнения вычислений. В отличие от потоков threading, процесс имеет собственное ядро и следовательно выделенную ему память, которое не используется совместно с другими процессами. Процесс может клонировать себя, создавая два или более экземпляра в одном ядре процессора.

Асинхронный ввод-вывод не является ни потоковым (```threading```), ни многопроцессорным (```multiprocessing```). По сути, это однопоточная, однопроцессная парадигма и не относится к параллельным вычислениям.

### Что такое условия гонки и потокобезопасность?
<a id="what-are-race-conditions-and-thread-safety"></a>
([наверх](#sections))

* Состояние гонки возникает, когда несколько потоков могут одновременно получать доступ к общей структуре данных или местоположению в памяти и изменять их, в следствии чего могут произойти непредсказуемые вещи.
  
  Если два пользователя одновременно редактируют один и тот же документ онлайн и второй пользователь сохранит данные в базу, то перезапишет работу первого пользователя. Чтобы избежать условий гонки, необходимо заставить второго пользователя ждать, пока первый закончит работу с документом и только после этого разрешить второму пользователю открыть и начать редактировать документ.

* Потокобезопасность работает путем создания копии локального хранилища в каждом потоке, чтобы данные не сталкивались с другим потоком.

### Алгоритм планирования доступа потоков к общим данным
<a id="algorithm-for-scheduling-access-of-streams-to-shared-data"></a>
([наверх](#sections))

Потоки используют одну и ту же выделенную память. Когда несколько потоков работают одновременно, то нельзя угадать порядок, в котором потоки будут обращаются к общим данным. Результат доступа к совместно используемым данным зависит от алгоритма планирования. который решает, какой поток и когда запускать. Если такого алгоритма нет, то конечные данные могут быть не такими как ожидаешь.

## GC
<a id="GC"></a>

* [Как в питоне обстоят дела с памятью (управлением памятью)](#memory-management-in-python)  
* [Сколько стоит проверка элемента в нотации?](#how-much-does-it-cost-to-validate-an-element-in-notation)  
* [Если есть два объекта и они указывают друг на друга](#two-objects)  

([наверх](#sections))

### Как в питоне обстоят дела с памятью (управлением памятью)
<a id="memory-management-in-python"></a>

### Сколько стоит проверка элемента в нотации?
<a id="how-much-does-it-cost-to-validate-an-element-in-notation"></a>
([наверх](#sections))

### Если есть два объекта и они указывают друг на друга
<a id="two-objects"></a>
([наверх](#sections))

## Итераторы, декораторы и генераторы
<a id="Iterators-decorators-and-generators"></a>
([наверх](#sections))

* [Что такое итератор?](#what-is-iterator)  
* [Что такое генератор?](what-is-generator)
* [СЧто такое декоратор?](#what-is-decorator)  
* [Какие декораторы стандартной библиотеки вы знаете?](#standard-library-decorators) 
* [Что такое list comprehension, какой синтаксис создания генераторов?](list-comprehension)
* [Является ли range итератором?](is-range-an-iterator)

([наверх](#sections))

### Что такое итератор?
<a id="what-is-iterator"></a>

Итераторы — объекты, которые позволяют обходить коллекции. Коллекции не должны обязательно существовать в памяти и быть конечными.

Итерируемый — объект, в котором есть метод ```__iter__```. В свою очередь, итератор — объект, в котором есть два метода: ```__iter__``` и ```__next__```. Почти всегда ```iterator``` возвращает себя из метода ```__iter__```, так как они выступают итераторами для самих себя, но есть исключения.

В целом стоит избегать прямого вызова ```__iter__``` и ```__next__```. При использовании ```for``` или генераторов списков Python вызывает эти методы сам. Если всё-таки необходимо вызвать методы напрямую, лучше использовать встроенные функции ```iter``` и ```next``` и в параметрах передаём итератор или контейнер. Например, если ```c``` — итерируемый, используем```iter(c)``` вместо ```c.__iter__()```. Если ```a``` — итератор, используем ```next(a)```, а не ```a.__next__()```. Это похоже на использование ```len```.

Раз уж речь зашла о ```len```, то стоит упомянуть, что итераторы не должны иметь и часто не имеют определённой длины. Поэтому в них часто нет имплементации ```__len__```. Чтобы подсчитать количество элементов в итераторе, приходится делать это вручную или использовать ```sum```. 

Некоторые итерируемые ```(iterable)``` не являются итераторами, но используют другие объекты как итераторы. Например, объект ```list``` относится к итерируемым, но не является итератором. В нём реализован метод ```__iter__```, но отсутствует метод ```__next__```. Итераторы объектов ```list``` относятся к типу ```listiterator```. У объектов ```list``` есть определённая длина, а у ```listiterator``` нет.

```
>>> a = [1, 2]
>>> type(a)
<type 'list'>
>>> type(iter(a))
<type 'listiterator'>
>>> it = iter(a)
>>> next(it)
1
>>> next(it)
2
>>> next(it)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
>>> len(a)
2
>>> len(it)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: object of type 'listiterator' has no len()
```

Когда итератор завершает работу, интерпретатор Python ожидает возбуждения исключения ```StopIteration```. Однако,  итераторы могут работать с бесконечными множествами. В таких случаях надо позаботиться о выходе из цикла.

Пример итератора - считает с нуля до бесконечности. Это упрощённая версия ```itertools.count```.

```
class count_iterator:
    n = 0

    def __iter__(self):
        return self

    def __next__(self):
        y = self.n
        self.n += 1
        return y
```

Пример использования. В последней строке сделана попытка превратить итератор в список. Это приводит к бесконечному циклу.

```
>>> counter = count_iterator()
>>> next(counter)
0
>>> next(counter)
1
>>> next(counter)
2
>>> next(counter)
3
>>> list(counter)  # Бесконечный цикл
```

Если у объекта нет метода ```__iter__```, его можно обойти, если определить метод ```__getitem__```. В этом случае встроенная функция iter возвращает итератор с типом ```iterator```, который использует ```__getitem__``` для обхода элементов списка. Этот метод возвращает ```StopIteration``` или ```IndexError```, когда обход завершается. Пример:

```
class SimpleList(object):
    def __init__(self, *items):
        self.items = items

    def __getitem__(self, i):
        return self.items[i]
```

И пример использования:

```
>>> a = SimpleList(1, 2, 3)
>>> it = iter(a)
>>> next(it)
1
>>> next(it)
2
>>> next(it)
3
>>> next(it)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
```

Ещё один интересный пример: генерация [последовательности Хофштадтера](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%A5%D0%BE%D1%84%D1%88%D1%82%D0%B0%D0%B4%D1%82%D0%B5%D1%80%D0%B0). В приведённом ниже коде итератор используется для генерации последовательности с помощью вложенных повторений.

```
Q(n)=Q(n−Q(n−1))+Q(n−Q(n−2))
```

Например, ```qsequence([1, 1])``` генерирует точную последовательность Хофштадтера. Мы используем исключение ```StopIteration```, чтобы показать, что последовательность не может продолжаться, так как для генерации следующего элемента должен использоваться несуществующий индекс. Если в параметрах укзать значения [1, 2], последовательность немедленно заканчивается.

```
class qsequence:
    def __init__(self, s):
        self.s = s[:]

    def __next__(self):
        try:
            q = self.s[-self.s[-1]] + self.s[-self.s[-2]]
            self.s.append(q)
            return q
        except IndexError:
            raise StopIteration()

    def __iter__(self):
        return self

    def current_state(self):
        return self.s
```

Пример использования:

```
>>> Q = qsequence([1, 1])
>>> next(Q)
2
>>> next(Q)
3
>>> [next(Q) for __ in range(10)]
[3, 4, 5, 5, 6, 6, 6, 8, 8, 8]
```

### Что такое генератор?
<a id="what-is-generator"></a>
([наверх](#sections))

Генераторами называют итераторы, определение которых выглядит как определение функций.  
Ещё одно определение: генераторы — функции, которые внутри используют выражение ```yield```. Генераторы не могут возвращать значения, вместо этого выдают элементы по готовности. Python автоматизирует запоминание контекста генератора, то есть текущий поток управления, значение локальных переменных и так далее. Каждый вызов метода ```__next__``` у объекта генератора возвращает следующее значение. Метод ```__iter__``` также реализуется автоматически. То есть генераторы можно использовать везде, где требуются итераторы.

```
def count_generator():
   n = 0
   while True:
     yield n
     n += 1
```

Как это применяется на практике.

```
>>> counter = count_generator()
>>> counter
<generator object count_generator at 0x106bf1aa0>
>>> next(counter)
0
>>> next(counter)
1
>>> iter(counter)
<generator object count_generator at 0x106bf1aa0>
>>> iter(counter) is counter
True
>>> type(counter)
<type 'generator'>
```

Теперь посмотрим на реализацию последовательности Q Хофштадтера с помощью генератора. Заметьте, эта реализация значительно проще использованного выше подхода. Однако здесь уже невозможно использовать методы типа current_state. Извне невозможно получить доступ к переменным, которые хранятся в контексте генератора.

Существует словарь ```gi_frame.f_locals```, но он относится к CPython, но не входит в стандарт языка Python.

Одно из возможных решений — получение одновременно списка и результата.

```
def hofstadter_generator(s):
    a = s[:]
    while True:
        try:
            q = a[-a[-1]] + a[-a[-2]]
            a.append(q)
            yield q
        except IndexError:
            Return
```

Итерация в данном примере завершается простым ```return``` без параметров. Внутри происходит возбуждение исключения ```StopIteration```. Следующий пример связан с [распределением Бернулли](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%91%D0%B5%D1%80%D0%BD%D1%83%D0%BB%D0%BB%D0%B8), которое реализуется с помощью двух генераторов. Речь идёт о бесконечной последовательности случайных булевых значений. При этом вероятность ```True``` равна ```p```, а вероятность ```False``` определяется формулой ```q=1-p```. Затем применяется экстрактор фон Неймана, который принимает процесс Бернулли с ```0 < p < 1``` как источник энтропии и возвращает чистый процесс Бернулли с ```p = 0.5```.

```
import random

def bernoulli_process(p):
    if p > 1.0 or p < 0.0:
        raise ValueError("p should be between 0.0 and 1.0.")

    while True:
        yield random.random() < p

def von_neumann_extractor(process):
    while True:
        x, y = next(proccess), next(process)
        if x != y:
            yield x
```

C помощью генераторов удобно реализовывать дискретные динамические системы. Пример ниже показывает, как с помощью генераторов реализуется [отображение тент](https://ru.wikipedia.org/wiki/%D0%9E%D1%82%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%82%D0%B5%D0%BD%D1%82).

```
>>> def tent_map(mu, x0):
...    x = x0
...    while True:
...        yield x
...        x = mu * min(x, 1.0 - x)
...
>>>
>>> t = tent_map(2.0, 0.1)
>>> for __ in range(30):
...    print(next(t))
...
0.1
0.2
0.4
0.8
0.4
0.8
0.4
0.8
0.4
0.8
0.4
0.8
0.4
0.8
0.4
0.8
0.4
0.799999999999
0.400000000001
0.800000000003
0.399999999994
0.799999999988
0.400000000023
0.800000000047
0.399999999907
0.799999999814
0.400000000373
0.800000000745
0.39999999851
0.79999999702
```
Ещё один пример касается [последовательности Коллатца](https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7%D0%B0_%D0%9A%D0%BE%D0%BB%D0%BB%D0%B0%D1%82%D1%86%D0%B0).

```
def collatz(n):
   yield n
   while n != 1:
     n = n / 2 if n % 2 == 0 else 3 * n + 1
     yield n
```

В этом примере не нужно вручную использовать ```StopIteration```. Это исключение срабатывает автоматически, когда поток управления достигает конца функции.

Пример использования генератора:

```
>>> # Если гипотеза Коллатца верна, list(collatz(n)) с любым n 
... # всегда завершается
>>> list(collatz(7))
[7, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1]
>>> list(collatz(13))
[13, 40, 20, 10, 5, 16, 8, 4, 2, 1]
>>> list(collatz(17))
[17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1]
>>> list(collatz(19))
[19, 58, 29, 88, 44, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1]
```

### Что такое декоратор?
<a id="what-is-decorator"></a>
([наверх](#sections))

### Какие декораторы стандартной библиотеки вы знаете?
<a id="standard-library-decorators"></a>
([наверх](#sections))

### Что такое list comprehension, какой синтаксис создания генераторов?
<a id="list-comprehension"></a>
([наверх](#sections))

### Является ли range итератором?
<a id="is-range-an-iterator"></a>
([наверх](#sections))

## Алгоритмы
<a id="Algorithms"></a>

<br/>

# SQL
<a id="Database"></a>

* [Что такое СУБД?](#What-is-a-DBMS)
* [Какие типы СУБД в соответствии с моделями данных существуют?](#types-of-DBMS)
* [Что такое первичный ключ?](#primary-key)
* [Что такое внешний ключ?](#external-key)
* [Ограничения в SQL](#limitations-in-SQL)
* [Что такое Self JOIN?](#self-join)
* [Подстановочные знаки](#wildcards)
* [Что делают псевдонимы Aliases?](#aliases)
* [Разница между командами DELETE и TRUNCATE](#difference-between-DELETE-and-TRUNCATE)
* [Разница между WHERE и HAVING](#difference-between-WHERE-and-HAVING)
* [Основные команды SQL](#basic-sql-commands)
* [Математические функции в SQL](#math-functions)
* [Оконные функции в SQL](#window-functions)

([наверх](#sections))

## Что такое СУБД?
<a id="What-is-a-DBMS"></a>

Чтобы правильно обрабатывать различные запросы (поиск, изменение, удаление и т.д) в базу данных, нужно специальное программное обеспечение, такое ПО получило название системы управления базами данных (СУБД).

СУБД — это общий термин, относящийся ко всем видам абсолютно разных инструментов, от компьютерных программ до встроенных библиотек. Эти приложения управляют или помогают управлять наборами данных. Так как эти данные могут быть разного формата и размера, были созданы разные виды СУБД.

СУБД основаны на моделях баз данных — определённых структурах для обработки данных. Каждая СУБД создана для работы с одной из них с учётом особенностей операций над информацией.

Хотя решений, реализующих различные модели баз данных, очень много, периодически некоторые из них становятся очень популярными и используются на протяжении многих лет. Сейчас самой популярной моделью является реляционная система управления базами данных (РСУБД).

## Какие типы СУБД в соответствии с моделями данных существуют?
<a id="types-of-DBMS"></a>
([наверх](#sections))

* Реляционные, которые поддерживают установку связей между таблицами с помощью первичных и внешних ключей. Пример — MySQL.
* Flat File — базы данных с двумерными файлами, в которых содержатся записи одного типа и отсутствует связь с другими файлами, как в реляционных. Пример — Excel.
* Иерархические подразумевают наличие записей, связанных друг с другом по принципу отношений один-к-одному или один-ко-многим. А вот для отношений многие-ко-многим следует использовать реляционную модель. Пример — Adabas.
* Сетевые похожи на иерархические, но в этом случае «ребёнок» может иметь несколько «родителей» и наоборот. Примеры — IDS и IDMS.
* Объектно-ориентированные СУБД работают с базами данных, которые состоят из объектов, используемых в ООП. Объекты группируются в классы и называются экземплярами, а классы в свою очередь взаимодействуют через методы. Пример — Versant.
* Объектно-реляционные обладают преимуществами реляционной и объектно-ориентированной моделей. Пример — IBM Db2.
* Многомерная модель является разновидностью реляционной и использует многомерные структуры. Часто представляется в виде кубов данных. Пример — Oracle Essbase.
* Гибридные состоят из двух и более типов баз данных. Используются в том случае, если одного типа недостаточно для обработки всех запросов. Пример — Altibase HDВ.

## Что такое первичный ключ?
<a id="primary-key"></a>
([наверх](#sections))

Первичный ключ или ```PRIMARY KEY``` предназначен для однозначной идентификации каждой записи в таблице и является строго уникальным (```UNIQUE```): две записи таблицы не могут иметь одинаковые значения первичного ключа. Нулевые значения (```NULL```) в ```PRIMARY KEY``` не допускаются. Если в качестве ```PRIMARY KEY``` используется несколько полей, их называют составным ключом.  
Первичный ключ, используется в качестве основного ключа и может быть использован для связи с дочерней таблицей, содержащей внешний ключ.

Пример:
```
CREATE TABLE USERS (
  id INT NOT NULL,
  name VARCHAR (20) NOT NULL,
  PRIMARY KEY (id)
);
```
Здесь в качестве первичного ключа используется поле id.

## Что такое внешний ключ?
<a id="external-key"></a>
([наверх](#sections))

Внешний ключ или ```FOREIGN KEY``` также является атрибутом ограничения и обеспечивает связь двух таблиц. По сути, это поле или несколько полей, которые ссылаются на ```PRIMARY KEY``` в родительской таблице.

Пример использования:
```
CREATE TABLE order (
  order_id INT NOT NULL,
  user_id INT,
  PRIMARY KEY (order_id),
  FOREIGN KEY (user_id) REFERENCES users(id)
);
```
В данном случае внешний ключ, привязанный к полю ```user_id``` в таблице ```order```, ссылается на первичный ключ ```id``` в таблице ```users```, и именно по этим полям происходит связывание двух таблиц.

## Ограничения в SQL
<a id="limitations-in-SQL"></a>
([наверх](#sections))

SQL-ограничения (constraints) указываются при создании или изменении таблицы. Это правила для ограничения типа данных, которые могут храниться в таблице. Действие с данными не будет выполнено, если нарушаются установленные ограничения.

* ```UNIQUE``` — гарантирует уникальность значений в столбце;
* ```NOT NULL``` — значение не может быть ```NULL```;
* ```INDEX``` — создаёт индексы в таблице для быстрого поиска/запросов;
* ```CHECK``` — значения столбца должны соответствовать заданным условиям;
* ```DEFAULT``` — предоставляет столбцу значения по умолчанию.

## Что такое Self JOIN?
<a id="self-join"></a>
([наверх](#sections))

Это выражение используется для того, чтобы таблица объединилась сама с собой, словно это две разные таблицы. Чтобы такое реализовать, одна из таких «таблиц» временно переименовывается.

Следующий SQL-запрос объединяет клиентов из одного города:
```
SELECT A.CustomerName AS CustomerName1, B.CustomerName AS CustomerName2, A.City
FROM Customers A, Customers B
WHERE A.CustomerID <> B.CustomerID
AND A.City = B.City
ORDER BY A.City;
```
## Подстановочные знаки
<a id="wildcards"></a>
([наверх](#sections))

Это специальные символы, которые нужны для замены каких-либо знаков в запросе. Они используются вместе с оператором ```LIKE```, с помощью которого можно отфильтровать запрашиваемые данные.

% — заменить ноль или более символов;
_ — заменить один символ.
Примеры:
```
SELECT * FROM user WHERE name LIKE '%test%';
```
Данный запрос позволяет найти данные всех пользователей, имена которых содержат в себе «test».
```
SELECT * FROM user WHERE name LIKE 't_est';
```
В этом случае имена искомых пользователей начинаются на «t», после содержат какой-либо символ и «est» в конце.

## Что делают псевдонимы Aliases?
<a id="aliases"></a>
([наверх](#sections))

SQL-псевдонимы нужны для того, чтобы дать временное имя таблице или столбцу. Это нужно, когда в запросе есть таблицы или столбцы с неоднозначными именами. В этом случае для удобства в составлении запроса используются псевдонимы. SQL-псевдоним существует только на время запроса.

Пример:
```
SELECT very_long_column_name AS alias_name
FROM table;
```
## Разница между командами DELETE и TRUNCATE
<a id="difference-between-DELETE-and-TRUNCATE"></a>
([наверх](#sections))

Команда ```DELETE``` — это DML-операция, которая удаляет записи из таблицы, соответствующие заданному условию:

```DELETE FROM table_name WHERE condition;```
При этом создаются логи удаления, то есть операцию можно отменить.

А вот команда ```TRUNCATE``` — это DDL-операция, которая полностью пересоздаёт таблицу, и отменить такое удаление невозможно:

```TRUNCATE TABLE table_name;```

DML (Data Manipulation Language) - язык манипулирования данными. Язык DML позволяет осуществлять манипуляции с данными таблиц, т.е. с ее строками. Он позволяет делать выборку данных из таблиц, добавлять новые данные в таблицы, а так же обновлять и удалять существующие данные.  

DDL (Data Definition Language) - язык описания данных. Язык DDL служит для создания и модификации структуры БД, т.е. для создания/изменения/удаления таблиц и связей.

## Разница между WHERE и HAVING
<a id="difference-between-WHERE-and-HAVING"></a>
([наверх](#sections))

В сущности, HAVING очень похож на WHERE - это тоже фильтр. Вы можете написать в HAVING name = ‘Anna’, как и в WHERE, и ошибки не будет. 

```
SELECT username, COUNT(*)
FROM table
WHERE username = ‘Anna’
GROUP BY username
HAVING COUNT(*)>1
```

В ```HAVING``` и только в нём можно писать условия по агрегатным функциям (```SUM```, ```COUNT```, ```MAX```, ```MIN``` и т. д.). То есть если вы хотите сделать что-то вроде ```COUNT(*) > 10```, то это возможно сделать только в ```HAVING```. 

Почему бы не оставить только ```HAVING```? Всё кроется в том, как SQL Server выполняет запрос, в каком порядке происходит его разбор и работа с данными. ```WHERE``` выполняется до формирования групп ```GROUP BY```. Это нужно для того, чтобы можно было оперировать как можно меньшим количеством данных и сэкономить ресурсы сервера и время пользователя. 

Следующим этапом формируются группы, которые указаны в ```GROUP BY```. После того как сформированы группы, можно накладывать условия на результаты агрегатных функций. И тут как раз наступает очередь ```HAVING```: выполняются условия, которые вы задали. 

Главное отличие ```HAVING``` от ```WHERE``` в том, что в ```HAVING``` можно наложить условия на результаты группировки, потому что порядок исполнения запроса устроен таким образом, что на этапе, когда выполняется ```WHERE```, ещё нет групп, а ```HAVING``` выполняется уже после формирования групп. 

## Основные команды SQL
<a id="basic-sql-commands"></a>
([наверх](#sections))

* ```SHOW DATABASES```  

  SQL-команда, которая отвечает за просмотр доступных баз данных.
  
* ```CREATE DATABASE```  
  
  Команда для создания новой базы данных.
  
* ```USE```  

  С помощью этой SQL-команды ```USE <database_name>``` выбирается база данных, необходимая для дальнейшей работы с ней.
  
* ```SOURCE```  

  ```SOURCE <file.sql>``` позволит выполнить сразу несколько SQL-команд, содержащихся в файле с расширением .sql.
  
* ```DROP DATABASE```  
  
  Стандартная SQL-команда для удаления целой базы данных.
  
* ```DROP TABLE```  

  Так можно удалить всю таблицу целиком.  
  
* ```DELETE```  

  SQL-команда ```DELETE FROM <table_name>``` используется для удаления данных из таблицы.
  
* ```SHOW TABLES```  

  С помощью этой команды можно увидеть все таблицы, которые доступны в базе данных.
  
* ```DESCRIBE```  
  
  С помощью ```DESCRIBE <table_name>``` можно просмотреть различные сведения (тип значений, является ключом или нет) о столбцах таблицы.
  
* ```CREATE TABLE```
  SQL-команда для создания новой таблицы:
  ```
  CREATE TABLE <table_name1> (
    <col_name1><col_type1>,
    <col_name2><col_type2>,
    <col_name3><col_type3>
    PRIMARY KEY(<col_name1>),
    FOREIGN KEY(<col_name2>) REFERENCES <table_name2>(<col_name2>)
  );
  ```
  Ограничения целостности при использовании ```CREATE TABLE```  
  Может понадобиться создать ограничения для определённых столбцов в таблице. При создании таблицы можно задать следующие ограничения:

  * ячейка таблицы не может иметь значение ```NULL```;
  * первичный ключ — ```PRIMARY KEY(col_name1, col_name2, …)```;
  * внешний ключ — ```FOREIGN KEY(col_namex1, …, col_namexn) REFERENCES table_name(col_namex1, …, col_namexn)```.  
  
  Можно задать больше одного первичного ключа. В этом случае получится составной первичный ключ.

  Пример
  Создайте таблицу «instructor»:
```
  CREATE TABLE instructor (
    ID CHAR(5),
    name VARCHAR(20) NOT NULL,
    dept_name VARCHAR(20),
    salary NUMERIC(8,2),
    PRIMARY KEY (ID),
    FOREIGN KEY (dept_name) REFERENCES department(dept_name)
  );
```

* UNION  

  Он используется для объединения полученных данных из двух или более запросов, которые должны иметь одинаковое количество столбцов с одинаковыми типами данных и расположенных в   том же порядке.  

  Пример использования:
  ```
  SELECT column(s) FROM first_table
  UNION
  SELECT column(s) FROM second_table;
  ```
  
* ```INSERT```  

  Команда ```INSERT INTO <table_name>``` в SQL отвечает за добавление данных в таблицу:
  ```
  INSERT INTO <table_name> (<col_name1>, <col_name2>, <col_name3>, …)
    VALUES (<value1>, <value2>, <value3>, …); 
  ```
  При добавлении данных в каждый столбец таблицы не требуется указывать названия столбцов.
  ```
  INSERT INTO <table_name>
    VALUES (<value1>, <value2>, <value3>, …);
  ```
  
* ```UPDATE```  

  SQL-команда для обновления данных таблицы:
  ```
  UPDATE <table_name>
    SET <col_name1> = <value1>, <col_name2> = <value2>, ...
    WHERE <condition>;
  ```
  
* ```SELECT```  
  
  ```SELECT``` используется для получения данных из выбранной таблицы:
  ```
  SELECT <col_name1>, <col_name2>, …
    FROM <table_name>; 
  ```
  Следующей командой можно вывести все данные из таблицы:
  ```
  SELECT * FROM <table_name>;
  ```
  
* ```SELECT DISTINCT```
 
  В столбцах таблицы могут содержаться повторяющиеся данные. ```SELECT DISTINCT``` используется для получения только неповторяющихся данных.
  ```
  SELECT DISTINCT <col_name1>, <col_name2>, …
    FROM <table_name>; 
  ```

* ```WHERE```  
  
  Можно использовать ключевое слово ```WHERE``` в ```SELECT``` для указания условий в запросе:
  ```
  SELECT <col_name1>, <col_name2>, …
    FROM <table_name>
    WHERE <condition>; 
  ```
  В запросе можно задавать следующие условия:

  сравнение текста;
  сравнение численных значений;
  логические операции AND (и), OR (или) и NOT (отрицание).
  Пример:
  ```
  SELECT * FROM table WHERE author='Достоевский';
  SELECT * FROM table WHERE price>3000;
  SELECT * FROM table WHERE amount=10; 
  ```
  
* ```GROUP BY```  
  
  Оператор ```GROUP BY``` часто используется с агрегатными функциями, такими как ```COUNT```, ```MAX```, ```MIN```, ```SUM``` и ```AVG```, для группировки выходных значений.
  ```
  SELECT <col_name1>, <col_name2>, …
    FROM <table_name>
    GROUP BY <col_namex>; 
  ```
  Пример
  Выведем количество курсов для каждого факультета:
  ```
  SELECT COUNT(course_id), dept_name
    FROM course
    GROUP BY dept_name; 
  ```
  
* ```HAVING```  
  
  Ключевое слово ```HAVING``` было добавлено в SQL по той причине, что ```WHERE``` не может использоваться для работы с агрегатными функциями.
  ```
  SELECT <col_name1>, <col_name2>, ...
    FROM <table_name>
    GROUP BY <column_namex>
    HAVING <condition> 
  ```
  Пример
  Выведем список факультетов, у которых более одного курса:
  ```
  SELECT COUNT(course_id), dept_name
    FROM course
    GROUP BY dept_name
    HAVING COUNT(course_id)>1; 
  ```
  
* ```ORDER BY```  
  
  ```ORDER BY``` используется для сортировки результатов запроса по убыванию или возрастанию. ```ORDER BY``` отсортирует по возрастанию, если не будет указан способ сортировки ```ASC``` или ```DESC```.
  ```
  SELECT <col_name1>, <col_name2>, …
    FROM <table_name>
    ORDER BY <col_name1>, <col_name2>, … ASC|DESC;
  ```
  Пример
  Выведем список курсов по возрастанию и убыванию количества кредитов:
  ```
  SELECT * FROM course ORDER BY credits;
  SELECT * FROM course ORDER BY credits DESC;
  ```
  
* ```BETWEEN```  

  ```BETWEEN``` используется для выбора значений данных из определённого промежутка. Могут быть использованы числовые и текстовые значения, а также даты.
  ```
  SELECT <col_name1>, <col_name2>, …
    FROM <table_name>
    WHERE <col_namex> BETWEEN <value1> AND <value2>;
  ```
  Пример
  Выведем список инструкторов, чья зарплата больше 50 000, но меньше 100 000:
  ```
  SELECT * FROM instructor
    WHERE salary BETWEEN 50000 AND 100000; 
  ```

* ```LIKE```  

  Оператор ```LIKE``` используется в ```WHERE```, чтобы задать шаблон поиска похожего значения.

  Есть два свободных оператора, которые используются в ```LIKE```:

  % (ни одного, один или несколько символов);
  _ (один символ).
  ```
  SELECT <col_name1>, <col_name2>, …
    FROM <table_name>
    WHERE <col_namex> LIKE <pattern>; 
  ```
  Пример
  Выведем список курсов, в имени которых содержится «to», и список курсов, название которых начинается с «CS-»:
  ```
  SELECT * FROM course WHERE title LIKE ‘%to%’;
  SELECT * FROM course WHERE course_id LIKE 'CS-___';
  ```

* ```IN```  
  
  С помощью ```IN``` можно указать несколько значений для оператора ```WHERE```:
  ```
  SELECT <col_name1>, <col_name2>, …
    FROM <table_name>
    WHERE <col_namen> IN (<value1>, <value2>, …);
  ```
  Пример
  Выведем список студентов с направлений Comp. Sci., Physics и Elec. Eng.:
  ```
  SELECT * FROM student
    WHERE dept_name IN (‘Comp. Sci.’, ‘Physics’, ‘Elec. Eng.’);
  ```
  
* ```JOIN```  

  ```JOIN``` используется для связи двух или более таблиц с помощью общих атрибутов внутри них.
  Чтобы объединить две таблицы в одну, следует использовать оператор ```JOIN```. Соединение таблиц может быть внутренним (```INNER```) или внешним (```OUTER```), причём внешнее соединение может быть левым (```LEFT```), правым (```RIGHT```) или полным (```FULL```).

  * ```INNER JOIN``` — получение записей с одинаковыми значениями в обеих таблицах, т.е. получение пересечения таблиц.
  * ```FULL OUTER JOIN``` — объединяет записи из обеих таблиц (если условие объединения равно true) и дополняет их всеми записями из обеих таблиц, которые не имеют совпадений. Для записей, которые не имеют совпадений из другой таблицы, недостающее поле будет иметь значение NULL.
  * ```LEFT JOIN``` — возвращает все записи, удовлетворяющие условию объединения, плюс все оставшиеся записи из внешней (левой) таблицы, которые не удовлетворяют условию объединения.
  * ```RIGHT JOIN``` — работает точно так же, как и левое объединение, только в качестве внешней таблицы будет использоваться правая.
  ```
  SELECT <col_name1>, <col_name2>, …
  FROM <table_name1>
  JOIN <table_name2>
  ON <table_name1.col_namex> = <table2.col_namex>;
  ```
  Следующий запрос выбирает все заказы с информацией о клиенте:
  ```
  SELECT Orders.OrderID, Customers.CustomerName
  FROM Orders
  INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID;
  ```
  
* ```VIEW```  
  
  ```VIEW``` — это виртуальная таблица SQL, созданная в результате выполнения выражения. Она содержит строки и столбцы и очень похожа на обычную SQL-таблицу. VIEW всегда показывает самую свежую информацию из базы данных.

  Создание
  ```
  CREATE VIEW <view_name> AS
    SELECT <col_name1>, <col_name2>, …
    FROM <table_name>
    WHERE <condition>;
  ```
  Удаление
  ```
  DROP VIEW <view_name>;
  ```

## Математические функции в SQL
<a id="math-functions"></a>
([наверх](#sections))

|**Функция**|	**Описание**	|**Пример**|
|:------------------------------|:--------------------------|:--------------------------|
| ```CEILING(x)``` |	возвращает наименьшее целое число, большее или равное x(округляет до целого числа в большую сторону)|```	CEILING(4.2)=5 CEILING(-5.8)=-5```|
| ```ROUND(x, k)``` |	округляет значение x до k знаков после запятой, если k не указано – x округляется до целого	|```ROUND(4.361)=4 ROUND(5.86592,1)=5.9```|
| ```FLOOR(x)``` |	возвращает наибольшее целое число, меньшее или равное x (округляет до  целого числа в меньшую сторону)	|```FLOOR(4.2)=4 FLOOR(-5.8)=-6```|
| ```POWER(x, y)``` |	возведение x в степень y	|```POWER(3,4)=81.0```|
| ```SQRT(x)``` |	квадратный корень из x	|```SQRT(4)=2.0 SQRT(2)=1.41...```|
| ```DEGREES(x)``` |	конвертирует значение x из радиан в градусы	|```DEGREES(3) = 171.8...```|
| ```RADIANS(x)``` |	конвертирует значение x из градусов в радианы	|```RADIANS(180)=3.14...```|
| ```ABS(x)``` |	модуль числа x	|```ABS(-1) = 1 ABS(1) = 1```|
| ```PI()``` |	pi = 3.1415926...	 ||
| ```COUNT(col_name)``` | возвращает количество строк||
| ```SUM(col_name)``` | возвращает сумму значений в данном столбце||
| ```AVG(col_name)``` | возвращает среднее значение данного столбца||
| ```MIN(col_name)``` | возвращает наименьшее значение данного столбца||
| ```MAX(col_name)``` | возвращает наибольшее значение данного столбца||

## Оконные функции в SQL
<a id="window-functions"></a>
([наверх](#sections))

Оконная функция выполняет вычисления для набора строк, некоторым образом связанных с текущей строкой. Можно сравнить её с агрегатной функцией, но, в отличие от обычной агрегатной функции, при использовании оконной функции несколько строк не группируются в одну, а продолжают существовать отдельно. Внутри же, оконная функция, как и агрегатная, может обращаться не только к текущей строке результата запроса.

Вот пример, показывающий, как сравнить зарплату каждого сотрудника со средней зарплатой его отдела:
```
SELECT depname, empno, salary, avg(salary) OVER (PARTITION BY depname)
  FROM empsalary;
```

```
  depname  | empno | salary |          avg          
-----------+-------+--------+-----------------------
 develop   |    11 |   5200 | 5020.0000000000000000
 develop   |     7 |   4200 | 5020.0000000000000000
 develop   |     9 |   4500 | 5020.0000000000000000
 develop   |     8 |   6000 | 5020.0000000000000000
 develop   |    10 |   5200 | 5020.0000000000000000
 personnel |     5 |   3500 | 3700.0000000000000000
 personnel |     2 |   3900 | 3700.0000000000000000
 sales     |     3 |   4800 | 4866.6666666666666667
 sales     |     1 |   5000 | 4866.6666666666666667
 sales     |     4 |   4800 | 4866.6666666666666667
(10 rows)
```

Первые три столбца извлекаются непосредственно из таблицы empsalary, при этом для каждой строки таблицы есть строка результата. В четвёртом столбце оказалось среднее значение, вычисленное по всем строкам, имеющим то же значение depname, что и текущая строка. (Фактически среднее вычисляет та же функция avg, которую мы знаем как агрегатную, но предложение ```OVER``` превращает её в оконную, так что она обрабатывает лишь заданный набор строк.)

Вызов оконной функции всегда содержит предложение ```OVER```, следующее за названием и аргументами оконной функции. Это синтаксически отличает её от обычной или агрегатной функции. Предложение ```OVER``` определяет, как именно нужно разделить строки запроса для обработки оконной функцией. Предложение ```PARTITION BY```, дополняющее ```OVER```, указывает, что строки нужно разделить по группам или разделам, объединяя одинаковые значения выражений ```PARTITION BY```. Оконная функция вычисляется по строкам, попадающим в один раздел с текущей строкой.

Вы можете также определять порядок, в котором строки будут обрабатываться оконными функциями, используя ```ORDER BY``` в ```OVER```. (Порядок ```ORDER BY``` для окна может даже не совпадать с порядком, в котором выводятся строки.) Например:
```
SELECT depname, empno, salary,
       rank() OVER (PARTITION BY depname ORDER BY salary DESC)
FROM empsalary;
```
```
  depname  | empno | salary | rank 
-----------+-------+--------+------
 develop   |     8 |   6000 |    1
 develop   |    10 |   5200 |    2
 develop   |    11 |   5200 |    2
 develop   |     9 |   4500 |    4
 develop   |     7 |   4200 |    5
 personnel |     2 |   3900 |    1
 personnel |     5 |   3500 |    2
 sales     |     1 |   5000 |    1
 sales     |     4 |   4800 |    2
 sales     |     3 |   4800 |    2
(10 rows)
```
Как показано здесь, функция rank выдаёт порядковый номер в разделе текущей строки для каждого уникального значения, по которому выполняет сортировку предложение ORDER BY. У функции rank нет параметров, так как её поведение полностью определяется предложением ```OVER``````.

Строки, обрабатываемые оконной функцией, представляют собой «виртуальные таблицы», созданные из предложения FROM и затем прошедшие через фильтрацию и группировку ```WHERE``` и ```GROUP BY``` и, возможно, условие ```HAVING```. Например, строка, отфильтрованная из-за нарушения условия ```WHERE```, не будет видна для оконных функций. Запрос может содержать несколько оконных функций, разделяющих данные по-разному с помощью разных предложений ```OVER```, но все они будут обрабатывать один и тот же набор строк этой виртуальной таблицы.

Мы уже видели, что ```ORDER BY``` можно опустить, если порядок строк не важен. Также возможно опустить ```PARTITION BY```, в этом случае будет только один раздел, содержащий все строки.

Есть ещё одно важное понятие, связанное с оконными функциями: для каждой строки существует набор строк в её разделе, называемый рамкой окна. По умолчанию, с указанием ORDER BY рамка состоит из всех строк от начала раздела до текущей строки и строк, равных текущей по значению выражения ```ORDER BY```. Без ```ORDER BY``` рамка по умолчанию состоит из всех строк раздела. Посмотрите на пример использования sum:
```
SELECT salary, sum(salary) OVER () FROM empsalary;
```
```
 salary |  sum  
--------+-------
   5200 | 47100
   5000 | 47100
   3500 | 47100
   4800 | 47100
   3900 | 47100
   4200 | 47100
   4500 | 47100
   4800 | 47100
   6000 | 47100
   5200 | 47100
(10 rows)
```
Так как в этом примере нет указания ```ORDER BY``` в предложении ```OVER```, рамка окна содержит все строки раздела, а он, в свою очередь, без предложения ```PARTITION BY``` включает все строки таблицы; другими словами, сумма вычисляется по всей таблице и мы получаем один результат для каждой строки результата. Но если мы добавим ```ORDER BY```, мы получим совсем другие результаты:
```
SELECT salary, sum(salary) OVER (ORDER BY salary) FROM empsalary;
```
```
 salary |  sum  
--------+-------
   3500 |  3500
   3900 |  7400
   4200 | 11600
   4500 | 16100
   4800 | 25700
   4800 | 25700
   5000 | 30700
   5200 | 41100
   5200 | 41100
   6000 | 47100
(10 rows)
```
Здесь в сумме накапливаются зарплаты от первой (самой низкой) до текущей, включая повторяющиеся текущие значения (обратите внимание на результат в строках с одинаковой зарплатой).

Оконные функции разрешается использовать в запросе только в списке ```SELECT``` и предложении ```ORDER BY```. Во всех остальных предложениях, включая ```GROUP BY```, ```HAVING``` и ```WHERE```, они запрещены. Это объясняется тем, что логически они выполняются после обычных агрегатных функций, а значит агрегатную функцию можно вызвать из оконной, но не наоборот.

Если вам нужно отфильтровать или сгруппировать строки после вычисления оконных функций, вы можете использовать вложенный запрос. Например:
```
SELECT depname, empno, salary, enroll_date
FROM
  (SELECT depname, empno, salary, enroll_date,
    rank() OVER (PARTITION BY depname ORDER BY salary DESC, empno) AS pos
   FROM empsalary
  ) AS ss
WHERE pos < 3;
```
Данный запрос покажет только те строки внутреннего запроса, у которых rank (порядковый номер) меньше 3.

Когда в запросе вычисляются несколько оконных функций для одинаково определённых окон, конечно можно написать для каждой из них отдельное предложение ```OVER```, но при этом оно будет дублироваться, что неизбежно будет провоцировать ошибки. Поэтому лучше определение окна выделить в предложение ```WINDOW```, а затем ссылаться на него в ```OVER```. Например:
```
SELECT sum(salary) OVER w, avg(salary) OVER w
  FROM empsalary
  WINDOW w AS (PARTITION BY depname ORDER BY salary DESC);
```
<br/>

# МатСтат и ТерВер
<a id="statistics-and-probability-theory"></a>
([наверх](#sections))

* [Меры центральной тенденции](#сentral-trend-measures)
* [Процентили и квартили](#percentile-and-quartile)
* [Математическое ожидание](#expected-value)
* [Меры изменчивости](#Measures-of-variability)

## Меры центральной тенденции
<a id="сentral-trend-measures"></a>

Используются, когда вам нужно отразить наиболее типичные значения, присутствующие в вашей
выборке.
Состав:
  1. Мода – наиболее часто встречающееся значение.
  2. Медиана – середина упорядоченного ряда значений.
  3. Среднее арифметическое – сумма значений, деленная на их количество.

Пример: определение наиболее типичной зарплаты в нашей стране можно осуществлять по двум показателям – среднему арифметическому и медиане. Первая определяется как количество денег, деленное на количество людей, а второе – как зарплата человека, стоящего ровно посередине между самым бедным и самым богатым. Как правило, эти значения различаются – средняя зарплата выше медианной. И чем это различие больше, тем выше социальное неравенство в обществе.

**Мода** — значение во множестве наблюдений, которое встречается наиболее часто. (Мода = типичность.) Иногда в совокупности встречается более чем одна мода (например: 6, 2, 6, 6, 8, 9, 9, 9, 0; мода — 6 и 9). В этом случае можно сказать, что совокупность мультимодальна. Из структурных средних величин только мода обладает таким уникальным свойством. Как правило, мультимодальность указывает на то, что набор данных не подчиняется нормальному распределению. Мода как средняя величина употребляется чаще для данных, имеющих нечисловую природу.

Для интервального ряда мода определяется по формуле:

![мода для интервального ряда](http://www.univer-nn.ru/statistics/moda.jpg)

здесь XMо — левая граница модального интервала, hМо — длина модального интервала, fМо − 1 — частота премодального интервала, fМо — частота модального интервала, fМо + 1 — частота послемодального интервала.

Модой абсолютно непрерывного распределения называют любую точку локального максимума плотности распределения. Для дискретных распределений модой считают любое значение ai, вероятность которого pi больше, чем вероятности соседних значений.

**Медиана** — это такое значение признака, которое разделяет ранжированный ряд распределения на две равные части — со значениями признака меньше медианы и со значениями признака больше медианы. Для нахождения медианы, нужно отыскать значение признака, которое находится на середине упорядоченного ряда.

В ранжированных рядах несгруппированные данные для нахождения медианы сводятся к поиску порядкового номера медианы. Медиана может быть вычислена по следующей формуле:

![медиана](http://www.univer-nn.ru/statistics/mediana.jpg)

где Хm — нижняя граница медианного интервала;  
im — медианный интервал;  
Sme— сумма наблюдений, которая была накоплена до начала медианного интервала;  
fme — число наблюдений в медианном интервале.  

* Медиана не зависит от тех значений признака, которые расположены по обе стороны от нее.  
* Аналитические операции с медианой весьма ограничены, поэтому при объединении двух распределений с известными медианами невозможно заранее предсказать величину медианы нового распределения.  
* Медиана обладает свойством минимальности. Его суть заключается в том, что сумма абсолютных отклонений значений х, от медианы представляет собой минимальную величину по сравнению с отклонением X от любой другой величины

**Среднее арифметическое** — такое среднее значение признака, при вычислении которого общий объем признака в совокупности сохраняется неизменным. Для того чтобы вычислить среднюю арифметическую, необходимо сумму всех значений признаков разделить на их число.

Оно применяется в тех случаях, когда объем варьирующего признака для всей совокупности является суммой значений признаков отдельных ее единиц. 

Среднее арифметическое может быть вычислена по формуле:

![ср.ар](http://www.univer-nn.ru/statistics/arifmet-simple.jpg)

где n — численность совокупности.  

Средняя арифметическая взвешенная — это средняя из вариантов, которые повторяются разное число раз или имеют различный вес. Она может быть рассчитана по формуле:

![ср.ар.взв](http://www.univer-nn.ru/statistics/arifmet-vzvesh.jpg)

## Процентили и квартили
<a id="percentile-and-quartile"></a>
([наверх](#sections))

Квантииль в математической статистике — значение, которое заданная случайная величина не превышает с фиксированной вероятностью. Если вероятность задана в процентах, то квантиль называется процентилем или перцентилем 

**Процентиль** — мера, в которой процентное значение общих значений равно этой мере или меньше ее. Например, 90 % значений данных находятся ниже 90-го процентиля, а 10 % значений данных находятся ниже 10-го процентиля.

**Квартили** — значения, которые делят таблицу данных (или ее часть) на четыре группы, содержащие приблизительно равное количество наблюдений. Общий объем делится на четыре равные части: 25%, 50%, 75% 100%.

* 0,25-квантиль называется первым (или нижним) кварти́лем (от лат. quarta — четверть);
* 0,5-квантиль называется медианой (от лат. mediāna — середина) или вторым кварти́лем;
* 0,75-квантиль называется третьим (или верхним) кварти́лем.

Процентиль можно пояснить и на примере симметричного распределения Гаусса, которое часто встречается в статистике для оценки веса, роста и т.п. 

![ImageGauss](https://investprofit.info/wp-content/uploads/2020/06/ImageGauss.gif)

На рисунке выше показаны 25, 50, 75 и 100 процентили. Случаи 25 и 75-ого процентиля, включающие четверть и три четверти выборки соответственно, называются квартилями.

Интеркварти́льным размахом (англ. Interquartile range) называется разность между третьим и первым квартилями, то есть ![f-la](https://wikimedia.org/api/rest_v1/media/math/render/svg/99e771cbcd10f208efd9cabd46f266a29a21c49e). Интерквартильный размах является характеристикой разброса распределения величины и является робастным аналогом дисперсии. Вместе, медиана и интерквартильный размах могут быть использованы вместо математического ожидания и дисперсии в случае распределений с большими выбросами, либо при невозможности вычисления последних.

**Дециль** характеризует распределение величин совокупности, при котором девять значений дециля делят её на десять равных частей. Любая из этих десяти частей составляет 1/10 всей совокупности. Так, первый дециль отделяет 10 % наименьших величин, лежащих ниже дециля, от 90 % наибольших величин, лежащих выше дециля.

Так же, как в случае моды и медианы, у интервального вариационного ряда распределения каждый дециль (и квартиль) принадлежит определённому интервалу и имеет вполне определённое значение.

## Математическое ожидание
<a id="expected-value"></a>
([наверх](#sections))

Математическое ожидание — означает среднее (взвешенное по вероятностям возможных значений) значение случайной величины. В случае непрерывной случайной величины подразумевается взвешивание по плотности распределения. Математическое ожидание случайного вектора равно вектору, компоненты которого равны математическим ожиданиям компонентов случайного вектора.

Математическое ожидание – это сумма произведений всех возможных значений случайной величины на вероятности этих значений.

Обозначается через ![матож1](https://wikimedia.org/api/rest_v1/media/math/render/svg/09de7acbba84104ff260708b6e9b8bae32c3fafa) в русскоязычной литературе также встречается обозначение ![матож2](https://wikimedia.org/api/rest_v1/media/math/render/svg/87b00856eb008c4ea9bc42894bb2bfa0b8605ac2). В статистике часто используют обозначение ![mu](https://wikimedia.org/api/rest_v1/media/math/render/svg/9fd47b2a39f7a7856952afec1f1db72c67af6161).

Для случайной величины, принимающей значения только 0 или 1 математическое ожидание равно p — вероятности «единицы». Математическое ожидание суммы таких случайных величин равно np, где n — количество таких случайных величин. При этом вероятности появления определенного кол-ва единиц рассчитываются по биномиальному распределению. Некоторые случайные величины не имеют математического ожидания, например, случайные величины, имеющие распределение Коши.

На практике математическое ожидание обычно оценивается как среднее арифметическое наблюдаемых значений случайной величины (выборочное среднее, среднее по выборке). 

## Меры изменчивости
<a id="Measures-of-variability"></a>
([наверх](#sections))

Используются, когда нужно отразить степень разброса значений относительно меры центральной
тенденции.
Состав:
  1. Размах – разность между максимальным и минимальным значениями.
  2. Дисперсия – сумма квадратов отклонений, деленная на их количество. Отклонение – это разность между средним арифметическим и конкретным значением. Дисперсии для генеральной совокупности и для выборки вычисляются по разным формулам.
  3. Стандартное отклонение – корень из дисперсии.

**Размах** — разность между наибольшим и наименьшим значениями результатов наблюдений. Пусть ![f-la3](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac794f5521dcce89913085a6d566e7cdb615dbb0) — взаимно независимые случайные величины с функцией распределения F(x) и плотностью вероятности f(x). В этом случае размах Wn определяется как разность между наибольшим и наименьшим значениями среди ![f-la3](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac794f5521dcce89913085a6d566e7cdb615dbb0); размах Wn представляет собой случайную величину, которой соответствует функция распределения:

![Размах](https://wikimedia.org/api/rest_v1/media/math/render/svg/4830e22fab4992a6f37ebc9423dc892b419bc08d)

(при w >= 0; если w < 0, то P {W <= w} = 0).
В математической статистике размах, надлежащим образом нормированный, применяется как оценка неизвестного квадратичного отклонения. Например, если Xk имеют нормальное распределение с параметрами (а, s), то при n = 5 и 10, соответственно, величины 0,4299W5 и 0,3249W10 будут несмещенными оценками s. Такие оценки часто используют при статистическом контроле качества, поскольку определение Р. нескольких результатов измерений не требует сложных вычислений.

**Дисперсией** случайной величины называют математическое ожидание квадрата отклонения случайной величины от её математического ожидания.

Пусть X — случайная величина, определённая на некотором вероятностном пространстве. Тогда дисперсией называется

![Дисперсия](https://wikimedia.org/api/rest_v1/media/math/render/svg/59b03d2df3d9b588fcda27ed5489de5541d56844)
где символ M обозначает математическое ожидание

# Задачи и алгоритмы машинного обучения
<a id="Machine-learning-tasks-and-algorithms"></a>

* [Задачи машинного обучения](#machine-learning-tasks)  
  * [Что такое обучение с учителем?](#supervised-learning)  
  * [Что такое обучение без учителя?](#unsupervised-learning)  
  * [Что такое классификация?](#classification)  
  * [Зачем использовать отбор (или селекцию) признаков (Feature selection)?](#Feature-selection)  
  * [Основные метрики задач классификации](#classification-metrics)
  * [Что такое кластеризация?](#clustering)  
  * [Что такое понижение размерности?](#dimension-reduction)  
  * [Что такое смещение и дисперсия, и каковы их отношения в моделировании данных?](#bias-and-variance)  
* [Алгоритмы машинного обучения](#machine-learning-algorithms)  
    * [Линейная регрессия](#linear-regression)  
    * [Логистическая регрессия](#logistic-regression)  
    * [Линейный дискриминантный анализ (LDA)](#LDA)
    * [Случайный лес (Random Forest)](#Random-Forest)
    * [Метод опорных векторов (SVM)](#SVM)
    * [K-ближайших соседей (KNN)](#KNN)
    * [Наивный Байесовский классификатор](#Bayes)
* [Бустинг](#boosting)  
* [Метрики регрессии](#Regression-metrics)
* [Обучение с помощью градиентного спуска](#gradient-descent-training)
* [Алгоритмы кластеризации такие как k-means и c-means, dbscan](#k-means-c-means-dbscan)
* [Иерархическая кластеризация](#hierarchical-clustering)
* [Алгоритмы понижения размерности такие как PCA, t-SNE](#lPCA-t-SNE)
* [Что такое регуляризация и чем она полезна?](#regularization)
* [Алгоритмы для нейронных сетей](#algorithms-for-neural-networksn)
* [Полносвязная нейронная сеть](#fully-connected-neural-network)
* [Свёрточная нейронная сеть](#convolutional-neural-networkn)
* [Рекуррентная нейронная сеть](#recurrent-neural-network)
* [Для каких задач подходит тот или иной алгоритм?](#algorithm-suitable)
* [Что такое градиент и для чего он нужен?](#gradient)
* [Что такое функция активации?](#activation-function)

([наверх](#sections))

## Задачи машинного обучения
<a id="machine-learning-tasks"></a>

### Что такое обучение с учителем?
<a id="supervised-learning"></a>
([наверх](#sections))

В данном случае в тренировочном датасете для кажого объекта у нас есть метка или лэйбл. В тестовом - этого лейбла нет и нам нужно его предсказать. 
Бывает два подтипа решения задач с учителем:

* Регрессия

В задачах регрессии меткой является вещественное число.  

Например, если мы предсказываем стоимость квартиры, то это будет задача регрессии.

* Классификация

В данном случае мы предсказываем не вещественное число, а конечно подмножество классов.  
Если таких классов два, то это задача бинарной классификации.
Если больше двух, то это многоклассовая классификация.  

Например, если мы предсказываем есть на фотографии машина или нет, то это задача бинарной классификации, потому-что у нас в данном случае два класса (либо машина есть, либо её нет)

### Что такое обучение без учителя?
<a id="unsupervised-learning"></a>
([наверх](#sections))

В данном случае у нас нет лейблов для объектов, и мы не пытаемся их предсказать.  
Бывает несколько подтипов решения задач без учителя:

* Кластеризация

В данном случае попытаемся выявить некоторые паттерны в данных и объединить их в кластеры. Так, чтобы в одном кластере были похожие объекты, а в разных кластерах они были разные.  

Например, мы хотим разбить фильмы по жанрам и мы не знаем какие у нас жанры есть, мы хотим понять чем они отличаются друг от друга и на какое количество кластеров их хорошо разбить.

* Задача понижения размерности  

В данном случае мы хотим вектора большой размерности ужать до меньшей, при этом сохранить как можно больше информации

* Визуализация

Уменьшаем размерность до состояния, когда её можно визуализировать

### Что такое классификация?
<a id="classification"></a>
([наверх](#sections))

Классификация — один из разделов машинного обучения, посвященный решению следующей задачи. Имеется множество объектов (ситуаций), разделённых некоторым образом на классы. Задано конечное множество объектов, для которых известно, к каким классам они относятся. Это множество называется обучающей выборкой. Классовая принадлежность остальных объектов не известна. Требуется построить алгоритм, способный классифицировать произвольный объект из исходного множества.  

Классифицировать объект — значит, указать номер (или наименование класса), к которому относится данный объект.  

Классификация объекта — номер или наименование класса, выдаваемый алгоритмом классификации в результате его применения к данному конкретному объекту.  

В математической статистике задачи классификации называются также задачами дискриминантного анализа.

В машинном обучении задача классификации относится к разделу обучения с учителем. Существует также обучение без учителя, когда разделение объектов обучающей выборки на классы не задаётся, и требуется классифицировать объекты только на основе их сходства друг с другом. В этом случае принято говорить о задачах кластеризации или таксономии, и классы называть, соответственно, кластерами или таксонами.

**Типология задач классификации**  
_Типы входных данных_
* Признаковое описание — наиболее распространённый случай. Каждый объект описывается набором своих характеристик, называемых признаками. Признаки могут быть числовыми или нечисловыми.  
* Матрица расстояний между объектами. Каждый объект описывается расстояниями до всех остальных объектов обучающей выборки. С этим типом входных данных работают немногие методы, в частности, метод ближайших соседей, метод парзеновского окна, метод потенциальных функций.  
* Временной ряд или сигнал представляет собой последовательность измерений во времени. Каждое измерение может представляться числом, вектором, а в общем случае — признаковым описанием исследуемого объекта в данный момент времени.  
* Изображение или видеоряд.  
* Встречаются и более сложные случаи, когда входные данные представляются в виде графов, текстов, результатов запросов к базе данных, и т. д. Как правило, они приводятся к первому или второму случаю путём предварительной обработки данных и извлечения признаков.  
Классификацию сигналов и изображений называют также распознаванием образов.

_Типы классов_
* Двухклассовая классификация. Наиболее простой в техническом отношении случай, который служит основой для решения более сложных задач.  
* Многоклассовая классификация. Когда число классов достигает многих тысяч (например, при распознавании иероглифов или слитной речи), задача классификации становится существенно более трудной.  
* Непересекающиеся классы.  
* Пересекающиеся классы. Объект может относиться одновременно к нескольким классам.  
* Нечёткие классы. Требуется определять степень принадлежности объекта каждому из классов, обычно это действительное число от 0 до 1.  

### Зачем использовать отбор (или селекцию) признаков (Feature selection)?
<a id="Feature-selection"></a>
([наверх](#sections))

Отбор признаков используют для устранения избыточных признаков (например, которые дублируются) и нерелевантных (например, которые не имеют отношения к решению задачи) признаков, что может

* повысить надёжность обучения (уменьшить эффект переобучения),
* улучшить интерпретируемость решения (модель будет зависеть от небольшого числа признаков),
* упростить поддержку решения (чем меньше входных данных, тем проще понять, из-за каких изменений входа алгоритм стал себя неадекватно вести),
* повысить скорость работы алгоритмов (чем меньше признаков, тем быстрее),
* удешевить решение задачи (часто получение признаков связано с денежными затратами — установка дополнительных датчиков, запросы нужной информации и т.п., здесь мы получаем возможность узнать, что чаcть признаков можно не использовать).

### Основные метрики задач классификации
<a id="classification-metrics"></a>
([наверх](#sections))

Рассматриваемые нами метрики основаны на использовании следующих исходов: истинно положительные (TP), истинно отрицательные (TN), ложно положительные (FP) и ложно отрицательные (FN)

![Performance-measurement](https://www.researchgate.net/profile/Nimmisha-Shajihan/publication/347447352/figure/fig3/AS:970048604741634@1608289018974/Performance-measurement-TP-TN-FP-FN-are-the-parameters-used-in-the-evaluation-of.jpg)

**Accuracy**


Одной из наиболее простых, а поэтому и распространенной метрикой является точность. Она показывает количество правильно проставленных меток класса (истинно положительных и истинно отрицательных) от общего количества данных и считается следующим образом

![Accuracy](https://webiomed.ai/media/ck_uploads/2020/07/08/bezimeni-3_VdaeGz8.png)

Однако, эта простота является также и причиной, почему её часто критикуют и почему она может абсолютно не подойти под решаемую задачу. Она не учитывает соотношения ложных срабатываний модели, что может быть критическим, особенно в медицинской сфере, когда стоит задача распознать все истинные случаи диагноза.

**Precision**

Эта точность показывает количество истинно положительных исходов из всего набора положительных меток и считается по следующей формуле

![Precision](https://webiomed.ai/media/ck_uploads/2020/07/08/bezimeni-4.png)

Важность этой метрики определяется тем, насколько высока для рассматриваемой задачи «цена» ложно положительного результата. Если, например, стоимость дальнейшей проверки наличия заболевания у пациента высока и мы просто не можем проверить все ложно положительные результаты, то стоит максимизировать данную метрику, ведь при Precision = 50% из 100 положительно определенных больных  диагноз будут иметь лишь 50 из них.

**Recall (true positive rate)**

«полнота» или «чувствительность». Эта метрика определяет количество истинно положительных среди всех меток класса, которые были определены как «положительный» и вычисляется по следующей формуле

![Recall](https://webiomed.ai/media/ck_uploads/2020/07/08/bezimeni-5.png)

Необходимо уделить особое внимание этой оценке, когда в поставленной задаче ошибка нераспознания положительного класса высока.

**F1-Score**

В том случае, если Precision и Recall являются одинаково значимыми, можно использовать их среднее гармоническое для получения оценки результатов

![F1-Score](https://webiomed.ai/media/ck_uploads/2020/07/08/bezimeni-9.png)

**ROC**

ROC (receiver operating characteristic) – график, показывающий зависимость верно классифицируемых объектов положительного класса от ложно положительно классифицируемых объектов негативного класса. Иными словами, соотношение True Positive Rate (Recall) и False Positive Rate. 

![roc](https://webiomed.ai/media/ck_uploads/2020/07/08/2.png)

Идеальное значение графика находится в верхней левой точке (TPR = 1, a FPR = 0). При этом, кривая, соответствующая FPR = TPR является случайным гаданием, а если график кривой модели или точка находятся ниже этого минимума, то это говорит лишь о том, что лучше подбрасывать монетку, чем использовать эту модель. При этом говорят, что кривая X доминирует над другой кривой Y, если X в любом точке находится левее и выше Y, что означает превосходство первого классификатора над вторым.

С помощью ROC — кривой, можно сравнить модели, а также их параметры для поиска наиболее оптимальной (с точки зрения tpr и fpr) комбинации. В этом случае ищется компромисс между количеством больных, метка которых была правильно определена как положительная и количеством больных, метка которых была неправильно определена как положительная.

**AUC (Area Under Curve)**

В качестве численной оценки ROC кривой принято брать площадь под этой кривой, которая является неплохим «итогом» для кривой. Если между кривыми X и Y существует доминирование первой над второй, то AUC (X) > AUC (Y), обратное не всегда верно. Но AUC обладает так же и статистическим смыслом: она показывает вероятность того, что случайно выбранный экземпляр негативного класса будет иметь меньше вероятность быть распознанным как позитивный класс, чем случайно выбранный позитивный класс.

**Мульти-классификация**

Одним из возможных способов обобщения метрик является вычисление среднего метрики по всем классам. Тогда в качестве «положительного» класса берется вычисляемый, а все остальные — в качестве «отрицательного».

В этом случае формулы для метрик будут выглядеть следующим образом:

![Мульти-классификация](https://webiomed.ai/media/ck_uploads/2020/07/08/bezimeni-8.png)

### Что такое кластеризация?
<a id="clustering"></a>
([наверх](#sections))

Кластерный анализ (Data clustering) — задача разбиения заданной выборки объектов (ситуаций) на непересекающиеся подмножества, называемые кластерами, так, чтобы каждый кластер состоял из схожих объектов, а объекты разных кластеров существенно отличались.

Задача кластеризации относится к широкому классу задач обучения без учителя.

**Типология задач кластеризации**

*Типы входных данных*  
Признаковое описание объектов. Каждый объект описывается набором своих характеристик, называемых признаками. Признаки могут быть числовыми или нечисловыми.
Матрица расстояний между объектами. Каждый объект описывается расстояниями до всех остальных объектов обучающей выборки.
Матрица расстояний может быть вычислена по матрице признаковых описаний объектов бесконечным числом способов, в зависимости от того, как ввести функцию расстояния (метрику) между признаковыми описаниями. Часто используется евклидова метрика, однако этот выбор в большинстве случаев является эвристикой и обусловлен лишь соображениями удобства.

Обратная задача — восстановление признаковых описаний по матрице попарных расстояний между объектами — в общем случае не имеет решения, а приближённое решение не единственно и может иметь существенную погрешность. Эта задача решается методами многомерного шкалирования.

Таким образом, постановка задачи кластеризации по матрице расстояний является более общей. С другой стороны, при наличии признаковых описаний часто удаётся строить более эффективные методы кластеризации.

*Цели кластеризации*  
Понимание данных путём выявления кластерной структуры. Разбиение выборки на группы схожих объектов позволяет упростить дальнейшую обработку данных и принятия решений, применяя к каждому кластеру свой метод анализа (стратегия «разделяй и властвуй»).
Сжатие данных. Если исходная выборка избыточно большая, то можно сократить её, оставив по одному наиболее типичному представителю от каждого кластера.
Обнаружение новизны (novelty detection). Выделяются нетипичные объекты, которые не удаётся присоединить ни к одному из кластеров.
В первом случае число кластеров стараются сделать поменьше. Во втором случае важнее обеспечить высокую (или фиксированную) степень сходства объектов внутри каждого кластера, а кластеров может быть сколько угодно. В третьем случае наибольший интерес представляют отдельные объекты, не вписывающиеся ни в один из кластеров.

Во всех этих случаях может применяться иерархическая кластеризация, когда крупные кластеры дробятся на более мелкие, те в свою очередь дробятся ещё мельче, и т. д. Такие задачи называются задачами таксономии.

Результатом таксономии является древообразная иерархическая структура. При этом каждый объект характеризуется перечислением всех кластеров, которым он принадлежит, обычно от крупного к мелкому. Визуально таксономия представляется в виде графика, называемого дендрограммой.

Классическим примером таксономии на основе сходства является биноминальная номенклатура живых существ, предложенная Карлом Линнеем в середине XVIII века. Аналогичные систематизации строятся во многих областях знания, чтобы упорядочить информацию о большом количестве объектов.

*Функции расстояния*
* Метрика Хэмминга
* Евклидова метрика
* Взвешенная евклидова метрика
* Метрика Минковского
*Методы кластеризации*
* Графовые алгоритмы кластеризации
* Статистические алгоритмы кластеризации
    * Алгоритм k-средних (k-means)
    * EM-алгоритм
* Алгоритм ФОРЕЛЬ
* Иерархическая кластеризация или таксономия
* Нейронная сеть Кохонена
* Ансамбль кластеризаторов

**Формальная постановка задачи кластеризации**

Пусть X  — множество объектов, Y — множество номеров (имён, меток) кластеров. Задана функция расстояния между объектами ![\rho(x,x')](https://latex.codecogs.com/png.image?\dpi{110}%20\rho(x,x%27)). Имеется конечная обучающая выборка объектов ![X^m = \{ x_1, \dots, x_m \} \subset X](https://latex.codecogs.com/png.image?\dpi{110}%20X^m%20=%20\{%20x_1,%20\dots,%20x_m%20\}%20\subset%20X). Требуется разбить выборку на непересекающиеся подмножества, называемые кластерами, так, чтобы каждый кластер состоял из объектов, близких по метрике p, а объекты разных кластеров существенно отличались. При этом каждому объекту ![x_i\in X^m](https://latex.codecogs.com/png.image?\dpi{110}%20x_i\in%20X^m) приписывается номер кластера ![y_i](https://latex.codecogs.com/png.image?\dpi{110}%20y_i).

Алгоритм кластеризации — это функция ![a:\, X\to Y](https://latex.codecogs.com/png.image?\dpi{110}%20a:\,%20X\to%20Y), которая любому объекту ![x\in X](https://latex.codecogs.com/png.image?\dpi{110}%20x\in%20X) ставит в соответствие номер кластера ![y\in Y](https://latex.codecogs.com/png.image?\dpi{110}%20y\in%20X). Множество Y в некоторых случаях известно заранее, однако чаще ставится задача определить оптимальное число кластеров, с точки зрения того или иного критерия качества кластеризации.

Кластеризация (обучение без учителя) отличается от классификации (обучения с учителем) тем, что метки исходных объектов ![y_i](https://latex.codecogs.com/png.image?\dpi{110}%20y_i) изначально не заданы, и даже может быть неизвестно само множество Y.

Решение задачи кластеризации принципиально неоднозначно, и тому есть несколько причин:

Не существует однозначно наилучшего критерия качества кластеризации. Известен целый ряд эвристических критериев, а также ряд алгоритмов, не имеющих чётко выраженного критерия, но осуществляющих достаточно разумную кластеризацию «по построению». Все они могут давать разные результаты.
Число кластеров, как правило, неизвестно заранее и устанавливается в соответствии с некоторым субъективным критерием.
Результат кластеризации существенно зависит от метрики, выбор которой, как правило, также субъективен и определяется экспертом.

### Что такое понижение размерности?
<a id="dimension-reduction"></a>
([наверх](#sections))

Под уменьшением размерности (англ. dimensionality reduction) в машинном обучении подразумевается уменьшение числа признаков набора данных. Наличие в нем признаков избыточных, неинформативных или слабо информативных может понизить эффективность модели, а после такого преобразования она упрощается, и соответственно уменьшается размер набора данных в памяти и ускоряется работа алгоритмов ML на нем. Уменьшение размерности может быть осуществлено методами выбора признаков (англ. feature selection) или выделения признаков 

### Что такое смещение и дисперсия, и каковы их отношения в моделировании данных?
<a id="bias-and-variance"></a>
([наверх](#sections))

Модель с большим смещением будет производить одинаковые ошибки для входа, независимо от обучающего множества, на котором происходило обучение. Модель смещает свои собственные предположения о реальных отношениях из-за связей, имеющихся в обучающих данных.

Модель с большой дисперсией, наоборот, будет производить различные ошибки для входа в зависимости от обучающего множества, на котором она училась.

Модель с большим смещением не гибкая, а модель с высокой дисперсией может быть, напротив, настолько гибкой, что смоделирует шум в обучающем множестве. То есть, модель с высокой дисперсией «переподгоняет» (overfit) обучающие данные, в то время как модель с большим смещением недостаточно точно подгоняет обучающие данные.

![Смещение и дисперсия](http://robotosha.ru/wp-content/uploads/2015/04/ml_bias_variance.png)

## Алгоритмы машинного обучения
<a id="machine-learning-algorithms"></a>
([наверх](#sections))

Алгоритмы машинного обучения можно описать как обучение целевой функции f, которая наилучшим образом соотносит входные переменные X и выходную переменную Y: Y = f(X).

Мы не знаем, что из себя представляет функция f. Ведь если бы знали, то использовали бы её напрямую, а не пытались обучить с помощью различных алгоритмов.

Наиболее распространённой задачей в машинном обучении является предсказание значений Y для новых значений X. Это называется прогностическим моделированием, и наша цель — сделать как можно более точное предсказание.

### Линейная регрессия
<a id="linear-regression"></a>
([наверх](#sections))

Алгоритм для задачи регрессии. Он предсказывает вещественное число, позволяет описывать линейную или биномиальную зависимость.  
Обучение происходит с помощью минимизации функции потерь, может быть две функции потерь.

Линейную регрессию можно представить в виде уравнения, которое описывает прямую, наиболее точно показывающую взаимосвязь между входными переменными X и выходными переменными Y. Для составления этого уравнения нужно найти определённые коэффициенты B для входных переменных.

![Линейная регрессия](https://tproger.ru/s3/uploads/2018/04/pic1.jpeg)

Средняя квадратичная ошибка:  

<img src="https://render.githubusercontent.com/render/math?math=MSE = \Sigma_{i=1}^{n}(y_i - y_i^p)^2">

Средняя абсолютная ошибка:  

<img src="https://render.githubusercontent.com/render/math?math=MAE = \Sigma_{i=1}^{n} \left| y_i - y_i^p \right |">

Обучение происходит с помощью градиентного спуска, также возможно аналитическое решение этой задачи с помощью нахождения обратной матрицы. Но в реальносnи оно не применятся, так как требует значительных вычислительных ресурсов, а также не для всех матриц можно найти обратную.

### Логистическая регрессия
<a id="logistic-regression"></a>
([наверх](#sections))

Это алгоритм для задачи классификации. Отличается от линейной регресси тем, что тут есть функция активации. 
Логистическая регрессия похожа на линейную тем, что в ней тоже требуется найти значения коэффициентов для входных переменных. Разница заключается в том, что выходное значение преобразуется с помощью нелинейной или логистической функции.

![Логистическая функция](https://tproger.ru/s3/uploads/2018/04/5vpYa.png)

Благодаря тому, как обучается модель, предсказания логистической регрессии можно использовать для отображения вероятности принадлежности образца к классу 0 или 1. Это полезно в тех случаях, когда нужно иметь больше обоснований для прогнозирования.

Для задач бинарной классификации используется сигмоида.  
Для задач многоклассовой классификации используется функция Softmax:  

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ab3ef6ba51afd36c1d2baf06540022053b2dca73">

Функции потерь тоже отличаются:  
Для задач бинарной классификации - это LogLoss:  

<img src="https://alexanderdyakonov.files.wordpress.com/2018/03/log_loss_23.png">  

Для задач многоклассовой классификации - это categorical cross entropy

### Линейный дискриминантный анализ (LDA)
<a id="LDA"></a>
([наверх](#sections))

Логистическая регрессия используется, когда нужно отнести образец к одному из двух классов. Если классов больше, чем два, то лучше использовать алгоритм LDA (Linear discriminant analysis).

Представление LDA довольно простое. Оно состоит из статистических свойств данных, рассчитанных для каждого класса. Для каждой входной переменной это включает:

* Среднее значение для каждого класса;
* [Дисперсию](#Measures-of-variability), рассчитанную по всем классам.

Предсказания производятся путём вычисления дискриминантного значения для каждого класса и выбора класса с наибольшим значением. Предполагается, что данные имеют нормальное распределение, поэтому перед началом работы рекомендуется удалить из данных аномальные значения. Это простой и эффективный алгоритм для задач классификации.

### Случайный лес (Random Forest)
<a id="Random-Forest"></a>
([наверх](#sections))

RF (random forest) — это множество решающих деревьев. В задаче регрессии их ответы усредняются, в задаче классификации принимается решение голосованием по большинству. Все деревья строятся независимо по следующей схеме:

* Выбирается подвыборка обучающей выборки размера samplesize (м.б. с возвращением) – по ней строится дерево (для каждого дерева — своя подвыборка).
* Для построения каждого расщепления в дереве просматриваем max_features случайных признаков (для каждого нового расщепления — свои случайные признаки).
* Выбираем наилучшие признак и расщепление по нему (по заранее заданному критерию). Дерево строится, как правило, до исчерпания выборки (пока в листьях не останутся представители только одного класса), но в современных реализациях есть параметры, которые ограничивают высоту дерева, число объектов в листьях и число объектов в подвыборке, при котором проводится расщепление.

В библиотеке scikit-learn есть такая реализация RF (пример только для задачи классификации):

```
class sklearn.ensemble.RandomForestClassifier(n_estimators=10,
criterion='gini', max_depth=None, min_samples_split=2,
min_samples_leaf=1, min_weight_fraction_leaf=0.0,
max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07,
bootstrap=True, oob_score=False, n_jobs=1,
random_state=None, verbose=0, warm_start=False,
class_weight=None)
```
С алгоритмом работают по стандартной схеме, принятой в scikit-learn:

```
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import roc_auc_score
# далее - (X, y) - обучение, (X2, y2) - контроль
# модель - здесь (для контраста) рассмотрим регрессор
model =  RandomForestRegressor(n_estimators=10 ,
                               oob_score=True,
                               random_state=1)
model.fit(X, y) # обучение
a = model.predict(X2) # предсказание

print ("AUC-ROC (oob) = ", roc_auc_score(y, model.oob_prediction_))
print ("AUC-ROC (test) = ", roc_auc_score(y2, a))
```

**Число деревьев — n_estimators**  
Чем больше деревьев, тем лучше качество, но время настройки и работы RF также пропорционально увеличиваются. Обратите внимание, что часто при увеличении n_estimators качество на обучающей выборке повышается (может даже доходить до 100%), а качество на тесте выходит на асимптоту (можно прикинуть, скольких деревьев Вам достаточно).

![n_estimators](https://alexanderdyakonov.files.wordpress.com/2016/11/n_estimators.png?w=700)

**Число признаков для выбора расщепления — max_features**  
График качества на тесте от значения этого праметра унимодальный, на обучении он строго возрастает. При увеличении max_features увеличивается время построения леса, а деревья становятся «более однообразными». По умолчанию он равен sqrt(n) в задачах классификации и n/3 в задачах регрессии. Это самый важный параметр! Его настраивают в первую очередь (при достаточном числе деревьев в лесе).  

![max_features](https://alexanderdyakonov.files.wordpress.com/2016/11/max_features.png?w=700)

**Максимальная глубина деревьев — max_depth**  
Ясно, что чем меньше глубина, тем быстрее строится и работает RF. При увеличении глубины резко возрастает качество на обучении, но и на контроле оно, как правило, увеличивается. Рекомендуется использовать максимальную глубину (кроме случаев, когда объектов слишком много и получаются очень глубокие деревья, построение которых занимает значительное время). При использовании неглубоких деревьев изменение параметров, связанных с ограничением числа объектов в листе и для деления, не приводит к значимому эффекту (листья и так получаются «большими»). Неглубокие деревья рекомендуют использовать в задачах с большим числом шумовых объектов (выбросов).

![max_depth](https://alexanderdyakonov.files.wordpress.com/2016/11/max_depth.png?w=700)

### Метод опорных векторов (SVM)
<a id="SVM"></a>
([наверх](#sections))

Гиперплоскость — это линия, разделяющая пространство входных переменных. В методе опорных векторов гиперплоскость выбирается так, чтобы наилучшим образом разделять точки в плоскости входных переменных по их классу: 0 или 1. В двумерной плоскости это можно представить как линию, которая полностью разделяет точки всех классов. Во время обучения алгоритм ищет коэффициенты, которые помогают лучше разделять классы гиперплоскостью.

![SVM](https://tproger.ru/s3/uploads/2018/04/8.jpeg)

Расстояние между гиперплоскостью и ближайшими точками данных называется разницей. Лучшая или оптимальная гиперплоскость, разделяющая два класса, — это линия с наибольшей разницей. Только эти точки имеют значение при определении гиперплоскости и при построении классификатора. Эти точки называются опорными векторами. Для определения значений коэффициентов, максимизирующих разницу, используются специальные алгоритмы оптимизации.

Метод опорных векторов, наверное, один из самых эффективных классических классификаторов, на который определённо стоит обратить внимание.

### K-ближайших соседей (KNN)
<a id="KNN"></a>
([наверх](#sections))

К-ближайших соседей — очень простой и очень эффективный алгоритм. Модель KNN (K-nearest neighbors) представлена всем набором тренировочных данных. 

Предсказание для новой точки делается путём поиска K ближайших соседей в наборе данных и суммирования выходной переменной для этих K экземпляров.

Вопрос лишь в том, как определить сходство между экземплярами данных. Если все признаки имеют один и тот же масштаб (например, сантиметры), то самый простой способ заключается в использовании евклидова расстояния — числа, которое можно рассчитать на основе различий с каждой входной переменной.

KNN может потребовать много памяти для хранения всех данных, но зато быстро сделает предсказание. Также обучающие данные можно обновлять, чтобы предсказания оставались точными с течением времени.

Идея ближайших соседей может плохо работать с многомерными данными (множество входных переменных), что негативно скажется на эффективности алгоритма при решении задачи. Это называется проклятием размерности. Иными словами, стоит использовать лишь наиболее важные для предсказания переменные.

### Наивный Байесовский классификатор
<a id="Bayes"></a>
([наверх](#sections))

Наивный Байес — простой, но удивительно эффективный алгоритм.

Модель состоит из двух типов вероятностей, которые рассчитываются с помощью тренировочных данных:

Вероятность каждого класса.
Условная вероятность для каждого класса при каждом значении x.
После расчёта вероятностной модели её можно использовать для предсказания с новыми данными при помощи теоремы Байеса. Если у вас вещественные данные, то, предполагая нормальное распределение, рассчитать эти вероятности не составляет особой сложности.

Наивный Байес называется наивным, потому что алгоритм предполагает, что каждая входная переменная независимая. Это сильное предположение, которое не соответствует реальным данным. Тем не менее данный алгоритм весьма эффективен для целого ряда сложных задач вроде классификации спама или распознавания рукописных цифр.

## Бустинг
<a id="boosting"></a>
([наверх](#sections))

Бустинг (англ. boosting — улучшение) — это процедура последовательного построения композиции алгоритмов машинного обучения, когда каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих алгоритмов. Бустинг представляет собой жадный алгоритм построения композиции алгоритмов. Изначально понятие бустинга возникло в работах по вероятно почти корректному обучению в связи с вопросом: возможно ли, имея множество плохих (незначительно отличающихся от случайных) алгоритмов обучения, получить хороший.

В течение последних 10 лет бустинг остаётся одним из наиболее популярных методов машинного обучения, наряду с нейронными сетями и машинами опорных векторов. Основные причины — простота, универсальность, гибкость (возможность построения различных модификаций), и, главное, высокая обобщающая способность.

Бустинг над [решающими деревьями](https://learnmachinelearning.wikia.org/ru/wiki/%D0%A0%D0%B5%D1%88%D0%B0%D1%8E%D1%89%D0%B5%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_(Decision_tree)) считается одним из наиболее эффективных методов с точки зрения качества [классификации](#classification). Во многих экспериментах наблюдалось практически неограниченное уменьшение частоты ошибок на независимой тестовой выборке по мере наращивания композиции. Более того, качество на тестовой выборке часто продолжало улучшаться даже после достижения безошибочного распознавания всей обучающей выборки. Это перевернуло существовавшие долгое время представления о том, что для повышения обобщающей способности необходимо ограничивать сложность алгоритмов. На примере бустинга стало понятно, что хорошим качеством могут обладать сколь угодно сложные композиции, если их правильно настраивать.

Впоследствии феномен бустинга получил теоретическое обоснование. Оказалось, что взвешенное голосование не увеличивает эффективную сложность алгоритма, а лишь сглаживает ответы базовых алгоритмов. Количественные оценки обобщающей способности бустинга формулируются в терминах отступа. Эффективность бустинга объясняется тем, что по мере добавления базовых алгоритмов увеличиваются отступы обучающих объектов. Причём бустинг продолжает раздвигать классы даже после достижения безошибочной классификации обучающей выборки.

К сожалению, теоретические оценки обобщающей способности дают лишь качественное обоснование феномену бустинга. Хотя они существенно точнее более общих [оценок Вапника-Червоненкиса](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%92%D0%B0%D0%BF%D0%BD%D0%B8%D0%BA%D0%B0_%E2%80%94_%D0%A7%D0%B5%D1%80%D0%B2%D0%BE%D0%BD%D0%B5%D0%BD%D0%BA%D0%B8%D1%81%D0%B0), всё же они сильно завышены, и требуемая длина обучающей выборки оценивается величиной порядка ![10^4 \dots 10^6](https://latex.codecogs.com/png.image?\dpi{110}%2010^4%20\dots%2010^6). Более основательные эксперименты показали, что иногда бустинг всё же переобучается.

## Метрики регрессии
<a id="Regression-metrics"></a>
([наверх](#sections))

**Средняя квадратическая ошибка (MSE)**

![MSE](https://www.machinelearningmastery.ru/img/0-798429-593663.png)

где yᵢ фактический ожидаемый результат и ŷᵢ это прогноз модели.

MSE в основном измеряет среднеквадратичную ошибку наших прогнозов. Для каждой точки вычисляется квадратная разница между прогнозами и целью, а затем усредняются эти значения.

Чем выше это значение, тем хуже модель. Он никогда не бывает отрицательным, поскольку мы возводим в квадрат отдельные ошибки прогнозирования, прежде чем их суммировать, но для идеальной модели это будет ноль.

Преимущество:Полезно, если у нас есть неожиданные значения, о которых мы должны заботиться. Очень высокое или низкое значение, на которое мы должны обратить внимание.

Недостаток:Если мы сделаем один очень плохой прогноз, возведение в квадрат сделает ошибку еще хуже, и это может исказить метрику в сторону переоценки плохости модели. Это особенно проблематичное поведение, если у нас есть зашумленные данные (то есть данные, которые по какой-либо причине не совсем надежны) - даже в «идеальной» модели может быть высокий MSE в этой ситуации, поэтому становится трудно судить, насколько хорошо модель выполняет. С другой стороны, если все ошибки малы или, скорее, меньше 1, то ощущается противоположный эффект: мы можем недооценивать недостатки модели.

**Среднеквадратическая ошибка (RMSE)**

RMSE - это просто квадратный корень из MSE. Квадратный корень введен, чтобы масштаб ошибок был таким же, как масштаб целей.

Важно понять, в каком смысле RMSE похож на MSE, и в чем разница.

Во-первых, они похожи с точки зрения их минимизаторов, каждый минимизатор MSE также является минимизатором для RMSE и наоборот, поскольку квадратный корень является неубывающей функцией. Например, если у нас есть два набора предсказаний, A и B, и скажем, что MSE для A больше, чем MSE для B, то мы можем быть уверены, что RMSE для A больше RMSE для B. И это также работает в противоположном направлении.

**Средняя абсолютная ошибка (MAE)**

В MAE ошибка рассчитывается как среднее абсолютных разностей между целевыми значениями и прогнозами. MAE - это линейная оценка, которая означает, что все индивидуальные различия взвешены одинаковов среднем. Например, разница между 10 и 0 будет вдвое больше разницы между 5 и 0. Однако то же самое не верно для RMSE. Математически он рассчитывается по следующей формуле:

![MAE](https://www.machinelearningmastery.ru/img/0-411862-507651.png)

Что важно в этой метрике, так это то, что она наказывает огромные ошибки, которые не так плохо, как MSE.Таким образом, он не так чувствителен к выбросам, как среднеквадратическая ошибка.

**R в квадрате (R²)**

Коэффициент детерминации, или R² (иногда читаемый как R-два), является еще одним показателем, который мы можем использовать для оценки модели, и он тесно связан с MSE, но имеет преимущество в том, что безмасштабное не имеет значения, являются ли выходные значения очень большими или очень маленькими,R² всегда будет между -∞ и 1.

Когда R² отрицательно, это означает, что модель хуже, чем предсказание среднего значения.

![R²](https://www.machinelearningmastery.ru/img/0-582527-102961.png)

R² - это соотношение между тем, насколько хороша наша модель, и тем, насколько хороша модель наивного среднего.

## Обучение с помощью градиентного спуска
<a id="gradient-descent-training"></a>
([наверх](#sections))

Градиентный спуск — самый используемый алгоритм обучения, он применяется почти в каждой модели машинного обучения. Градиентный спуск — это, по сути, и есть то, как обучаются модели. Метод градиентного спуска с некоторой модификацией широко используется для обучения персептрона и глубоких нейронных сетей, и известен как метод обратного распространения ошибки.

**Что такое градиентный спуск**  
Градиентный спуск — метод нахождения минимального значения функции потерь (существует множество видов этой функции). Минимизация любой функции означает поиск самой глубокой впадины в этой функции. Функция используется, чтобы контролировать ошибку в прогнозах модели машинного обучения. Поиск минимума означает получение наименьшей возможной ошибки или повышение точности модели. Мы увеличиваем точность, перебирая набор учебных данных при настройке параметров нашей модели (весов и смещений).

Итак, градиентный спуск нужен для минимизации функции потерь.

**Градиент** - это вектор, касающийся функции и указывающий в направлении наибольшего увеличения этой функции. Градиент равен нулю при локальном максимуме или минимуме, потому что нет единого направления увеличения. В математике градиент определяется как частная производная для каждой входной переменной функции. Например, у нас есть функция:

![Градиент](https://www.machinelearningmastery.ru/img/0-629980-531134.gif)

Поскольку градиент - это вектор, указывающий на наибольшее увеличение функции, отрицательный градиент - это вектор, указывающий на наибольшее уменьшение функции. Следовательно, мы можем минимизировать функцию, итеративно немного двигаясь в направлении отрицательного градиента. Это логика градиентного спуска.

![ГрадСпуск](https://www.machinelearningmastery.ru/img/0-608727-517151.jpeg)

Важным параметром в градиентном спуске является скорость обучения, которая определяет размер каждого шага. Когда скорость обучения слишком велика, градиентный спуск может перепрыгнуть через долину и оказаться на другой стороне. Это приведет к расхождению функции стоимости. С другой стороны, когда скорость обучения слишком мала, алгоритм будет сходиться долго. Следовательно, перед началом градиентного спуска необходима правильная скорость обучения.

**Различные типы градиентного спуска**
Существует 3 варианта градиентного спуска:

1. Мini-batch: тут вместо перебирания всех примеров обучения и с каждой итерацией, выполняющей вычисления только на одном примере обучения, мы обрабатываем n учебных примеров сразу. Этот выбор хорош для очень больших наборов данных.

2. Стохастический градиентный спуск: в этом случае вместо использования и зацикливания на каждом примере обучения, мы просто используем один раз. Есть несколько вещей замечаний:

С каждой итерацией ГС вам нужно перемешать набор обучения и выбрать случайный пример обучения.
Поскольку вы используете только один пример обучения, ваш путь к локальным минимумам будет очень шумным, как у пьяного человека, который много выпил.

3. Пакетный градиентный спуск

Пакетный градиентный спуск использует всю партию обучающих данных на каждом шаге. Он рассчитывает ошибку для каждой записи и принимает среднее значение для определения градиента Преимущество Batch Gradient Descent заключается в том, что алгоритм более эффективен в вычислительном отношении и обеспечивает стабильный путь обучения, что облегчает сходимость. Тем не менее, пакетный градиентный спуск занимает больше времени, когда тренировочный набор большой.

## Алгоритмы кластеризации такие как k-means и c-means, dbscan
<a id="k-means-c-means-dbscan"></a>
([наверх](#sections))

## Иерархическая кластеризация
<a id="hierarchical-clustering"></a>
([наверх](#sections))

## Алгоритмы понижения размерности такие как PCA, t-SNE
<a id="lPCA-t-SNE"></a>
([наверх](#sections))

## Что такое регуляризация и чем она полезна?
<a id="regularization"></a>
([наверх](#sections))

Регуляризация – это метод добавления дополнительной информации к условию для решения некорректно поставленных задач или для предотвращения переобучения. Под регуляризацией часто понимается «штраф» за сложность модели.

термин изначально возник в теории [некорректно поставленных задач](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%80%D1%80%D0%B5%D0%BA%D1%82%D0%BD%D0%BE_%D0%BF%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0), т.е. задач , в которых не выполняются требования:

* решение существует,
* решение единственно,
* решение непрерывно зависит от данных.

Собственно, регуляризация (изначально в версии А. Н. Тихонова) борется со вторым и третьим пунктом с помощью дополнительных ограничений на решение. По факту: в задаче оптимизации к целевой функции добавляют регуляризационное слагаемое (т.н. штраф).

Сейчас термин стал намного шире: часто любой способ борьбы с переобучением, который касается настройки модели,  в машинном обучением называют регуляризацией (начиная от dropout и заканчивая prunning).

## Алгоритмы для нейронных сетей
<a id="algorithms-for-neural-networksn"></a>
([наверх](#sections))

## Полносвязная нейронная сеть
<a id="fully-connected-neural-network"></a>
([наверх](#sections))

## Свёрточная нейронная сеть
<a id="convolutional-neural-networkn"></a>
([наверх](#sections))

## Рекуррентная нейронная сеть
<a id="recurrent-neural-network"></a>
([наверх](#sections))

## Для каких задач подходит тот или иной алгоритм?
<a id="algorithm-suitable"></a>
([наверх](#sections))

## Что такое функция активации?
<a id="activation-function"></a>
([наверх](#sections))

Функция активации (англ. activation function) a(x) определяет выходное значение нейрона в зависимости от результата взвешенной суммы входов и порогового значения.


# Сеть
<a id="network"></a>

* [Что такое веб-сервисы?](#web-services)
* [REST](#rest)  
* [http](#http)
* [JSON](#json)
* [XML](#xml)
* [Очереди сообщений](#message-queues)
* [Что происходит в тот момент, когда вы вводите в адресной строке браузера URL сайта и нажимаете ввод?](#www)
* [Виды сетевых протоколов](#types-of-network-protocols)

([наверх](#sections))

## Что такое веб-сервисы?
<a id="web-services"></a>

([наверх](#sections))

Веб-сервисы (или веб-службы) — это технология, позволяющая системам обмениваться данными друг с другом через сетевое подключение. Обычно веб-сервисы работают поверх протокола HTTP или протокола более высокого уровня. Веб-сервис — просто адрес, ссылка, обращение к которому позволяет получить данные или выполнить действие.

Главное отличие веб-сервиса от других способов передачи данных: стандартизированность. Приняв решение использовать веб-сервисы, можно сразу переходить к структуре данных и доступным функциям. Например, В SOAP (как более строгом протоколе), уже решён вопрос уведомления об ошибках.

Самые известные способы реализации веб-сервисов:

* XML-RPC (XML Remote Procedure Call) — протокол удаленного вызова процедур с использованием XML. Прародитель SOAP. Предельно прост в реализации.

* SOAP (Simple Object Access Protocol) — стандартный протокол по версии W3C. Четко структурирован и задокументирован.

* JSON-RPC (JSON Remote Procedure Call) — более современный аналог XML-RPC. Основное отличие — данные передаются в формате JSON.

* REST (Representational State Transfer) — архитектурный стиль взаимодействия компьютерных систем в сети основанный на методах протокола HTTP.

* Специализированные протоколы для конкретного вида задач, такие как GraphQL.

* Менее распространенный, но более эффективный gRPC, передающий данные в бинарном виде и использующий HTTP/2 в качестве транспорта.

## REST
<a id="rest"></a>

([наверх](#sections))

REST (Representational state transfer) – это стиль архитектуры программного обеспечения для распределенных систем, таких как World Wide Web, который, как правило, используется для построения веб-служб (набор рекомендаций, который позволяет унифицировать взаимодействие клиентских и серверных приложений). Термин REST был введен в 2000 году Роем Филдингом, одним из авторов HTTP-протокола. Системы, поддерживающие REST, называются RESTful-системами. Единого стандарта у него нет — не протокол, а целый архитектурный стиль. Этим она отличается от многих аналогичных. При этом допустимо использовать XML, HTTP, JSON и URL.

REST не использует конвертацию данных при передаче, данные передаются в исходном виде — это снижает нагрузку на клиент веб-сервиса, но увеличивает нагрузку на сеть. Управление данными происходит с помощью методов HTTP:

GET — получить данные;

POST — добавить данные;

PUT — изменить данные;

DELETE — удалить данные.


В качестве пакета обычно отправляется JSON массив на указанный конкретный URL. Там срабатывает так называемая функция, а в зависимости от уже отправленных данных и текущего запроса начинается определенное действие.

Использование этих методов позволяет реализовать типичный CRUD (Create/Read/Update/Delete) для любой информации. Но это лишь соглашение: часто используются только 2 метода: GET для получения и POST для всего остального. Разобраться поможет такое понятие, как REST-Patterns . Паттерны связывают HTTP методы с тем, что они делают.

Преимущества:

* простота реализации;

* экономичность в плане ресурсов;

* не требует программных надстроек (json_decode есть почти в каждом языке).

Недостатки:

* отсутствие спецификации;

* неоднозначность методов управления данными.

Протокол по типу концентрированного REST API, работающий по HTTP = качественным веб-сервисам  

Так можно назвать веб-приложение, которое представляет ресурсы в формате, подходящем для других компьютеров. Они включают в себя разные интерфейсы.

Те варианты, которые применяются для транслирования, тоже можно учитывать как «веб-сервисы». Клиент, который пользуется этим, способен запросить все что угодно, а сервер ему отвечает и предоставляет результаты. Задействуют любой удобный язык программирования или подходящие платформы. Не имеет никакого значения, какой конкретно использовался, так как все выполняется через общий тип.

Когда API документируется, то неважно, чем пользовались разработчики при его создании — Ruby это был, Java или Python или что-то принципиально другое. Все запросы высылаются через один и тот же HTTP, решения приходят таким же способом.

## JSON
<a id="json"></a>

([наверх](#sections))

JSON это сокращение от JavaScript Object Notation — формата передачи данных. Как можно понять из названия, JSON произошел из JavaScript, но он доступен для использования на многих других языках, включая Python, Ruby, PHP и Java.  
JSON - один из самых популярных методов обмена данными между сервером и клиентом в Интернет (API и REST)

Сам по себе JSON использует расширение .json. Когда же он определяется в других файловых форматах, как .html, он появляется в кавычках как JSON строка или может быть объектом, назначенным на переменную. Такой формат легко передавать между сервером и клиентской частью, ну или браузером.  
Легкочитаемый и компактный, JSON представляет собой хорошую альтернативу XML и требует куда меньше форматирования контента. Это информативное руководство поможет вам быстрее разобраться с данными, которые вы можете использовать с JSON и основной структурой с синтаксисом этого же формата.

Анализ структура JSON
![Анализ структура JSON](https://habrastorage.org/webt/x6/q_/sg/x6q_sgnunckncpntu5xyf7zeu1i.png)

**Синтаксис и структура**

Объект JSON это формат данных — ключ-значение, который обычно рендерится в фигурных скобках. 

JSON объект:

```
{
  "first_name" : "Sammy",
  "last_name" : "Shark",
  "location" : "Ocean",
  "online" : true,
  "followers" : 987 
}
```
JSON может содержать другие вложенные объекты в JSON, в дополнение к вложенным массивам. Такие объекты и массивы будут передаваться, как значения назначенные ключам и будут представлять собой связку ключ-значение.

Вынесем имя из первого json объекта и добавим ещё три.
```
{ 
  "sammy" : {
    "username"  : "SammyShark",
    "location"  : "Indian Ocean",
    "online"    : true,
    "followers" : 987
  },
  "jesse" : {
    "username"  : "JesseOctopus",
    "location"  : "Pacific Ocean",
    "online"    : false,
    "followers" : 432
  },
  "drew" : {
    "username"  : "DrewSquid",
    "location"  : "Atlantic Ocean",
    "online"    : false,
    "followers" : 321
  },
  "jamie" : {
    "username"  : "JamieMantisShrimp",
    "location"  : "Pacific Ocean",
    "online"    : true,
    "followers" : 654
  }
}
```

Вложенные массивы
Данные также могут быть вложены в формате JSON, используя JavaScript массивы, которые передаются как значения. JavaScript использует квадратные скобки [ ] для формирования массива. Массивы по своей сути — это упорядоченные коллекции и могут включать в себя значения совершенно разных типов данных.  
Мы можем использовать массив при работе с большим количеством данных, которые могут быть легко сгруппированны вместе, как например, если есть несколько разных сайтов и профайлов в социальных сетях ассоциированных с одним пользователем.

```
{ 
  "first_name" : "Sammy",
  "last_name" : "Shark",
  "location" : "Ocean",
  "websites" : [ 
    {
      "description" : "work",
      "URL" : "https://www.digitalocean.com/"
    },
    {
      "desciption" : "tutorials",
      "URL" : "https://www.digitalocean.com/community/tutorials"
    }
  ],
  "social_media" : [
    {
      "description" : "twitter",
      "link" : "https://twitter.com/digitalocean"
    },
    {
      "description" : "facebook",
      "link" : "https://www.facebook.com/DigitalOceanCloudHosting"
    },
    {
      "description" : "github",
      "link" : "https://github.com/digitalocean"
    }
  ]
}
```

## XML
<a id="xml"></a>

([наверх](#sections))

Начнём с сравнения с JSON

XML расшифровывается как eXtensible Markup Language. Это способ хранения данных, которые могут быть прочитаны как людьми, так и машинами. Формат XML доступен для использования во многих языках программирования.  
Во многих случаях, XML очень схож с JSON. XML должен быть спарсен с XML парсером, но JSON может быть спарсен стандартным функционалом. Так же, в отличие от JSON, XML не может использовать массивы.
```
<users>
    <user>
        <username>SammyShark</username> <location>Indian Ocean</location>
    </user>
    <user>
        <username>JesseOctopus</username> <location>Pacific Ocean</location>
    </user>
    <user>
        <username>DrewSquir</username> <location>Atlantic Ocean</location>
    </user>
    <user>
        <username>JamieMantisShrimp</username> <location>Pacific Ocean</location>
    </user>
</users>
```

А вот это уже формат JSON:
```
{"users": [
  {"username" : "SammyShark", "location" : "Indian Ocean"},
  {"username" : "JesseOctopus", "location" : "Pacific Ocean"},
  {"username" : "DrewSquid", "location" : "Atlantic Ocean"},
  {"username" : "JamieMantisShrimp", "location" : "Pacific Ocean"}
] }
```
Простейший XML-документ выглядит следующим образом:
```
<?xml version="1.0" encoding="windows-1251"?>
<book category="WEB">
   <title lang="en">Learning XML</title>
   <author>Erik T. Ray</author>
   <year>2003</year>
   <price></price>
</book>
```

Первая строка — это XML декларация. Здесь определяется версия XML (1.0) и кодировка файла. На следующей строке описывается корневой элемент документа ```<book>``` (открывающий тег). Следующие 4 строки описывают дочерние элементы корневого элемента (```title```, ```author```, ```year```, ```price```). Последняя строка определяет конец корневого элемента ```</book>``` (закрывающий тег).

Документ XML состоит из элементов (elements). Элемент начинается открывающим тегом (start-tag) в угловых скобках, затем идет содержимое (content) элемента, после него записывается закрывающий тег (end-teg) в угловых скобках.

Информация, заключенная между тегами, называется содержимым или значением элемента: ```<author>Erik T. Ray</author>```. Т.е. элемент ```author``` принимает значение ```Erik T. Ray```. Элементы могут вообще не принимать значения.

Элементы могут содержать атрибуты, так, например, открывающий тег ```<title lang="en">``` имеет атрибут ```lang```, который принимает значение ```en```. Значения атрибутов заключаются в кавычки (двойные или ординарные).

Некоторые элементы, не содержащие значений, допустимо записывать без закрывающего тега. В таком случае символ ```/``` ставится в конце открывающего тега:

```<name first="Иван" second="Петрович" />```

**Структура XML**  

XML документ должен содержать корневой элемент. Этот элемент является «родительским» для всех других элементов.

Все элементы в XML документе формируют иерархическое дерево. Это дерево начинается с корневого элемента и разветвляется на более низкие уровни элементов.

Все элементы могут иметь подэлементы (дочерние элементы):

```
<корневой>
   <потомок>
     <подпотомок>.....</подпотомок>
   </потомок>
</корневой>
```

**Правила синтаксиса (Валидность)**  

Структура XML документа должна соответствовать определенным правилам. XML документ отвечающий этим правилам называется валидным (англ. Valid — правильный) или синтаксически верным. Соответственно, если документ не отвечает правилам, он является невалидным .

Основные правила синтаксиса XML:

1. Теги XML регистрозависимы — теги XML являются регистрозависимыми. Так, тег ```<Letter>``` не то же самое, что тег ```<letter>```.

Открывающий и закрывающий теги должны определяться в одном регистре:
```
<Message>Это неправильно</message>
<message>Это правильно</message>
```

2. XML элементы должны соблюдать корректную вложенность:
```
<b><i>Некорректная вложенность</b></i>
<b><i>Корректная вложенность</i></b>
```

3. У XML документа должен быть корневой элемент — XML документ должен содержать один элемент, который будет родительским для всех других элементов. Он называется корневым элементом.

4. Значения XML атрибутов должны заключаться в кавычки:

```
<note date="12/11/2007">Корректная запись</note>
<note date=12/11/2007>Некорреткная запись</note>
```

**Сущности**  

Некоторые символы в XML имеют особые значения и являются служебными. Если вы поместите, например, символ ```<``` внутри XML элемента, то будет сгенерирована ошибка, так как парсер интерпретирует его, как начало нового элемента.

В примере ниже будет сгенерирована ошибка, так как в значении ```"ООО<Мосавтогруз>"``` атрибута ```НаимОрг``` содержатся символы ```<``` и ```>```.  
```
<НПЮЛ ИННЮЛ="7718962261" КПП="771801001" НаимОрг="ООО<Мосавтогруз>"/>
```
Также ошибка будет сгенерирована и в слудющем примере, если название организации взять в обычные кавычки (английские двойные):
```
<НПЮЛ ИННЮЛ="7718962261" КПП="771801001" НаимОрг="ООО"Мосавтогруз""/>
```
Чтобы ошибки не возникали, нужно заменить символ ```<``` на его сущность. В XML существует 5 предопределенных сущностей:

| *Сущность*     | *Символ*      | *Значение*|
|:------------:|:--------------:| :---:|
| ```&lt;```        | 	<	        | меньше, чем |
| ```&gt;```        | 	>         | больше, чем |
| ```&amp;```        | 	&      | амперсанд|
| ```&apos;```     | 	'        | апостроф |
| ```&quot;```        | 	"        | кавычки |

Таким образом, корректными будут следующие формы записей:
``` 
<НПЮЛ ИННЮЛ="7718962261" КПП="771801001" НаимОрг="ООО&quot;Мосавтогруз&quot;"/>
``` 
или
``` 
<НПЮЛ ИННЮЛ="7718962261" КПП="771801001" НаимОрг="ООО«Мосавтогруз»"/>
``` 
В последнем примере английские двойные кавычки заменены на французские кавычки («ёлочки»), которые не являются служебными символами.

**XSD схема**  

XML Schema — язык описания структуры XML-документа, его также называют XSD. Как большинство языков описания XML, XML Schema была задумана для определения правил, которым должен подчиняться документ. Но, в отличие от других языков, XML Schema была разработана так, чтобы её можно было использовать в создании программного обеспечения для обработки документов XML.

После проверки документа на соответствие XML Schema читающая программа может создать модель данных документа, которая включает:

словарь (названия элементов и атрибутов);
модель содержания (отношения между элементами и атрибутами и их структура);
типы данных.
Каждый элемент в этой модели ассоциируется с определённым типом данных, позволяя строить в памяти объект, соответствующий структуре XML-документа. Языкам объектно-ориентированного программирования гораздо легче иметь дело с таким объектом, чем с текстовым файлом.

Простой пример схемы на XML Schema, расположенной в файле "country.xsd" и описывающей данные о населении страны:

``` 
<?xml version="1.0" encoding="utf-8"?>
<xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema">
  <xs:element name="country">
    <xs:complexType>
      <xs:sequence>
        <xs:element name="country_name" type="xs:string"/>
        <xs:element name="population" type="xs:decimal"/>
      </xs:sequence>
    </xs:complexType>
  </xs:element>
</xs:schema>
``` 
Пример документа, соответствующего этой схеме:

``` 
<?xml version="1.0" encoding="utf-8"?>
<country>
    <country_name>France</country_name>
    <population>59.7</population>
</country>
``` 

## http
<a id="http"></a>

([наверх](#sections))

HTTP — это протокол, позволяющий получать различные ресурсы, например HTML-документы. Протокол HTTP лежит в основе обмена данными в Интернете. HTTP является протоколом клиент-серверного взаимодействия, что означает инициирование запросов к серверу самим получателем, обычно веб-браузером (web-browser). Полученный итоговый документ будет (может) состоять из различных поддокументов, являющихся частью итогового документа: например, из отдельно полученного текста, описания структуры документа, изображений, видео-файлов, скриптов и многого другого.

Когда мы вводим в браузере URL-адрес, например www.google.com, на сервер отправляется запрос на веб-сайт, идентифицированный URL-адресом.
Затем этот сервер формирует и выдает ответ. Важным является формат этих запросов и ответов. Эти форматы определяются протоколом HTTP — Hyper Text Transfer Protocol.

Когда мы набираем URL в браузере, он отправляет запрос GET на указанный сервер. Затем сервер отвечает HTTP-ответом, который содержит данные в формате HTML — Hyper Text Markup Language. Затем браузер получает этот HTML-код и отображает его на экране.

Допустим, мы заполняем форму, присутствующую на веб-странице, со списком элементов. В таком случае, когда мы нажимаем кнопку «Submit» (Отправить), HTTP-запрос POST отправляется на сервер.

HTTP — это клиент-серверный протокол, то есть запросы отправляются какой-то одной стороной — участником обмена (user-agent) (либо прокси вместо него). Чаще всего в качестве участника выступает веб-браузер, но им может быть кто угодно, например, робот, путешествующий по Сети для пополнения и обновления данных индексации веб-страниц для поисковых систем.

Каждый запрос (англ. request) отправляется серверу, который обрабатывает его и возвращает ответ (англ. response). Между этими запросами и ответами как правило существуют многочисленные посредники, называемые прокси, которые выполняют различные операции и работают как шлюзы или кеш, например.

## Очереди сообщений
<a id="message-queues"></a>

([наверх](#sections))

**Sync vs Async: синхронное и асинхронное взаимодействие**

Очереди сообщений (Message Queue) — это форма асинхронной коммуникации между сервисами. Поэтому, прежде чем говорить о них, покажем на упрощенном, немного искусственном примере разницу между синхронным и асинхронным взаимодействием.

Предположим, вы разрабатываете сайт книжного магазина и у вас есть сервис, к которому обращается пользователь, например отправка рецензии на прочитанную книгу. При нажатии кнопки «Отправить» вызывается некоторый API, который, в свою очередь, может обратиться к другим API.

При синхронном взаимодействии все запросы в этой цепочке вызовов выполняются строго друг за другом, а при выполнении последнего запроса ответы последовательно передаются в обратном направлении. В итоге пользователь вынужден пару секунд ждать сообщения о публикации своего отзыва, хотя его не интересуют особенности серверной обработки и он вполне обоснованно хочет увидеть сообщение сразу после нажатия кнопки. Конечно, время ожидания будет во многом определяться мощностью оборудования, но при пиковых нагрузках оно может стать серьезной проблемой.

Еще один недостаток такой схемы — обработка сбоев. Если на одном из шагов возникнет исключение, оно каскадно возвратится назад, и пользователь получит уведомление об ошибке с просьбой повторно отправить рецензию. Вряд ли кого-то обрадует получение подобного сообщения после длительного ожидания.  

Синхронное взаимодействие на основе REST API  

![Синхронное взаимодействие на основе REST API](https://mcs.mail.ru/wp-content/uploads/2021/02/1-4.png)

Описанную схему можно изменить, добавив асинхронные вызовы. Достаточно вызвать в асинхронном режиме первый REST API и параллельно вернуть пользователю сообщение о том, что его рецензия принята и будет размещена, например, в течение суток. В итоге сайт не блокируется, а вызовы всех последующих API происходят независимо от пользователя.

Но у такой схемы также есть существенный недостаток: в случае сбоя в одном из API информация, введенная пользователем, будет потеряна. Если в первом примере в случае ошибок достаточно повторно отправить рецензию, то здесь ее необходимо заполнить заново.

Вариант асинхронного взаимодействия на основе REST API

![Вариант асинхронного взаимодействия на основе REST API](https://mcs.mail.ru/wp-content/uploads/2021/02/2-2.png)

Для устранения недостатков обеих схем как раз и предназначены очереди сообщений.

**Принципы работы очередей сообщений**

Очереди предоставляют буфер для временного хранения сообщений и конечные точки, которые позволяют подключаться к очереди для отправки и получения сообщений в асинхронном режиме.

В сообщениях могут содержаться запросы, ответы, ошибки и иные данные, передаваемые между программными компонентами. Компонент, называемый производителем (Producer), добавляет сообщение в очередь, где оно будет храниться, пока другой компонент, называемый потребителем (Consumer), не извлечет сообщение и не выполнит с ним необходимую операцию.

Очередь сообщений

![Очередь сообщений](https://mcs.mail.ru/wp-content/uploads/2021/02/3-2.png)

Очереди поддерживают получение сообщений как методом Push, так и методом Pull:  
* метод Pull подразумевает периодический опрос очереди получателем по поводу наличия новых сообщений;
* метод Push — отправку уведомления получателю в момент прихода сообщения. Второй метод реализует модель «Издатель/Подписчик» (Publisher/Subscriber).
Так как очереди могут использоваться несколькими производителями и потребителями одновременно, обычно их реализуют с помощью дополнительной системы, называемой брокером. Брокер сообщений (Message Broker) занимается сбором и маршрутизацией сообщений на основе предопределенной логики. Сообщения могут передаваться с некоторым ключом — по этому ключу брокер понимает, в какую из очередей (одну или несколько) должно попасть сообщение.

Вернемся к примеру с отправкой рецензии. Пусть та часть сервиса, к которому обращается пользователь, выступит в качестве производителя и будет направлять запросы на создание рецензий в очередь. Сразу после добавления сообщения в очередь пользователю можно направлять уведомление об успехе операции. Вся последующая логика обработки будет выполняться независимо от него на стороне потребителя, подписанного на очередь.

Завершив обработку, потребитель отправит подтверждение в очередь, после чего исходное сообщение будет удалено. Но если во время обработки произойдет сбой и подтверждение не будет получено вовремя, сообщение может быть повторно извлечено потребителем из очереди.

Вариант асинхронного взаимодействия на основе очереди сообщений

![Вариант асинхронного взаимодействия на основе очереди сообщений](https://i2.wp.com/mcsjournal.ru/wp-content/uploads/2021/02/4-3.png?resize=1024%2C112&ssl=1)

Таким образом, использование очередей сообщений решает сразу две задачи: сокращает время ожидания пользователя за счет асинхронной обработки и предотвращает потерю информации при сбоях. Но не следует рассматривать очереди как универсальное средство для любого вида приложений: как и у любого инструмента, у них есть свои преимущества и недостатки, о которых мы поговорим ниже.

**Польза и преимущества очередей сообщений в микросервисной архитектуре**

Используя очереди сообщений в качестве основного средства взаимодействия микросервисов (Microservices Communication), можно добиться следующих преимуществ:

1. Отделение логически независимых компонентов друг от друга (Decoupling)  
Отличительная черта микросервисов — их автономность. И очереди во многом помогают уменьшить зависимости между ними. Каждое сообщение, передаваемое в очереди, — это всего лишь массив байтов с некоторыми метаданными. Метаданные нужны для направления в конкретную очередь, а информация, содержащаяся в основной части (теле) сообщения, может быть практически любой. Брокер не анализирует данные, он выступает лишь в качестве маршрутизатора. Это позволяет настроить взаимодействие между компонентами, работающими даже на разных языках и платформах.

2. Улучшение масштабируемости  
Очереди сообщений упрощают независимое масштабирование микросервисов. Наблюдая за состоянием очередей, можно масштабировать те сервисы, на которые приходится большая часть нагрузки. Кроме этого, очереди легко позволяют не только увеличивать число экземпляров существующих сервисов, но и добавлять новые с минимальным временем простоя. Все, что для этого требуется, — добавить нового потребителя, прослушивающего события в очереди.

Однако сами очереди также необходимо масштабировать, и это может создать дополнительные сложности.

3. Балансировка нагрузки  
Если один из сервисов не справляется с нагрузкой, требуется возможность запускать больше его экземпляров быстро и без дополнительных настроек. Обычно для этих целей используют балансировщик нагрузки, интегрированный с сервером обнаружения служб и предназначенный для распределения трафика. При использовании очередей сообщений сам брокер по умолчанию является балансировщиком нагрузки. Если несколько потребителей слушают очередь одновременно, сообщения будут распределяться между ними в соответствии с настроенной стратегией.

4. Повышение надежности  
Выход из строя одного из компонентов не сказывается на работе всей системы: при восстановлении он обработает сообщение, находящееся в очереди. Ваш веб-сайт по-прежнему может работать, даже если задерживается часть обработки заказа, например, из-за проблем с сервером БД или системой электронной почты.

Правда, при этом очередь сама приобретает статус SPoF (Single Point Of Failure), поэтому необходимо заранее предусмотреть действия на случай ее аварийного отключения.

5. Безопасность
Большинство брокеров выполняют аутентификацию приложений, которые пытаются получить доступ к очереди, и позволяют использовать шифрование сообщений как при их передаче по сети, так и при хранении в самой очереди. Таким образом, очередь снимает с ваших сервисов бремя организации авторизации запросов.

**Варианты использования очередей сообщений**  

Очереди сообщений полезны в тех случаях, где возможна асинхронная обработка. Рассмотрим наиболее частые сценарии использования очередей сообщений (Message Queue use Cases):

1. Фоновая обработка долгосрочных задач на веб-сайтах
Сюда можно отнести задачи, которые не связаны напрямую с основным действием пользователя сайта и могут быть выполнены в фоновом режиме без необходимости ожидания с его стороны. Это обработка изображений, преобразование видео в различные форматы, создание отзывов, индексирование в поисковых системах после изменения данных, отправка электронной почты, формирование файлов и так далее.

2. Буферизация при пакетной обработке данных
Очереди можно использовать в качестве буфера для некоторой массовой обработки, например пакетной вставки данных в БД или HDFS. Очевидно, что гораздо эффективнее добавлять сто записей за раз, чем по одной сто раз, так как сокращаются накладные расходы на инициализацию и завершение каждой операции. Но для стандартной архитектуры может стать проблемой генерация данных клиентской службой быстрее, чем их может обработать получатель. Очередь же предоставляет временное хранилище для пакетов с данными, где они будут храниться до завершения обработки принимающей стороной.

3. Отложенные задачи
Многие системы очередей позволяют производителю указать, что доставка сообщений должна быть отложена. Это может быть полезно при реализации льготных периодов. Например, вы разрешаете покупателю отказаться от размещения заказа в течение определенного времени и ставите отложенное задание в очередь. Если покупатель отменит операцию в указанный срок, сообщение можно удалить из очереди.

4. Сглаживание пиковых нагрузок
Помещая данные в очередь, вы можете быть уверены, что данные будут сохранены и в конечном итоге обработаны, даже если это займет немного больше времени, чем обычно, из-за большого скачка трафика. Увеличить скорость обработки в таких случаях также возможно — за счет масштабирования нужных обработчиков.

5. Гарантированная доставка при нестабильной инфраструктуре
Нестабильная сеть в сочетании с очередью сообщений создает надежный системный ландшафт: каждое сообщение будет отправлено, как только это будет технически возможно.

6. Упорядочение транзакций
Многие брокеры поддерживают очереди FIFO, полезные в системах, где важно сохранить порядок транзакций. Если 1000 человек размещают заказ на вашем веб-сайте одновременно, это может создать некоторые проблемы с параллелизмом и не будет гарантировать, что первый заказ будет выполнен первым. С помощью очереди можно определить порядок их обработки.

7. Сбор аналитической информации
Очереди часто применяют для сбора некоторой статистики, например использования определенной системы и ее функций. Как правило, моментальная обработка такой информации не требуется. Когда сообщения поступают в веб-службу, они помещаются в очередь, а затем при помощи дополнительных серверов приложений обрабатываются и отправляются в базу данных.

8. Разбиение трудоемких задач на множество маленьких частей
Если у вас есть некоторая задача для группы серверов, то вам необходимо выполнить ее на каждом сервере. Например, при редактировании шаблона мониторинга потребуется обновить мониторы на каждом сервере, использующем этот шаблон. Вы можете поставить сообщение в очередь для каждого сервера и выполнять их одновременно в виде небольших операций.

9. Прочие сценарии, требующие гарантированной доставки информации и высокого уровня отказоустойчивости
Это обработка финансовых транзакций, бронирование авиабилетов, обновление записей о пациентах в сфере здравоохранения и так далее.

**Сложности использования и недостатки очередей сообщений: как с ними справляться**

Несмотря на многочисленные преимущества очередей сообщений, самостоятельное их внедрение может оказаться довольно сложной задачей по нескольким причинам:

* По сути, это еще одна система, которую необходимо купить/установить, правильно сконфигурировать и поддерживать. Также потребуются дополнительные мощности.
* Если брокер когда-либо выйдет из строя, это может остановить работу многих систем, взаимодействующих с ним. Как минимум необходимо позаботиться о резервном копировании данных.
* С ростом числа очередей усложняется и отладка. При синхронной обработке сразу очевидно, какой запрос вызвал сбой, например, благодаря иерархии вызовов в IDE. В очередях потребуется позаботиться о системе трассировки, чтобы быстро связать несколько этапов обработки одного запроса для обнаружения причины ошибки.
* При использовании очередей вы неизбежно столкнетесь с выбором стратегии доставки сообщений. В идеале сообщения должны обрабатываться каждым потребителем однократно. Но на практике это сложно реализовать из-за несовершенства сетей и прочей инфраструктуры. Большинство брокеров поддерживают две стратегии: доставка хотя бы раз (At-least-once) или максимум раз (At-most-once). Первая может привести к дубликатам, вторая — к потере сообщений. Обе требуют тщательного мониторинга. Некоторые брокеры также гарантируют строго однократную доставку (Exactly-once) с использованием порядковых номеров пакетов данных, но даже в этом случае требуется дополнительная проверка на стороне получателя.

**В каких случаях очереди неэффективны**  

Конечно, очереди не являются универсальным средством для любых приложений. Рассмотрим варианты, когда очереди не будут самым эффективным решением:

* У вашего приложения простая архитектура и функции, и вы не ожидаете его роста. Важно понимать, что очереди сообщений — это дополнительная сложность. Эту систему также необходимо настраивать, поддерживать, осуществлять мониторинг ее работы и так далее. Да, можно использовать Managed-решение, но вряд ли это будет оправдано для небольших приложений. Добавление очередей должно упрощать архитектуру, а не усложнять ее.
* Вы используете монолитное программное обеспечение, в котором развязка (Decoupling) невозможна или не приоритетна. Если вы не планируете разбивать монолит на микросервисы, но вам требуется асинхронность — для ее реализации обычно достаточно стандартной многопоточной модели. Очереди могут оказаться избыточным решением до тех пор, пока не возникнет явная необходимость в разделении приложения на автономные компоненты, способные независимо выполнять задачи.

## Что происходит в тот момент, когда вы вводите в адресной строке браузера URL сайта и нажимаете ввод?
<a id="www"></a>

([наверх](#sections))

## Виды сетевых протоколов
<a id="types-of-network-protocols"></a>

([наверх](#sections))

TCP/IP – совокупность протоколов передачи информации. TCP/IP – это особое обозначение всей сети, которая функционирует на основе протоколов TCP, а также IP.

TCP – вид протокола, который является связующим звеном для установки качественного соединения между 2 устройствами, передачи данных и верификации их получения.

IP – протокол, в функции которого входит корректность доставки сообщений по выбранному адресу. При этом информация делится на пакеты, которые могут поставляться по-разному.

MAC – вид протокола, на основании которого происходит процесс верификации сетевых устройств. Все устройства, которые подключены к сети Интернет, содержат свой оригинальный MAC-адрес.

ICMP – протокол, который ответственен за обмен данными, но не используется для процесса передачи информации.

UDP – протокол, управляющий передачей данных, но данные не проходят верификацию при получении. Этот протокол функционирует быстрее, чем протокол TCP.

HTTP – протокол для передачи информации (гипертекста), на базе которого функционируют все сегодняшние сайты. В его возможности входит процесс запрашивания необходимых данных у виртуально удаленной системы (файлы, веб-страницы и прочее).

FTP – протокол передачи информации из особого файлового сервера на ПК конечного пользователя.

POP3 – классический протокол простого почтового соединения, который ответственен за передачу почты.

SMTP – вид протокола, который может устанавливать правила для передачи виртуальной почты. Он ответственен за передачу и верификацию доставки, а также оповещения о возможных ошибках.

# Big Data
<a id="Big-Data"></a>
    
    
* [Что такое DWH](#dwh)  
* [Data Lake](#data-lake)
* [Витрины данных](#data-marts)
* [ETL и ETL-запросы](#ETL)
* [Что такое Hadoop?](#hadoop)
* [Data Vault](#Data-Vault)
   * [Распределенная файловая система HDFS](#distributed-file-system-HDFS)
     * [Архитектура HDFS](#HDFS-architecture)
     * [Shell-команды](#Shell-commands)
     * [Java API](#Java_API1)
   * [MapReduce](#MapReduce)
     * [Парадигма MapReduce](#MapReduce-paradigm)
     * [Фреймворк MapReduce](#MapReduce-framework)
     * [Java API](#Java_API2)
     * [Hadoop Streaming](#Hadoop-Streaming)
   * [Решение задач с помощью MapReduce](#Solving-problems-with-MapReduce)
     * [Алгоритмы на MapReduce](#Algorithms-on-MapReduce)
     * [Реляционные функции](#Relational-functions)
     * [Расчет TF-IDF](#TF-IDF-calculation)
   * [Алгоритмы на графах в MapReduce](#Graph-Algorithms-in-MapReduce)
     * [Графы в MapReduce](#Graphs-in-MapReduce)
     * [Поиск кратчайшего пути в графе](#Finding-the-shortest-path-in-a-graph)
     * [PageRank](#PageRank)
     * [Проблемы MR-алгоритмов на графах](#Problems-of-MR-algorithms-on-graphs)
   * [Pig и Hive](#Pig-and-Hive)
     * [Pig](#Pig)
     * [Основные операторы PigLatin](#Basic-operators-of-PigLatin)
     * [Hive](#Hive)
     * [Pig vs Hive](#Pig-vs-Hive)
   * [NoSQL базы данных: HBase и Cassandra](#HBase-and-Cassandra)
     * [Способы хранения данных](#Data-storage-methods)
     * [NoSQL](#NoSQL)
     * [Введение в HBase](#Introduction-to-HBase)
     * [Архитектура HBase](#HBase-architecture)
     * [Cassandra](#Cassandra)
   * [Spark](#Spark)
     * [Основные понятия Spark](#Basic-concepts-of-Spark)
     * [Операторы Spark](#Spark-Operators)
     * [Фреймворк Spark](#Spark-framework)
   * [YARN. MapReduce 2.0](#YARN)
     * [Что такое YARN?](#What-is-YARN)
     * [Компоненты YARN](#YARN-components)
     * [MapReduce 2.0](#MapReduce-2.0)

([наверх](#sections))

## Что такое DWH
<a id="dwh"></a>

DWH — Data warehouse — Корпоративное хранилище данных (КХД) — склад всех нужных и важных для принятия решений данных компании.  

Потребность в КХД сформировалась примерно в 90-х годах прошлого века, когда в секторе enterprise стали активно использоваться разные информационные системы для учета множества бизнес-показателей. Каждое такое приложение успешно решало задачу автоматизации локального производственного процесса, например, выполнение бухгалтерских расчетов, проведение транзакций, HR-аналитика и т.д.  

При этом схемы представления (модели) справочных и транзакционных данных в одной системе могут кардинально отличаться от другой, что влечет расхождение информации. Кроме того, большое разнообразие моделей данных затрудняет получение консолидированной отчетности, когда нужна целостная картина из всех прикладных систем. Поэтому возникли корпоративные хранилища данных (Data Warehouse, DWH) – предметно-ориентированные базы данных для консолидированной подготовки отчётов, интегрированного бизнес-анализа и оптимального принятия управленческих решений на основе полной информационной картины.

__Архитектура КХД__

Вышеприведенное определение DWH показывает, что это средство хранения данных является реляционным. Однако, не стоит считать КХД просто большой базой данных с множеством взаимосвязанных таблиц. В отличие от традиционной SQL-СУБД, Data Warehouse имеет сложную многоуровневую (слоеную) архитектуру, которая называется LSA – Layered Scalable Architecture. По сути, LSA реализует логическое деление структур с данными на несколько функциональных уровней. Данные копируются с уровня на уровень и трансформируются при этом, чтобы в итоге предстать в виде согласованной информации, пригодной для анализа.

Классически LSA реализуется в виде следующих уровней:

1. Операционный слой первичных данных(Primary Data Layer или стейджинг)  
Здесь выполняется загрузка информации из систем-источников в исходном качестве и сохранением полной истории изменений. Здесь происходит абстрагирование следующих слоев хранилища от физического устройства источников данных, способов их сбора и методов выделения изменений.
2. Ядро хранилища (Core Data Layer)  
Центральный компонент, который выполняет консолидацию данныхиз разных источников, приводя их к единым структурам и ключам. Именно здесь происходит основная работа с качеством данных и общие трансформации, чтобы абстрагировать потребителей от особенностей логического устройства источников данных и необходимости их взаимного сопоставления. Так решается задача обеспечения целостности и качества данных.
3. Аналитические витрины (Data Mart Layer)  
Тут данные преобразуются к структурам, удобным для анализа и использования в BI-дэшбордах или других системах-потребителях. Когда витрины берут данные из ядра, они называются регулярными. Если же для быстрого решения локальных задач не нужна консолидация данных, витрина может брать первичные данные из операционного слоя и называется соответственно операционной. Также бывают вторичные витрины, которые используются для представления результатов сложных расчетов и нетипичных трансформаций. Таким образом, витрины обеспечивают разные представления единых данных под конкретную бизнес-специфику.
4. Сервисный слой (Service Layer)  
Обеспечивает управление всеми вышеописанными уровнями. Он не содержит бизнес-данных, но оперирует метаданными и другими структурами для работы с качеством данных, позволяя выполнять сквозной аудит данных (data lineage), использовать общие подходы к выделению дельты изменений и управления загрузкой. Также здесь доступны средства мониторинга и диагностики ошибок, что ускоряет решение проблем.


__LSA – слоеная архитектура DWH: как устроено хранилище данных__  
![LSA – слоеная архитектура DWH: как устроено хранилище данных](https://www.bigdataschool.ru/wp-content/uploads/2020/04/%D0%B4%D0%B2%D1%85_1.png)

Все слои, кроме сервисного, состоят из области постоянного хранения данных и модуля загрузки и трансформации. Области хранения содержат технические (буферные) таблицы для трансформации данных и целевые таблицы, к которым обращается потребитель. Для обеспечения процессов загрузки и аудита ETL-процессов данные в целевых таблицах стейджинга, ядра и витринах маркируются техническими полями (мета-атрибутами). Еще выделяют слой виртуальных провайдеров данных и пользовательских отчетов для виртуального объединения (без хранения) данных из различных объектов. Каждый уровень может быть реализован с помощью разных технологий хранения и преобразования данных или универсальных продуктов, например, SAP NetWeaver Business Warehouse (SAP BW).

__В чём разница между обычной базой данных и DWH__

1. Типы хранимых данных.   
Обычные СУБД хранят данные строго для определенных подсистем. База данных склада хранит складские запасы и ничего более. База данных кадровиков хранит данные по персоналу, но не товары или сделки. DWH, как правило, хранит информацию разных подразделений — там найдутся данные и по товарам, и по персоналу, и по сделкам.
2. Объемы данных.  
Обычная БД, которая ведется в рамках стандартной деятельности компании, содержит только актуальную информацию, нужную в данный момент для функционирования определенной системы. В DWH пишутся не столько копии актуальных состояний, сколько исторические данные и агрегированные значения. Например, состояние запасов разных категорий товаров на конец смены за последние пять лет. Иногда в DWH пишутся и более крупные пачки данных, если они имеют критическое значение для бизнеса — допустим, полные данные по продажам и сделкам. То есть, по сути, это копия СУБД отдела продаж.
3. Место в рабочих процессах.  
Информация обычно сразу попадает в рабочие базы данных, а уже оттуда некоторые записи переползают в DWH. Склад данных, по сути, отражает состояние других БД и процессов в компании уже после того, как вносятся изменения в рабочих базах.

DWH — это система данных, отдельная от оперативной системы обработки данных. В корпоративных хранилищах в удобном для анализа виде хранятся архивные данные из разных, иногда очень разнородных источников. Эти данные предварительно обрабатываются и загружаются в хранилище в ходе процессов извлечения, преобразования и загрузки, называемых ETL. Решения ETL и DWH — это (упрощенно) одна система для работы с корпоративной информацией и ее хранения.

__Что дают DWH-решения для BI и принятия решений в компании__

Понятное дело, что просто так тратить деньги и время на консервирование кучи разных записей, которые и так можно накопать в других базах данных, никто не станет. Ответ заключается в том, что DWH необходима для того, чтобы делать BI — business intelligence.  

Что такое BI с DWH? Бизнес-аналитика (BI) — это процесс анализа данных и получения информации, помогающей компаниям принимать решения.  

Допустим, у вас в онлайн-магазине упала выручка. Менеджеры зовут на помощь бизнес-аналитика и просят его разобраться. Тот идет в DWH, вынимает оттуда данные по продажам, выручке, количеству пользователей, расходам — и собирает отчет, который в подробностях и с цифрами говорит о причинах падения финансовых показателей. Менеджеры внимательно смотрят на эту информацию и принимают решения по реорганизации ассортимента товаров и маркетинговых политик.
Если бы такого аналитического отчета не было — управленцам пришлось бы искать проблему наугад.

Логичный вопрос: казалось бы, зачем держать для этого всего DWH? Аналитики вполне могут ходить в базы данных разных систем и просто выдергивать оттуда то, что им надо.

Ответ: так, конечно, тоже можно делать. Но — не нужно. И вот почему:

1. Доступ к нужным данным.   
Если компания большая, на получение данных из разных источников нужно собирать разрешения и доступы. У каждого подразделения в такой ситуации, как правило, свои базы данных со своими паролями, которые надо будет запрашивать отдельно. В DWH все нужное уже будет под рукой в готовом виде. Можно просто пойти и дернуть там необходимую статистику.
2. Сохранность нужных данных.   
Данные в DWH не теряются и хранятся в виде, удобном для принятия решений: есть исторические записи, есть агрегированные значения. В операционной базе данных такой информации может и не быть. Например, админы уж точно не будут хранить на складском сервере архив запасов за 10 лет — БД склада в таком случае была бы слишком тяжелой. А вот хранить агрегированные запасы со склада в DWH — это нормально.
3. Устойчивость работы бизнес-систем.   
DWH оптимизируется для работы аналитиков, а эти ребята могут запрашивать очень большие объемы информации. Если они будут делать это с помощью DWH — ничего страшного, даже если их запрос будет обрабатываться очень долго. А если запросить слишком много записей с боевой базы данных сервера — он может уйти в отказ до конца выполнения запроса от аналитики и создать проблемы для других систем. DWH исключает риск того, что аналитики что-то повесят или сломают.   

__Почему бизнес-аналитика невозможна без DWH__

DWH и бизнес-аналитики переводят управление компаниями из искусства в науку. Имея под рукой результаты измерений по сотням показателей, можно выдвигать гипотезы и ставить эксперименты. Правильные решения легко подтверждаются объективными цифрами, которые достают аналитики из DWH.

Оптимальные управленческие решения — это не всегда максимизация прибыли. Это еще и выращивание новых производственных мощностей, минимизация негативного влияния на экологию, достойное качество жизни сотрудников, лояльность клиентов и стабильность бизнеса в долгосрочной перспективе. Все эти, казалось бы, сложные и эфемерные показатели можно анализировать с помощью BI и данных из DWH.

Без DWH и аналитиков управление бизнесом превращается в слепую езду по льду — возможно, при определенной сноровке вы попадете куда надо, но шансов улететь в сугроб или в столб все же куда больше.  

## Data Lake
<a id="data-lake"></a>
([наверх](#sections))

В 2010-х годах, с наступлением эпохи Big Data, фокус внимания от традиционных DWH сместился озерам данных (Data Lake). Однако, считать озеро данных новым поколением КХД не совсем корректно по следующим причинам:

1. Разное целевое назначение  
DWH используется менеджерами, аналитиками и другими конечными бизнес-пользователями, тогда как озеро данных – в основном Data Scientist’ами. Напомним, в Data Lake хранится неструктурированная, т.н. сырая информация: видеозаписи с беспилотников и камер наружного наблюдения, транспортная телеметрия, графические изображения, логи пользовательского поведения, метрики сайтов и информационных систем, а также прочие данные с разными форматами хранения (схемами представления). Они пока непригодны для ежедневной аналитики в BI-системах, но могут использоваться Data Scientist’ами для быстрой отработки новых бизнес-гипотез с помощью алгоритмов машинного обучения;
2. Разные подходы к проектированию   
Дизайн DWH основан на реляционной логике работы с данными – третья нормальная форма для нормализованных хранилищ, схемы звезды или снежинки для хранилищ с измерениями. При проектировании озера данных архитектор Big Data и Data Engineer большее внимание уделяют ETL-процессам с учетом многообразия источников и приемников разноформатной информации. А вопрос ее непосредственного хранения решается достаточно просто – требуется лишь масштабируемая, отказоустойчивая и относительно дешевая файловая система, например, HDFS или Amazon S3;
3. Цена  
обычно Data Lake строится на базе бюджетных серверов с Apache Hadoop, без дорогостоящих лицензий и мощного оборудования, в отличие от больших затрат на проектирование и покупку специализированных платформ класса Data Warehouse, таких как SAP, Oracle, Teradata и пр.

Таким образом, озеро данных существенно отличается от КХД. Тем не менее, архитектурный подход LSA может использоваться и при построении Data Lake. Например, именно такая слоенная структура была принята за основу озера данных в Тинькоф-банке:

* на уровне RAW хранятся сырые данные различных форматов (tsv, csv, xml, syslog, json и т.д.);
* на операционном уровне (ODD, Operational Data Definition) сырые данные преобразуются в приближенный к реляционному формат;
* на уровне детализации (DDS, Detail Data Store) собирается консолидированная модель детальных данных;
* уровень MART выполняет роль прикладных витрин данных для бизнес-пользователей и моделей машинного обучения.  

В данном примере для структурированных запросов к большим данным используется Apache Hive – популярное средство класса SQL-on-Hadoop. Само файловое хранилище организовано в кластере Hadoop на основе коммерческого дистрибутива от Cloudera (CDH). Традиционное DWH банка реализовано на массивно-параллельной СУБД Greenplum. От себя добавим, что альтернативой Apache Hive могла выступить Cloudera Impala, которая также, как Greenplum, Arenadata DB и Teradata, основана на массивно-параллельной архитектуре. Впрочем, выбор Hive обоснован, если требовалась высокая отказоустойчивость и большая пропускная способность. Подробнее о сходствах и различиях Apache Hive и Cloudera Impala мы рассказывали здесь. Возвращаясь к кейсу Тинькофф-банка, отметим, что BI-инструменты считывают данные из озера и классического DWH, обогащая типичные OLAP-отчеты информацией из хранилища Big Data. Это используется для анализа интересов, прогнозирования поведения, а также выявления текущих и будущих потребностей, которые возникают у посетителей сайта банка.

## Витрины данных
<a id="data-marts"></a>
([наверх](#sections))

Витрины данных — подмножество (срез) хранилища данных, представляющее собой массив тематической, узконаправленной информации, ориентированной, например, на пользователей одной рабочей группы или департамента.

Концепция имеет ряд несомненных достоинств:

* Аналитики видят и работают только с теми данными, которые им реально нужны.
* Целевая БД максимально приближена к конечному пользователю.
* Витрины данных обычно содержат тематические подмножества заранее агрегированных данных, их проще проектировать и настраивать.
* Для реализации витрин данных не требуется высокомощная вычислительная техника.
Но концепция витрин данных имеет и очень серьёзные пробелы. По существу, здесь предполагается реализация территориально распределённой информационной системы с мало контролируемой избыточностью, но не предлагается способов, как обеспечить целостность и непротиворечивость хранимых в ней данных.

## ETL и ETL-запросы
<a id="ETL"></a>
([наверх](#sections))

__ETL__

В переводе ETL (Extract, Transform, Load) — извлечение, преобразование и загрузка. То есть процесс, с помощью которого данные из нескольких систем объединяют в единое хранилище данных.

Представьте ритейлера с розничными и интернет-магазинами. Ему нужно анализировать тенденции продаж и онлайн, и офлайн. Но бэкэнд-системы для них, скорее всего, будут отдельными. Они могут иметь разные поля или форматы полей для сбора данных, использовать системы, которые не могут «общаться» друг с другом.

И вот тогда наступает момент для ETL.

ETL-система извлекает данные из обеих систем, преобразует их в соответствии с требованиями к формату хранилища данных, а затем загружает в это хранилище.

Схема всегда выглядит так: сначала извлечение данных из одного или нескольких источников, потом их подготовка к интеграции, после этого идет загрузка, и извлеченные данные попадают в общую базу.

__ETL на практике__  

Современные инструменты ETL собирают, преобразуют и хранят данные из миллионов транзакций в самых разных источниках данных и потоках. Эта возможность предоставляет множество новых возможностей: анализ исторических записей для оптимизации процесса продаж, корректировка цен и запасов в реальном времени, использование машинного обучения и искусственного интеллекта для создания прогнозных моделей, разработка новых потоков доходов, переход в облако и многое другое.

**Облачная миграция** Процесс переноса данных и приложений в облако называют облачной миграцией. Она помогает сэкономить деньги, сделать приложения более масштабируемыми и защитить данные. ETL в таком случае используют для перемещения данных в облако.

**Хранилище данных** Хранилище данных — база данных, куда передают данные из различных источников, чтобы их можно было совместно анализировать в коммерческих целях. Здесь ETL используют для перемещения данных в хранилище данных.

**Машинное обучение** Машинное обучение — метод анализа данных, который автоматизирует построение аналитических моделей. ETL может использоваться для перемещения данных в одно хранилище для машинного обучения.

**Интеграция маркетинговых данных** Маркетинговая интеграция включает в себя перемещение всех маркетинговых данных — о клиентах, продажах, из социальных сетей и веб-аналитики — в одно место, чтобы вы могли проанализировать их. ETL используют для объединения маркетинговых данных.

**Интеграция данных IoT** То есть данных, собранных различными датчиками, в том числе встроенными в оборудование. ETL помогает перенести данные от разных IoT в одно место, чтобы вы могли сделать их подробный анализ.

**Репликация базы данных** — данные из исходных баз данных копируют в облачное хранилище. Это может быть одноразовая операция или постоянный процесс, когда ваши данные обновляются в облаке сразу же после обновления в исходной базе. ETL можно использовать для осуществления процесса репликации данных.

**Бизнес-аналитика** Бизнес-аналитика — процесс анализа данных, позволяющий руководителям, менеджерам и другим заинтересованным сторонам принимать обоснованные бизнес-решения. ETL можно использовать для переноса нужных данных в одно место, чтобы их можно было использовать.

__Популярные ETL-системы__  

**Cloud Big Data** — PaaS-сервис для анализа больших данных (big data) на базе Apache Hadoop, Apache Spark, ClickHouse. Легко масштабируется, позволяет заменить дорогую и неэффективную локальную инфраструктуру обработки данных на мощную облачную инфраструктуру. Помогает обрабатывать структурированные и неструктурированные данные из разных источников, в том числе в режиме реального времени. Развернуть кластер интеграции и обработки данных в облаках можно за несколько минут, управление осуществляется через веб-интерфейс, командную строку или API.

**IBM InfoSphere** — инструмент ETL, часть пакета решений IBM Information Platforms и IBM InfoSphere. Доступен в различных версиях (Server Edition, Enterprise Edition и MVS Edition). Помогает в очистке, мониторинге, преобразовании и доставке данных, среди преимуществ: масштабируемость, возможность интеграции почти всех типов данных в режиме реального времени.

**PowerCenter** — набор продуктов ETL, включающий клиентские инструменты PowerCenter, сервер и репозиторий. Данные хранятся в хранилище, где к ним получают доступ клиентские инструменты и сервер. Инструмент обеспечивает поддержку всего жизненного цикла интеграции данных: от запуска первого проекта до успешного развертывания критически важных корпоративных приложений.

**iWay Software** предоставляет возможность интеграции приложений и данных для удобного использования в режиме реального времени. Клиенты используют их для управления структурированной и неструктурированной информацией. В комплект входят: iWay DataMigrator, iWay Service Manager и iWay Universal Adapter Framework.

**Microsoft SQL Server** — платформа управления реляционными базами данных и создания высокопроизводительных решений интеграции данных, включающая пакеты ETL для хранилищ данных.

**OpenText** — платформа интеграции, позволяющая извлекать, улучшать, преобразовывать, интегрировать и переносить данные и контент из одного или нескольких хранилищ в любое новое место назначения. Позволяет работать со структурированными и неструктурированными данными, локальными и облачными хранилищами.

**Oracle GoldenGate** — комплексный программный пакет для интеграции и репликации данных в режиме реального времени в разнородных IT-средах. Обладает упрощенной настройкой и управлением, поддерживает облачные среды.

**Pervasive Data Integrator** — программное решение для интеграции между корпоративными данными, сторонними приложениями и пользовательским программным обеспечением. Data Integrator поддерживает сценарии интеграции в реальном времени.

**Pitney Bowes** предлагает большой набор инструментов и решений, нацеленных на интеграцию данных. Например, Sagent Data Flow — гибкий механизм интеграции, который собирает данные из разнородных источников и предоставляет полный набор инструментов преобразования данных для повышения их коммерческой ценности.

**SAP Business Objects** — централизованная платформа для интеграции данных, качества данных, профилирования данных, обработки данных и отчетности. Предлагает бизнес-аналитику в реальном времени, приложения для визуализации и аналитики, интеграцию с офисными приложениями.

**Sybase** включает Sybase ETL Development и Sybase ETL Server. Sybase ETL Development — инструмент с графическим интерфейсом для создания и проектирования проектов и заданий по преобразованию данных. Sybase ETL Server — масштабируемый механизм, который подключается к источникам данных, извлекает и загружает данные в хранилища.

__Open source ETL-средства__

Большинство инструментов ETL с открытым исходным кодом помогают в управлении пакетной обработкой данных и автоматизации потоковой передачи информации из одной системы данных в другую. Эти рабочие процессы важны при создании хранилища данных для машинного обучения.  

Некоторые из бесплатных и открытых инструментов ETL принадлежат поставщикам, которые в итоге хотят продать корпоративный продукт, другие обслуживаются и управляются сообществом разработчиков, стремящихся демократизировать процесс.  

Open source ETL-инструменты интеграции данных:  

**Apache Airflow** — платформа с удобным веб-интерфейсом, где можно создавать, планировать и отслеживать рабочие процессы. Позволяет пользователям объединять задачи, которые нужно выполнить в строго определенной последовательности по заданному расписанию. Пользовательский интерфейс поддерживает визуализацию рабочих процессов, что помогает отслеживать прогресс и видеть возникающие проблемы.  

**Apache Kafka** — распределенная потоковая платформа, которая позволяет пользователям публиковать и подписываться на потоки записей, хранить потоки записей и обрабатывать их по мере появления. Kafka используют для создания конвейеров данных в реальном времени. Он работает как кластер на одном или нескольких серверах, отказоустойчив и масштабируем.  

**Apache NiFi** — распределенная система для быстрой параллельной загрузки и обработки данных с большим числом плагинов для источников и преобразований, широкими возможностями работы с данными. Пользовательский веб-интерфейс NiFi позволяет переключаться между дизайном, управлением, обратной связью и мониторингом.  

**CloverETL** (теперь CloverDX) был одним из первых инструментов ETL с открытым исходным кодом. Инфраструктура интеграции данных, основанная на Java, разработана для преобразования, отображения и манипулирования данными в различных форматах. CloverETL может использоваться автономно или встраиваться и подключаться к другим инструментам: RDBMS, JMS, SOAP, LDAP, S3, HTTP, FTP, ZIP и TAR. Хотя продукт больше не предлагается поставщиком, его можно безопасно загрузить с помощью SourceForge. CloverDX по-прежнему поддерживает CloverETL в соответствии со стандартным соглашением о поддержке.  

**Jaspersoft ETL** — один из продуктов с открытым исходным кодом TIBCO Community Edition, позволяет пользователям извлекать данные из различных источников, преобразовывать их на основе определенных бизнес-правил и загружать в централизованное хранилище данных для отчетности и аналитики. Механизм интеграции данных инструмента основан на Talend. Community Edition прост в развертывании, позволяет создавать витрины данных для отчетности и аналитики.  

**Apatar** — кроссплатформенный инструмент интеграции данных с открытым исходным кодом, который обеспечивает подключение к различным базам данных, приложениям, протоколам, файлам. Позволяет разработчикам, администраторам баз данных и бизнес-пользователям интегрировать информацию разного формата из различных источников данных. У инструмента интуитивно понятный пользовательский интерфейс, который не требует кодирования для настройки заданий интеграции данных. Инструмент поставляется с предварительно созданным набором инструментов интеграции и позволяет пользователям повторно использовать ранее созданные схемы сопоставления.

## Что такое Hadoop?
<a id="hadoop"></a>
([наверх](#sections))

Hadoop - инструмент для обработки Big Data. Hadoop - это проект Apache, является системой для распределённых вычислений. При этом эта система является масштабируемой и отказоустойчивой. 

__История Hadoop__  
Начинался как проект в Apache Nutch  
В 2004 году Google публикует статьи про GFS и MapReduce  
На основе этих статей формируется распределённая файловая система  

__Системные принципы Hadoop__  
* Горизонтальное (Scale-out) масштабирование вместо вертикального (Scale-Up)
* Отправление кода к данным
* Умение обрабатывать падения нод и отказы оборудования
* Инкапсуляция сложности работы распределённых и многопоточных приложений

__Масштабирование__  
* Вертикальное  
  - Добавить дополнительные ресурсы к существующему железу (CPU, RAM)  
  - Если нельзя улучшить железо, то надо покупать более мощное новое  
  - Закон Мура не успевает за ростом объёма данных  
* Горизонтальное 
  - Добавить больше машин к существующему кластеру
  - Приложение поддерживает добавлние/удаление серверов
  - Просто масштабировать "вниз"

## Data Vault
<a id="data-vault"></a>

([наверх](#sections))

Большинство компаний сегодня накапливают различные данные, полученные в процессе работы. Часто данные приходят из различных источников — структурированные и не очень, иногда в режиме реального времени, а иногда они доступны в строго определенные периоды. Все это разнообразие нужно структурированно хранить, чтоб потом успешно анализировать, рисовать красивые отчеты и вовремя замечать аномалии. Для этих целей проектируется хранилище данных (Data Warehouse, DWH).

Data Vault - это один из подходов к построению такого универсального хранилища. 

Data Vault состоит из трех основных компонентов — **Хаб** (Hub), **Ссылка** (Link) и **Сателлит** (Satellite).

**Хаб**

Хаб — основное представление сущности (Клиент, Продукт, Заказ) с позиции бизнеса. Таблица-Хаб содержит одно или несколько полей, отражающих сущность в понятиях бизнеса. В совокупности эти поля называются «бизнес ключ». Идеальный кандидат на звание бизнес-ключа это ИНН организации или VIN номер автомобиля, а сгенерированный системой ID будет наихудшим вариантом. Бизнес ключ всегда должен быть уникальным и неизменным.
Хаб так же содержит мета-поля _load timestamp_ и _record source_, в которых хранятся время первоначальной загрузки сущности в хранилище и ее источник (название системы, базы или файла, откуда данные были загружены). 

Таблицы Хабы  

![Таблицы Хабы](https://habrastorage.org/r/w780/webt/8q/r3/ik/8qr3ikyx5nwg2dsjaqbxdrw7y4a.png)

**Ссылка**

Таблицы-Ссылки связывают несколько хабов связью многие-ко-многим. Она содержит те же метаданные, что и Хаб. Ссылка может быть связана с другой Ссылкой, но такой подход создает проблемы при загрузке, так что лучше выделить одну из Ссылок в отдельный Хаб.

Таблица-ссылка  

![Таблица-ссылка](https://habrastorage.org/r/w780/webt/sr/q1/p2/srq1p2pdfcgx-xmqjl6xrsh_hpq.png)

**Сателлит**

Таблицы-Сателлиты содержат все описательные атрибуты Хаба или Ссылки (контекст). Помимо контекста Сателлит содержит стандартный набор метаданных (_load timestamp_ и _record source_) и один и только один ключ «родителя». В Сателлитах можно без проблем хранить историю изменения контекста, каждый раз добавляя новую запись при обновлении контекста в системе-источнике. Для упрощения процесса обновления большого сателлита в таблицу можно добавить поле hash diff: MD5 или SHA-1 хеш от всех его описательных атрибутов. Для Хаба или Ссылки может быть сколь угодно Сателлитов, обычно контекст разбивается по частоте обновления. Контекст из разных систем-источников принято класть в отдельные Сателлиты.

Таблицы-Сателлиты  

![Таблицы-Сателлиты](https://habrastorage.org/r/w780/webt/kl/fa/7r/klfa7re28amqqotkvwxovutlpo8.png)

Таблицы Data Vault: хабы, ссылки, спутники  

![Таблицы Data Vault: хабы, ссылки, спутники](https://www.bigdataschool.ru/wp-content/uploads/2020/04/%D0%B4%D0%B0%D1%82%D0%B0%D0%B2%D0%BE%D0%BB_1.png)

**Как с этим работать?**

![Building a Scalable Data Warehouse with Data Vault 2.0](https://habrastorage.org/r/w780/webt/2v/q_/kv/2vq_kviv6uebjj_m1svk5rafubi.png)

Сначала данные из операционных систем поступают в staging area. Staging area используется как промежуточное звено в процессе загрузки данных. Одна из основных функций Staging зоны это уменьшение нагрузки на операционные базы при выполнении запросов. Таблицы здесь полностью повторяют исходную структуру, но любые ограничения на вставку данных, вроде not null или проверки целостности внешних ключей, должны быть выключены с целью оставить возможность вставить даже поврежденные или неполные данные (особенно это актуально для excel-таблиц и прочих файлов). Дополнительно в stage таблицах содержатся хеши бизнес ключей и информация о времени загрузки и источнике данных.

После этого данные разбиваются на Хабы, Ссылки и Сателлиты и загружаются в Raw Data Vault. В процессе загрузки они никак не агрегируются и не пересчитываются.

Business Vault — опциональная вспомогательная надстройка над Raw Data Vault. Строится по тем же принципам, но содержит переработанные данные: агрегированные результаты, сконвертированные валюты и прочее. Разделение чисто логическое, физически Business Vault находится в одной базе с Raw Data Vault и предназначен в основном для упрощения формирования витрин.

Когда нужные таблицы созданы и заполнены, наступает очередь витрин данных (Data Marts). Каждая витрина это отдельная база данных или схема, предназначенная для решения задач различных пользователей или отделов. В ней может быть специально собранная «звезда» или коллекция денормализованных таблиц. Если возможно, таблицы внутри витрин лучше делать виртуальными, то есть вычисляемыми «на лету». Для этого обычно используются SQL представления (SQL views).

**Заполнение Data Vault**

Cначала загружаются Хабы, потом Ссылки и затем Сателлиты. Хабы можно загружать параллельно, так же как и Сателлиты и Ссылки, если конечно не используется связь link-to-link.

Есть вариант и вовсе выключить проверку целостности и загружать все данные одновременно. Как раз такой подход соответствует одному из основных постулатов DV — «Загружать все доступные данные все время (Load all of the data, all of the time)» и именно здесь играют решающую роль бизнес ключи. Суть в том, что возможные проблемы при загрузке данных должны быть минимизированы, а одна из наиболее распространенных проблем это нарушение целостности. Подход, конечно, спорный, но лично я им пользуюсь и нахожу действительно удобным: данные все равно проверяются, но после загрузки. Часто можно столкнуться с проблемой отсутствия записей в нескольких Хабах при загрузке Ссылок и последовательно разбираться, почему тот или иной Хаб не заполнен до конца, перезапуская процесс и изучая новую ошибку. Альтернативный вариант — вывести недостающие данные уже после загрузки и увидеть все проблемы за один раз. Бонусом получаем устойчивость к ошибкам и возможность не следить за порядком загрузки таблиц.

**Преимущества и недостатки**

[+] Гибкость и расширяемость.  
С Data Vault перестает быть проблемой как расширение структуры хранилища, так и добавление и сопоставление данных из новых источников. Максимально полное хранилище «сырых» данных и удобная структура их хранения позволяют нам сформировать витрину под любые требования бизнеса, а существующие решения на рынке СУБД хорошо справляются с огромными объемами информации и быстро выполняют даже очень сложные запросы, что дает возможность виртуализировать большинство витрин.  
[+] Agile-подход из коробки.  
Моделировать хранилище по методологии Data Vault довольно просто. Новые данные просто «подключаются» к существующей модели, не ломая и не модифицируя существующую структуру. При этом мы будем решать поставленную задачу максимально изолированно, загружая только необходимый минимум, и, вероятно, наша временнáя оценка для такой задачи станет точнее. Планирование спринтов будет проще, а результаты предсказуемы с первой же итерации.  
[–] Обилие JOIN'ов  
За счет большого количества операций join запросы могут быть медленнее, чем в традиционных хранилищах данных, где таблицы денормализованы.  
[–] Сложность.  
В описанной выше методологии есть множество важных деталей, разобраться в которых вряд ли получится за пару часов. К этому можно прибавить малое количество информации в интернете и почти полное отсутствие материалов на русском языке (надеюсь это исправить). Как следствие, при внедрении Data Vault возникают проблемы с обучением команды, появляется много вопросов относительно нюансов конкретного бизнеса. К счастью, существуют ресурсы, на которых можно задать эти вопросы. Большой недостаток сложности это обязательное требование к наличию витрин данных, так как сам по себе Data Vault плохо подходит для прямых запросов.  
[–] Избыточность.  
Довольно спорный недостаток, но я часто вижу вопросы об избыточности, поэтому прокомментирую этот момент со своей точки зрения.  

## Распределённая файловая система HDFS
<a id="distributed-file-system-HDFS"></a>
* [Архитектура HDFS](#HDFS-architecture)  

([наверх](#sections))

### Архитектура HDFS
<a id="HDFS-architecture"></a>

HDFS (Hadoop Distributed File System) - это распределённая файловая система в hadoop. Как и любая другая файловая система она служит для хранения данных.

HDFS:
* Работает на кластере серверов
* Для пользователя как "Один большой диск"
* Работает поверх обычных файловых систем (ext3, ext4, XFS)  
* Не теряет данные если выходят из строя диски или сервера

HDFS подходит для:  
* Хранения больших данных
   - Терабайты, петабайты
   - Миллионы файлов
   - Файлы размером от 100 Мбэ
* Стриминга данных
   - Паттерн "write once / read many times"
   - Оптимизация под последовательное чтение

HDFS не подходит для:  
* Low-latency reads
   - Высокая пропускная способность вместо быстрого доступа к данным
   - HBase помогает решить эту задачу
* Большого количество небольших файлов
   - Лучше миллион больших файлов, чем миллиард маленьких
* Многопоточная запись
   - Один процесс записи на файл
   - данные дописываются в конец файла

__Демоны HDFS__  
![Демоны HDFS](https://russianblogs.com/images/753/dc2fb07713850c486dd1e421bc6843d9.png)

**Namenode**  
Отвечает за:
* Файловое пространство
* Мета-информацию
* Расположение блоков файлов  
Запускается на 1й выделенной машине

**Datanode**  
Отвечает за:
* Хранение и передачу блоков данных
* Отправку сообщений о состоянии на Namenode

Запускается на каждой машине кластера

**Файлы и блоки**

* Файлы в HDFS состоят из блоков  
  блок - еденица хранения данных
* Управление через Namenode
* Хранится в Datanode

Реплицируются по машинам в процессе записи:  

* Один и тот же блок хранится на нескольких Datanode
* Фактор репликации по умлочанию равен 3
* Это нужно для fault-tolerance и упрощения доступа 

* Стандартный размер блоков 64 Мб или 128 Мб
* Основной мотив этого - снизить стоимость seek time по сравнению со скоростью передачи данных (transfer rate)

**Репликация блоков**

* Namenode определяет, где распологать блоки
* Баланс между надёжностью и производительностью
  - Попытка снизить нагрузку на сеть
  - Попытка улучшить надёжность 

Фактор репликации по умлочанию равен 3

- 1-я реплика на локальную машину
- 2-я реплика на другую машину из той же стойки
- 3-я реплика на другую машину из другой стойки

**Namenode Использование памяти**

Чем больше кластер, тем больше ОЗУ требуется
Больше размер блока -> меньше блоков
Меньше блоков -> больше файлов в FS

Если Namenode падает, то HDFS не работает
Namenode - это едина точка отказа
Она должна быть на отдельной надёжной машиной.

**Доступ к HDFS**

* Direct Access
  - Взаимодействует с HDFS  с помощью нативного клиента
  - Java, C++
  - Клиент запрашивает метаданные от NN
  - Клиент напрямую запрашивает данные от DN
  - Используется для MapReduce
* Через Proxy Server
  - Доступ к HDFS через Proxy Server - middle man
  - ответ в форматие JSON, XML
  - Серверы REST, Thrift и Avro - механизм сериализации

### Shell-команды
<a id="Shell-commands"></a>

([наверх](#sections))

Для работы с HDFS через командную строку

```$hdfs dfs (значит, что будем работать непосредственно с фаловой системой) -<command> -<option><URL>```
```$hdfs dfs -ls / (листинг корневой директории)```
  
**URI**   
  
hdfs://localhost:8020/user/home

Для того чтобы ссылка считалась URI необходимо наличие:
- либо scheme+authority+path,
- либо sheme+path,
- либо только path.
  
Вывод списка команд
```$hdfs dfs - help```
  
Информация по командк
```$hdfs dfs - help <command>```
  
__Основные команды в shell__
  
```ls``` - листинг директории и статистика файлов
```mkdir``` - создать директорию
  ```$hdfs dfs -mkdir /data/new_path```
```cat``` - вывод источника в stdout
  - Весь файл: ```$hdfs dfs -cat /dir/file.txt```
  - Полезно вывод перенаправить через pipe в less, head, tail и т.д.
  - Получить первые 100 строк из файла: ```$hdfs dfs -cat /dir/file.tx | head -n 100```

text - аналог команды cut, который разархивирует архивы
  
```tail``` - выводит последние сроки файла
  ```$hdfs dfs -cat /dir/file.tx | tail``` - плохо
  ```$hdfs dfs -tail /dir/file.tx``` - хорошо
  
```cp``` - копировать файл из одного места в другое
  ```$hdfs dfs -cp /dir/file1 /otherDir/file2```
Подходит только для небольших файлов
  
```distcp``` - копирует большие файлы, или много файлов
  ```$hdfs dfs -distcp /dir/file1 /otherDir/file2```
  
```mv``` - перемещения файла
  ```$hdfs dfs -mv /dir/file1 /dir2```
  
```put```(copyFromLocal) - копирование локального файла в HDFS
  ```$hdfs dfs -put loaclfile /dir/file```
  
```get```(copyToLocal) - копирование файла bp HDFS в локальную FS 
  ```$hdfs dfs -get /dir/file loaclfile```
  
```rm``` - удалить файл в корзину
  ```$hdfs dfs -rm /dir/file```
  
```rm -r``` - рекурсивно удалить директорию
  ```$hdfs dfs -rm -r /dir```
  
```du``` - размер файла или директории в байтах
  ```$hdfs dfs -du /dir```
  
```du -h``` - размер в удобно читаемом формате
  ```$hdfs dfs -du -h /dir```
  
```fsck``` - проверка некосистентности файловой системы. Показывает проблемы. Не устраняет проблем, только информация. 
  ```$hdfs fsck <path>```
  
```dfsadmin``` - команда для администрирования HDFS
   ```$hdfs dfsadmin -<command>```
   ```$hdfs dfsadmin -report``` - отображает статистику по HDFS
   ```$hdfs dfsadmin -safemode``` - включение безопасного режима
  
```balancer``` - балансирует блоки HDFS по серверам
 
### Java_API
<a id="Java_API1"></a>  

([наверх](#sections))
  
Файловая система реализуется в Java Api с помощью абстрактного класса FileSystem
  org.apache.hadoop.fs.FileSystem
  Абстракный класс представляет абстрактную фаловую систему
  Это именно класс, а не интерфейс
  
Hadoop представляет несколько конкретных реализаций:
  * org.apache.hadoop.fs.LocalFileSystem
  Подходит для нативных FS, использующих локальные диски
  * org.apache.hadoop.hdfs.DistributedFileSystem
  Реализация распределённой фаловой системы HDFS
  * org.apache.hadoop.hdfs.HftpFileSystem
  Доступ к HDFS в read-only режиме через HTTP
  * org.apache.hadoop.fs.ftp.FTPFileSystem
  Файловая система поверх FTP-сервера
  
Объект Path представляет файл или директорию
Path - это URI в FS
  
**Объект Configuration**
  Объект Configuration хранит конфигурацию сервера и клиента  
  Использует простую парадигму key-value  
  
**Чтение данных из файла**  
  * Создать объект FileSystem  
  * Открыть InputStream, указывающий на path  
  * Скопировать данные по байтам используя IOUtils  
  * Закрыть InputStream  
  
**Запись данных в файл**
  * Создать объект FileSystem  
  * Открыть OutputStream
    - Указываем на Path из FileSystem  
    - Используем FSDataOutputStream
    - Автоматически создаются все директории в пути, если не существуют
  * Копируем данные по байтам использую IOUtils
  
## MapReduce
<a id="MapReduce"></a>
     * [Парадигма MapReduce](#MapReduce-paradigm)
     
([наверх](#sections))

### Парадигма MapReduce 
<a id="MapReduce-paradigm"></a> 
  
MapReduce - модель распределённых вычислений для обработки больших объёмов данных
MapReduce - не алгоритм, мы можем говорить, что алгоритмы могут быть реализованы с помощью MapReduce
MapReduce используется, когда для вычислений не хватает памяти и возникает необходимость проведения паралельных вычислений

Map - обработка данных
Reduce - свёртка данных
  
Схема MapReduce  
![Схема MapReduce](https://image.slidesharecdn.com/mapreduce-190706072338/95/05-hadoop-mapreduce-mapreduce-2-638.jpg?cb=1562399572)
  
**Входные данные**
  * Входные данные должны быть разделяемы
  * Данные в каждом split должны быть независимы
  * Один воркер обрабатывает один сплит
  * Воркер запускается там, где лежит его сплит
  
**Передача данных между Map и Reduce**
  * Промежуточные данные пишутся на локальный диск, а не в HDFS
  * Для каждого редьюсера маппер создаёт свой файл с данными
  * Данные - это пара (Key, Value)
  * Данные с одним ключом попадают на один редьюсер
  * Редьюсеры начинают работать после завершения всех мапперов
  
![Процесс передачи файлов в MapReduce](https://cdn.edureka.co/blog/wp-content/uploads/2016/11/MapReduce-Way-MapReduce-Tutorial-Edureka.png)  

**Результат MapReduce задачи**
  * Каждый редьюсер пишет в один файл
  * Число редьюсеров задаёт пользователь
  * Данные сохраняются в HDFS
  * Данные вида Key -> Value
  * Формат данных определяется пользователем

### Фреймворк MapReduce 
<a id="MapReduce-framework"></a> 

Фреймворк MapReduce обеспечивает:
  * Подготовку данных
  * Запуск всех нужных воркеров
  * Взаимодействие между маппером и редьюсером
  * Обработку ошибок

_**Работа демонов в MapReduce**_

MapReduce в Hadoop-е основан на HDFS-е, это означает, что на нашем кластере есть машины, сервера, на которых запущены демоны datanode, непосредственно демоны файловой системы HDFS. Работают поверх локальной файловой системы линукс и обсепечивают взаимодействие с фаловой системой HDFS. Кроме того есть отдельный сервер на котором находится демон namenode, не хранит данные, но отвечает за хранение метаинформации (блоки каких файлов где хранятся). В MapReduce существует два типа демонов: 
  * jobtracker - это процесс который в целом отвечает за запуск задачи.  
  * tasktracker - запущен на каждой машине кластера. Отвечает за запуск конкретных воркеров на конкретном сервере.  

Обычно в кластере располагают tasktracker и datanode совместно, таким образом обеспечивается наилучшее взаимодействие между HDFS и MapReduce в Hadoop-е

**jobtracker**
  * Управляет запуском тасков и определяет, на каком tasktracker будет запущен воркер
  * Управляет процессом работы MapReduce задач (jobs)
  * Мониторит прогресс выполнения задач
  * Перезапускает зафейленные или медленные таски

**tasktracker**
  * Отвечает за работу всех worker на одном сервере
  * Получает от jobtracker информацияю о том какой worker на каких данных нужно запустить
  * Посылает в jobtracker статистику о прогруссе выполненения задачи
  * Сообщает в jobtracker об удачном завершении или падении воркера

**Система слотов**
  * Для каждого tasktracker определяется число слотов
  * Таск запускается на одном слоте
  * M маппером + R редьюсеров = N слотов
  * Для каждого слота определяется кол-во потребляемой ОЗУ

**Опциональные функции**
  * partition (k2, v2, |reducers|) -> № of reducer
    - распределяет ключи по редьюсерам
    - часто просто хеш от key: hash(k2) mod n
  * combine
    - Мини-reducers которые выполняются после завершения фазы map
    - Используется в качетсве оптимизации для снижения сетевого трафика на reduce
    - Не должен менять тип ключа и значения

### Hadoop Streaming
<a id="Hadoop-Streaming"></a> 

Hadoop Streaming:
  * Используется стандартный механизм ввода/вывода в Unix для взаимодействия программы и Hadoop
  * Разработка MR задачи почти на любом языке программирования
  * Обычно используется:
    - Для обработки текста
    - При отсутствии опыта программирования на Java
    - Для быстрого написания прототипа

**Streaming в MapReduce**
  * На вход функции ```map()``` данные подаются через стандартный ввод
  * В ```map()``` они обрабатываются построчно
  * Функция ```map()``` пишет пары ```key/value```, разделяемые через символ табуляции, в стандартный вывод
  * На вход функции ```reduce()``` данные подаются через стандартный ввод, отсортированный по ключам
  * Функция ```reduce()``` пишет пары ```key/value``` в стандартный вывод

**Запуск задачи через Streaming Framework**

``` hadoop jar $HADOOP_HOME/hadoop/hadoop-streamin.jar \``` 
``` -D mapred.job.name="Name of job" \``` Название задачи
``` -files smthMap.py, smthReduce.py \``` Файлы, которые нужно донести
``` -input inputfile.txt``` Входные данные
``` -output /tmp/Name of job/ \``` Куда записывать
``` -mapper smthMap.py \``` 
``` -combiner smthReduce.py \``` 
``` -reducer smthReduce.py``` 

## Pig и Hive
<a id="Pig-and-Hive"></a>
([наверх](#sections))

При разработке MapReduce программ необходимо реализовывать одни и теже алгоритмы для разных наборов данных. С помощью эти программ можно запустить MapReduce программы для анализа данных.

### Pig
<a id="Pig"></a>
([наверх](#sections))

Pig - высокоуровневая платформа поверх Hadoop, разработан в Yahoo! в 2006 году
  - Язык программирования высокого уровня Pig Latin
  - Код программы преобразуется в MapReduce задачи

**Для чего нужен Pig?**
  * Для написания задач MapReduce требуются программисты
    - Которые должны уметь думать в стиле "map&reduce"
    - Скорее всего должны знатьязык Java 
  * Pig предоставляет язык, который могут использовать:
    - Аналитики 
    - Data scientist-ы
    - Статистики

**Основные возможности Pig**
  * Join Datasets
  * Sort Datasets
  * Filter
  * Data Types
  * Group By
  * Пользовательские функции

**Компоненты Pig**
  * Pig Latin - набор команд, разработан для описания последовательности преобразования данных
  * Компидятор Pig - преобразует программы на языке Pig Latin в mapReduce задачи
  * Среда выполнения

**Режимы выполнения**
  * Local
    - Запускается в рамках одной JVM
    - Работает исключительно с локальной файловой системой
    - ```$pig -x local```
  * Hadoop(MapReduce)
    - Pig преобразует программу Pig Latin в задачи MapReduce и выполняет их на кластере
    - ```$pig -x mapreduce```

**Запуск Pig**
  * Скрипт
    - Выполняются команды из файла
    - ```$pig script.pig```
  * Grunt
    - Интерактивная оболочка для выполнения команд Pig
    - Можно запускать скрипты из Grunt командной run или exec
  * Embedded
    - Можно выполнять команды Pig, используя класс PigServer
    - Имеется программный доступ к Grunt через класс PigRunner

**Pig Latin**  
_Строительные блоки_  
  - Field (поле) - часть данных
  - Tuple (кортеж) - упорядоченный набор полей, заключённый в скобки "( )"
  - Bag (мешок) - коллекция кортежей, заключённная в скобки "{ }"

_Схожесть с реляционными БД_
  - Bag - это таблица БД
  - Tuple - это строка в таблице
  - Но: Bag не требует, чтобы все tuples содержали одно и тоже число полей

_Операции DUMP и STORE_
  - DUMP - выводит результат на экран
  - STORE - сохраняет результаты (обычно в файл)

### Основные операторы Pig Latin
<a id="Basic-operators-of-PigLatin"></a>
([наверх](#sections))

## Spark
<a id="Spark"></a>
     
### Основные понятия Spark
<a id="Basic-concepts-of-Spark"></a>

([наверх](#sections))

Так же как и MapReduce, Spark предназначен для анализа больших объёвом данных. Также с одной стороны является фреймворком, с другой стороны является неким подходом. Тоесть некоторой парадигмой в которой могут быть реализованы алгоритмы для обработки больших объёмов данных. 

**Преимущества Spark**

  * Следующая ступень в обработке BigData:
    - Интерактивные задачи
    - Интерактивная аналитика
  * Может работать с разными типами данных (текст, графы, базы данных)
  * Может обрабатывать данные по частям (batch) и в потоке (streaming)
  * Имеет 80 высокоуровневых функций для обработки данных (кроме map и reduce)


### Операторы Spark
<a id="Spark-Operators"></a>

([наверх](#sections))


### Фреймворк Spark
<a id="Spark-framework"></a>

([наверх](#sections))




# Бизнес
<a id="business"></a>
([наверх](#sections))

* [UML-диаграммы](#uml)  

## UML-диаграммы
<a id="uml"></a>

* [Диаграмма классов](#class-diagram)  
* [Диаграмма компонентов](#component-diagram)
* [Диаграмма развертывания](#deployment-diagram)
* [Диаграмма объектов](#object-diagram)
* [Диаграмма пакетов](#package-diagram)
* [Диаграмма составной структуры](#composite-structure-diagram)
* [Диаграмма профилей](#profile-diagram)
* [Диаграмма прецедентов](#use-case-diagram)
* [Диаграмма деятельности](#activity-diagram)
* [Диаграмма состояний](#state-diagram)
* [Диаграмма последовательности](#sequence-diagram)
* [Диаграмма Коммуникации](#communication-diagram)
* [Диаграмма обзора взаимодействия](#interaction-overview-chart)
* [Временная диаграмма](#timing-diagram)

([наверх](#sections))

Unified Modeling Language (UML) — унифицированный язык моделирования. Modeling подразумевает создание модели, описывающей объект. Unified (универсальный, единый) — подходит для широкого класса проектируемых программных систем, различных областей приложений, типов организаций, уровней компетентности, размеров проектов. UML описывает объект в едином заданном синтаксисе, поэтому где бы вы не нарисовали диаграмму, ее правила будут понятны для всех, кто знаком с этим графическим языком — даже в другой стране.

**Для чего используется UML?**

Одна из задач UML — служить средством коммуникации внутри команды и при общении с заказчиком. Рассмотрим возможные варианты использования диаграмм. 

* Проектирование. UML-диаграммы помогут при моделировании архитектуры больших проектов, в которой можно собрать как крупные, так и более мелкие детали и нарисовать каркас (схему) приложения. По нему впоследствии будет строиться код.  

* Реверс-инжиниринг — создание UML-модели из существующего кода приложения, обратное построение. Может применяться, например, на проектах поддержки, где есть написанный код, но документация неполная или отсутствует. 

* Из моделей можно извлекать текстовую информацию и генерировать относительно удобочитаемые тексты — документировать. Текст и графика будут дополнять друг друга.
  
![Типы UML диаграмм](https://habrastorage.org/webt/ry/i-/-p/ryi--p6wtfhvszjhcbhaogsdz8w.png)

### Диаграмма классов
<a id="class-diagram"></a>

([наверх](#sections))

Диаграмма классов — это центральная методика моделирования, которая используется практически во всех объектно-ориентированных методах. Эта диаграмма описывает типы объектов в системе и различные виды статических отношений, которые существуют между ними.

Три наиболее важных типа отношений в диаграммах классов (на самом деле их больше), это:

* Ассоциация, которая представляет отношения между экземплярами типов, к примеру, человек работает на компанию, у компании есть несколько офисов.

* Наследование, которое имеет непосредственное соответствие наследованию в Объектно-Ориентированном дизайне.

* Агрегация, которая представляет из себя форму композиции объектов в объектно-ориентированном дизайне.

![Диаграммка классов](https://habrastorage.org/webt/_f/xw/jo/_fxwjox5thnp7l9c5yayfy4pa4m.jpeg)

### Диаграмма компонентов
<a id="component-diagram"></a>

([наверх](#sections))

На языке унифицированного моделирования диаграмма компонентов показывает, как компоненты соединяются вместе для формирования более крупных компонентов или программных систем.

Она иллюстрирует архитектуры компонентов программного обеспечения и зависимости между ними.

Эти программные компоненты включают в себя компоненты времени выполнения, исполняемые компоненты, а также компоненты исходного кода.

![Диаграмма компонентов](https://habrastorage.org/webt/ff/dr/83/ffdr83yesqcv78hua6zxt45tbye.jpeg)

### Диаграмма развертывания
<a id="deployment-diagram"></a>

([наверх](#sections))

Диаграмма развертывания помогает моделировать физический аспект объектно-ориентированной программной системы. Это структурная схема, которая показывает архитектуру системы, как развертывание (дистрибуции) программных артефактов.

Артефакты представляют собой конкретные элементы в физическом мире, которые являются результатом процесса разработки.

Диаграмма моделирует конфигурацию времени выполнения в статическом представлении и визуализирует распределение артефактов в приложении.

В большинстве случаев это включает в себя моделирование конфигураций оборудования вместе с компонентами программного обеспечения, на которых они размещены.

![Диаграмма развертывания](https://habrastorage.org/webt/hu/eq/h9/hueqh9ow4b15ivon2h5jahaxyck.jpeg)

### Диаграмма объектов
<a id="object-diagram"></a>

([наверх](#sections))

Статическая диаграмма объектов является экземпляром диаграммы класса; она показывает снимок подробного состояния системы в определенный момент времени. Разница в том, что диаграмма классов представляет собой абстрактную модель, состоящую из классов и их отношений.

Тем не менее, диаграмма объекта представляет собой экземпляр в конкретный момент, который имеет конкретный характер.Использование диаграмм объектов довольно ограничено, а именно — чтобы показать примеры структуры данных.

![Диаграмма объектов](https://habrastorage.org/webt/4v/lq/wn/4vlqwntp_ip8_jyyqm3s2goxk9a.jpeg)

### Диаграмма пакетов
<a id="package-diagram"></a>

([наверх](#sections))

Диаграмма пакетов — это структурная схема UML, которая показывает пакеты и зависимости между ними.

Она позволяет отображать различные виды системы, например, легко смоделировать многоуровневое приложение.

![Диаграмма пакетов](https://habrastorage.org/webt/x2/sm/5t/x2sm5tb7tz6lgeg0opfnrqmxnm8.jpeg)

### Диаграмма составной структуры
<a id="composite-structure-diagram"></a>

([наверх](#sections))

Диаграмма составной структуры аналогична диаграмме классов и является своего рода диаграммой компонентов, используемой в основном при моделировании системы на микроуровне, но она изображает отдельные части вместо целых классов. Это тип статической структурной диаграммы, которая показывает внутреннюю структуру класса и взаимодействия, которые эта структура делает возможными.

Эта диаграмма может включать внутренние части, порты, через которые части взаимодействуют друг с другом или через которые экземпляры класса взаимодействуют с частями и с внешним миром, и соединители между частями или портами. Составная структура — это набор взаимосвязанных элементов, которые взаимодействуют во время выполнения для достижения какой-либо цели. Каждый элемент имеет определенную роль в сотрудничестве.

![Диаграмма составной структуры](https://habrastorage.org/webt/xe/_q/mb/xe_qmbmhorarotvrzrzj-owtrtq.jpeg)

### Диаграмма профилей
<a id="profile-diagram"></a>

([наверх](#sections))

Диаграмма профилей позволяет нам создавать специфичные для домена и платформы стереотипы и определять отношения между ними. Мы можем создавать стереотипы, рисуя формы стереотипов и связывая их с композицией или обобщением через интерфейс, ориентированный на ресурсы. Мы также можем определять и визуализировать значения стереотипов.

![Диаграмма профилей](https://habrastorage.org/webt/jn/c2/8v/jnc28vsuumwxobgsxmvvszusz1i.jpeg)

### Диаграмма прецедентов
<a id="use-case-diagram"></a>

([наверх](#sections))

Диаграмма прецедентов описывает функциональные требования системы с точки зрения прецедентов. По сути дела, это модель предполагаемой функциональности системы (прецедентов) и ее среды (актеров).

Прецеденты позволяют связать то, что нам нужно от системы с тем, как система удовлетворяет эти потребности.

![Диаграмма прецедентов](https://habrastorage.org/webt/q7/tm/yr/q7tmyr_d_aosxppt8i6rbk20ggq.jpeg)

### Диаграмма деятельности
<a id="activity-diagram"></a>

([наверх](#sections))

Диаграммы деятельности представляют собой графическое представление рабочих процессов поэтапных действий и действий с поддержкой выбора, итерации и параллелизма.
Они описывают поток управления целевой системой, такой как исследование сложных бизнес-правил и операций, а также описание прецедентов и бизнес-процессов.
В UML диаграммы деятельности предназначены для моделирования как вычислительных, так и организационных процессов.

![Диаграмма деятельности](https://habrastorage.org/webt/5h/dm/mh/5hdmmhiwtzdrswnnw6vgqy8ezzw.jpeg)

### Диаграмма состояний
<a id="state-diagram"></a>

([наверх](#sections))

Диаграмма состояний — это тип диаграммы, используемый в UML для описания поведения систем, который основан на концепции диаграмм состояний Дэвида Харела. Диаграммы состояний отображают разрешенные состояния и переходы, а также события, которые влияют на эти переходы. Она помогает визуализировать весь жизненный цикл объектов и, таким образом, помогает лучше понять системы, основанные на состоянии.

![Диаграмма состояний](https://habrastorage.org/webt/wf/67/mr/wf67mrwroyogobe5agpoaj2juy8.jpeg)

### Диаграмма последовательности
<a id="sequence-diagram"></a>

([наверх](#sections))

Диаграмма последовательности моделирует взаимодействие объектов на основе временной последовательности. Она показывает, как одни объекты взаимодействуют с другими в конкретном прецеденте.

![Диаграмма последовательности](https://habrastorage.org/webt/wr/6n/26/wr6n26qbnsdpvlknj151uwatzvw.jpeg)

### Диаграмма Коммуникации
<a id="communication-diagram"></a>

([наверх](#sections))

Как и диаграмма последовательности, диаграмма коммуникации также используется для моделирования динамического поведения прецедента. Если сравнивать с Диаграммой последовательности, Диаграмма коммуникации больше сфокусирована на показе взаимодействия объектов, а не временной последовательности. На самом деле, диаграмма коммуникации и диаграмма последовательности семантически эквивалентны и могут перетекать одна в другую.

![Диаграмма Коммуникации](https://habrastorage.org/webt/sj/6h/gz/sj6hgzpzw-zsymldpiizkaap2rg.jpeg)

### Диаграмма обзора взаимодействия
<a id="interaction-overview-chart"></a>

([наверх](#sections))

Диаграмма обзора взаимодействий фокусируется на обзоре потока управления взаимодействиями. Это вариант Диаграммы деятельности, где узлами являются взаимодействия или события взаимодействия. Диаграмма обзора взаимодействий описывает взаимодействия, в которых сообщения и линии жизни скрыты. Мы можем связать «реальные» диаграммы и добиться высокой степени навигации между диаграммами внутри диаграммы обзора взаимодействия.

![Диаграмма обзора взаимодействия](https://habrastorage.org/webt/rk/8i/9l/rk8i9lhvjrrvsu8cuthg50fdx0s.jpeg)

### Временная диаграмма
<a id="timing-diagram"></a>

([наверх](#sections))

Временная диаграмма показывает поведение объекта (ов) в данный период времени. По сути — это особая форма диаграммы последовательности и различия между ними состоят в том, что оси меняются местами так, что время увеличивается слева направо, а линии жизни отображаются в отдельных отсеках, расположенных вертикально.

![Временная диаграмма](https://habrastorage.org/webt/fy/ea/gv/fyeagvt6jnk57o6hdkegem61lyi.jpeg)

# Источники
<a id="sources"></a>
([наверх](#sections))

* [Документация python](https://docs.python.org/3/library/)
* ["Как устроен Python. Гид для разработчиков, программистов и интересующихся" Мэтт Харрисон](https://t.me/pythonbooks/389)  
* [Магические методы Rafe Kettler](https://rszalski.github.io/magicmethods/)  
* [Техническая документация по SQL Server](https://docs.microsoft.com/ru-ru/sql/sql-server/?view=sql-server-ver15)  
* [SQL и NoSQL: разбираемся в основных моделях баз данных](https://tproger.ru/translations/sql-nosql-database-models/)  
* [Основные команды SQL, которые должен знать каждый программист](https://tproger.ru/translations/sql-recap/)  
* [Профессиональный информационно-аналитический ресурс, посвященный машинному обучению, распознаванию образов и интеллектуальному анализу данных](https://www.machinelearning.ru/)
* [Inside the Python GIL](http://www.dabeaz.com/python/GIL.pdf)
* [Журнал Mail.ru Cloud Solutions об IT-бизнесе, технологиях и цифровой трансформации](https://mcs.mail.ru/blog/)
* [Школа больших данных](https://www.bigdataschool.ru/)
* [Блог компании Тинькофф](https://habr.com/ru/company/tinkoff/profile/)
* [Блог Хекслета](https://ru.hexlet.io/blog)
